### 第1章 分布式架构

#### 1.1 从集中式到分布式

##### 1.1.1 集中式的特点

所谓的集中式系统就是指由一台或多台主计算机组成中心节点，数据集中存储于这个中 心节点中，并且整个系统的所有业务单元都集中部署在这个中心节点上，系统的所有功 能均由其集中处理。也就是说，在集中式系统中，每个终端或客户端机器仅仅负责数据 的录入和输出，而数据的存储与控制处理完全交由主机来完成。

集中式系统最大的特点就是部署结构简单。由于集中式系统往往基于底层性能卓越的大 型主机，因此无须考虑如何对服务进行多个节点的部署，也就不用考虑多个节点之间的 分布式协作问题。 

##### 1.1.2分布式的特点

在《分布式系统概念与设计》一书中，对分布式系统做了如下定义：

分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。

一个标准的分布式系统在没有任何特定业务逻辑约束的情况下，都会有如下几个特征。

- 分布性：分布式系统中的多台计算机都会在空间上随意分布，同时，机器的分布情况也会随时变动。
- 对等性：分布式系统中的计算机没有主/从之分，既没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。副本（Replica）是分布式系统最常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。在常见的分布式系统中，为了对外提供高可用的服务，我们往往会对数据和服务进行副本处理。数据副本是指在不同的节点上持久化同一份数据，当某一个节点上存 储的数据丢失时，可以从副本上读取到该数据，这是解决分布式系统数据丢失问题最为有效的手段。另一类副本是服务副本，指多个节点提供同样的服务，每个节点 都有能力接收来自外部的请求并进行相应的处理。
- 并发性：在一个计算机网络中，程序运行过程中的并发性操作是非常常见的行为，例如同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，诸如数据库或分布式存储等，如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一。
- 缺乏全局时钟：一个典型的分布式系统是由一系列在空间上随意分布的多个进程组成的，具有明显的分布性，这些进程之间通过交换消息来进行相互通信。因此，在分布式系统中，很难定义两个事件究竟谁先谁后，原因就是分布式系统缺乏一个全局的时钟序列控制。
- 故障总是会发生：组成分布式系统的所有计算机，都有可能发生任何形式的故障。一个被大量工程实践所检验过的黄金定理是：任何在设计阶段考虑到的异常情况，一定会在系统实际运行中发生，并且，在系统实际运行过程中还会遇到很多在设计时未能考虑到的异常故障。所以，除非需求指标允许，在系统设计时不能放过任何异常情况。

##### 1.1.3 分布式环境的各种问题

- 通信异常：从集中式向分布式演变的过程中，必然引入了网络因素，而由于网络本身的不可靠性， 因此也引入了额外的问题。分布式系统需要在各个节点之间进行网络通信，因此每次网 络通信都会伴随着网络不可用的风险，网络光纤、路由器或是DNS等硬件设备或是系统不可用都会导致最终分布式系统无法顺利完成一次网络通信。另外，即使分布式系统 各节点之间的网络通信能够正常进行，其延时也会远大于单机操作。通常我们认为在现代计算机体系结构中，单机内存访问的延时在纳秒数量级（通常是10ns左右），而正常的一次网络通信的延迟在0.1〜1ms左右（相当于内存访问延时的105〜106倍），如此巨大的延时差别，也会影响消息的收发的过程，因此消息丢失和消息延迟变得非常普遍。

- 网络分区：当网络由于发生异常情况，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够进行正常通信，而另一些节点则不能——我们将这个现象称为网络分区，就是俗称的“脑裂”。当网络分区出现时，分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能，包括对数据的事务处理，这就对分布式一致性提出了非常大的挑战。

- 三态：从上面的介绍中，我们已经了解到了在分布式环境下，网络可能会出现各式各样的问题， 因此分布式系统的每一次请求与响应，存在特有的“三态”概念，即成功、失败与超时。在传统的单机系统中，应用程序在调用一个函数之后，能够得到一个非常明确的响应：成功或失败。而在分布式系统中，由于网络是不可靠的，虽然在绝大部分情况下，网络通信也能够接收到成功或失败的响应，但是当网络出现异常的情况下，就可能会出现超时现象，通常有以下两种情况：

  - 由于网络原因，该请求（消息）并没有被成功地发送到接收方，而是在发送过程就发生了消息丢失现象。
  - 该请求（消息）成功的被接收方接收后，并进行了处理，但是在将响应反馈给发送方的过程中，发生了消息丢失现象。

  当出现这样的超时现象时，网络通信的发起方是无法确定当前请求是否被成功处理的。

- 节点故障：节点故障则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或“僵死”现象。通常根据经验来说，每个节点都有可能会出现故障，并且每天都在发生。

#### 1.2 从 ACID 到 CAP/BASE

##### 1.2.1 ACID

事务（Transaction）是由一系列对系统中数据进行访问与更新的操作所组成的一个程序执行逻辑单元（Unit），狭义上的事务特指数据库事务。一方面，当多个应用程序并发访问数据库时，事务可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。另一方面，事务为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持数据一致性的方法。 

事务具有四个特征，分别是原子性(Atomicity)、一致性(Consistency)、隔离性(Isolation)和持久性(Durability)，简称为事务的ACID特性。

###### 原子性

事务的原子性是指事务必须是一个原子的操作序列单元。事务中包含的各项操作在一次执行过程中，只允许出现以下两种状态之一。

- 全部成功执行。
- 全部不执行。

任何一项操作失败都将导致整个事务失败，同时其他已经被执行的操作都将被撤销并回滚，只有所有的操作全部成功，整个事务才算是成功完成。

###### —致性

事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处于一致性状态。也就是说，事务执行的结果必须是使数据库从一个一致性状态转变到另一个一致性状态，因此当数据库只包含成功事务提交的结果时，就能说数据库处于一致性状态。而如果数据库系统在运行过程中发生故障，有些事务尚未完成就被迫中断，这些未完成的事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是不一致的状态。

###### 隔离性

事务的隔离性是指在并发环境中，并发的事务是相互隔离的，一个事务的执行不能被其他事务干扰。也就是说，不同的事务并发操纵相同的数据时，每个事务都有各自完整的数据空间，即一个事务内部的操作及使用的数据对其他并发事务是隔离的，并发执行的各个事务之间不能互相干扰。

在标准SQL规范中，定义了 4 个事务隔离级别，不同的隔离级别对事务的处理不同，如未授权读取、授权读取、可重复读取和串行化。

- 未授权读取：未授权读取也被称为读未提交(Read Uncommitted)，该隔离级别允许脏读取，其隔离级别最低。换句话说，如果一个事务正在处理某一数据，并对其进行了更新，但同时尚未完成事务，因此还没有进行事务提交；而与此同时，允许另一个事务也能够访问该数据。举个例子来说，事务A和事务B同时进行，事务A在整个执行阶段，会将某数据项的值从1开始，做一系列加法操作(比如说加1操作)直到变成10之后进行事务提交，此时，事务B能够看到这个数据项在事务A操作过程中的所有中间值(如1变成2、2变成3等)，而对这一系列的中间值的读取就是未授权读取。

- 授权读取：授权读取也被称为读已提交(Read Committed)，它和未授权读取非常相近，唯一的区别就是授权读取只允许获取已经被提交的数据。同样以上面的例子来说，事务A和事务B同时进行，事务A进行与上述同样的操作，此时，事务B无法看到这个数据项在事务A操作过程中的所有中间值，只能看到最终的10。另外，如果说有一个事务C，和事务A进行非常类似的操作，只是事务C是将数据项从10加到20，此时事务B也同样可以读取到20，即授权读取允许不可重复读取。

- 可重复读取：可重复读取(Repeatable Read)，简单地说，就是保证在事务处理过程中，多次读取同一个数据时，其值都和事务开始时刻是一致的。因此该事务级别禁止了不可重复读取和脏读取，但是有可能出现幻影数据。所谓幻影数据，就是指同样的事务操作， 在前后两个时间段内执行对同一个数据项的读取，可能出现不一致的结果。在上面的例子，可重复读取隔离级别能够保证事务B在第一次事务操作过程中，始终对数据项读取到1，但是在下一次事务操作中，即使事务B (注意，事务名字虽然相同， 但是指的是另一次事务操作)采用同样的査询方式，就可能会读取到10或20。
- 串行化：串行化(Serializable)是最严格的事务隔离级别。它要求所有事务都被串行执行，即事务只能一个接一个地进行处理，不能并发执行。

<img src="img/Zookeeper/1.png" style="zoom: 50%;" />

| 隔离级别   | 脏 读  | 可重复读 | 幻 读  |
| ---------- | ------ | -------- | ------ |
| 未授权读取 | 存在   | 不可以   | 存在   |
| 授权读取   | 不存在 | 不可以   | 存在   |
| 可重复读取 | 不存在 | 可以     | 存在   |
| 串行化     | 不存在 | 可以     | 不存在 |

事务隔离级别越高，就越能保证数据的完整性和一致性，但同时对并发性能的影响也越大。通常，对于绝大多数的应用程序来说，可以优先考虑将数据库系统的隔离级别设置为授权读取，这能够在避免脏读取的同时保证较好的并发性能。尽管这种事务隔离级别会导致不可重复读、虚读和第二类丢失更新等并发问题，但较为科学的做法是在可能出现这类问题的个别场合中，由应用程序主动采用悲观锁或乐观锁来进行事务控制。

###### 持久性

事务的持久性也被称为永久性，是指一个事务一旦提交，它对数据库中对应数据的状态变更就应该是永久性的。换句话说，一旦某个事务成功结束，那么它对数据库所做的更新就必须被永久保存下来——即使发生系统崩溃或机器宕机等故障，只要数据库能够重新启动，那么一定能够将其恢复到事务成功结束时的状态。

##### 1.2.2 分布式事务

分布式事务是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于分布式系统的不同节点之上。通常一个分布式事务中会涉及对多个数据源或业务系统的操作。

我们可以设想一个最典型的分布式事务场景：一个跨银行的转账操作涉及调用两个异地的银行服务，其中一个是本地银行提供的取款服务，另一个则是目标银行提供的存款服务，这两个服务本身是无状态并且是互相独立的，共同构成了一个完整的分布式事务。如果从本地银行取款成功，但是因为某种原因存款服务失败了，那么就必须回滚到取款前的状态，否则用户可能会发现自己的钱不翼而飞了。

从上面这个例子中，我们可以看到，一个分布式事务可以看作是由多个分布式的操作序列组成的，例如上面例子中的取款服务和存款服务，通常可以把这一系列分布式的操作序列称为子事务。

##### 1.2.3 CAP 和 BASE 理论

###### CAP 定理

CAP 理论告诉我们，一个分布式系统不可能同时满足一致性（C：Consistency），可用性（A：Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中的两项。

- 一致性：在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。

  对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数据），这就是典型的分布式数据不一致情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性）。

- 可用性：可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。

  “有限的时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间（即响应时间）内返回对应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，“有限的时间内”是一个在系统设计之初就设定好的系统运行指标，通常不同的系统之间会有很大的不同。

  “返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出对请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果（“OutOfMemoryErrorw“或”System Has Crashed” ）。

- 分区容错性：分区容错性约束了一个分布式系统需要具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。

  网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。

| 放弃CAP定理 | 说 明                                                        |
| ----------- | ------------------------------------------------------------ |
| 放弃P       | 如果希望能够避免系统出现分区容错性问题，一种较为简单的做法是将所有的数据（或者仅仅是那些与事务相关的数据）都放在一个分布式节点上。这样的做法虽然无法100%地保证系统不会出错，但至少不会碰到由于网络分区带来的负面影响。但同时需要注意的是，放弃P的同时也就意味着放弃了系统的可扩展性。 |
| 放弃A       | 相对于放弃“分区容错性”来说，放弃可用性则正好相反，其做法是一旦系统遇到网络分区或其他故障时，那么受到影响的服务需要等待一定的时间，因此在等待期间系统无法对外提供正常的服务，即不可用。 |
| 放弃C       | 这里所说的放弃一致性，并不是完全不需要数据一致性，如果真是这样的话，那么系统的数据都是没有意义的，整个系统也是没有价值的。事实上，放弃一致性指的是放弃数据的强一致性，而保留数据的最终一致性。这样的系统无法保证数据保持实时的一致性，但是能够承诺的是，数据最终会达到一个一致的状态。这就引入了一个时间窗口的概念，具体多久能够达到数据一致取决于系统的设计，主要包括数据副本在不同节点之间的复制时间长短。 |

对于一个分布式系统而言，分区容错性可以说是一个最基本的要求。为什么这样说，其实很简单，因为既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。 因此系统架构设计师往往需要把精力花在如何根据业务特点在C（一致性）和A（可用性）之间寻求平衡。

###### BASE理论

BASE 是 Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的简写，是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性(Strong consistency)，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性(Eventual consistency)。

- 基本可用：基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用。以下两个就是“基本可用”的典型例子。

  - 响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障(比如系统部分机房发生断电或断网故障)，查询结果的响应时间增加到了 1〜2 秒。
  - 功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。

- 弱状态：弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

- 最终一致性：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

  在实际工程实践中，最终一致性存在以下五类主要变种。

  - 因果一致性(**Causal consistency**)：因果一致性是指，如果进程A在更新完某个数据项后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值，即不能发生丢失更新情况。与此同时，与进程A无因果关系的进程C的数据访问则没有这样的限制。
  - 读己之所写(**Read your writes**)：读己之所写是指，进程A更新一个数据项之后，它自己总是能够访问到更新过的最新值，而不会看到旧值。也就是说，对于单个数据获取者来说，其读取到的数据， 一定不会比自己上次写入的值旧。因此，读己之所写也可以看作是一种特殊的因果一致性。
  - 会话一致性 (**Session consistency**)：会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现“读己之所写”的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。
  - 单调读一致性(**Monotonic read consistency**)：单调读一致性是指如果一个进程从系统中读取出一个数据项的某个值后，那么系统对于该进程后续的任何数据访问都不应该返回更旧的值。
  - 单调写一致性(**Monotonic write consistency**)：单调写一致性是指，一个系统需要能够保证来自同一个进程的写操作被顺序地执行。

  总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统事务的ACID特性是相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。

### 第2章 一致性协议

#### 2.1 2PC 与 3PC

在分布式系统中，每一个机器节点虽然都能够明确地知道自己在进行事务操作过程中的结果是成功或失败，但却无法直接获取到其他分布式节点的操作结果。因此，当一个事务操作需要跨越多个分布式节点的时候，为了保持事务处理的ACID特性，就需要引入一个称为“协调者(Coordinator)"的组件来统一调度所有分布式节点的执行逻辑，这 些被调度的分布式节点则被称为“参与者”(Participant)。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务真正进行提交。基于这个思想，衍生出了二阶段提交和三阶段提交两种协议。

##### 2.1.1 2PC

2PC，是Two-Phase Commit的缩写，即二阶段提交，是计算机网络尤其是在数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务处理过程中能够保持原子性和一致性而设计的一种算法。通常，二阶段提交协议也被认为是一种一致性协议，用来保证分布式系统数据的一致性。目前，绝大部分的关系型数据库都是采用二阶段提交协议来完成分布式事务处理的，利用该协议能够非常方便地完成所有分布式事务参与者的协调，统一决定事务的提交或回滚，从而能够有效地保证分布式数据一致性，因此二阶段提交协议被广泛地应用在许多分布式系统中。

###### 阶段一：提交事务请求，“投票阶段”

1. 事务询问。协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。

2. 执行事务。各参与者节点执行事务操作，并将Undo和Redo信息记入事务日志中。

3. 各参与者向协调者反馈事务询问的响应。如果参与者成功执行了事务操作，那么就反馈给协调者Yes响应，表示事务可以执行；如果参与者没有成功执行事务，那么就反馈给协调者No响应，表示事务不可以执行。

###### 阶段二：执行事务提交，“执行阶段”

- 执行事务提交：假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务提交。
  1. 发送提交请求。协调者向所有参与者节点发出Commit请求。
  2. 事务提交。参与者接收到Commit请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源。
  3. 反馈事务提交结果。参与者在完成事务提交之后，向协调者发送Ack消息。
  4. 完成事务。协调者接收到所有参与者反馈的Ack消息后，完成事务。
- 中断事务：假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。
  1. 发送回滚请求。协调者向所有参与者节点发出Rollback请求。
  2. 事务回滚。参与者接收到Rollback请求后，会利用其在阶段一中记录的Undo信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。
  3. 反馈事务回滚结果。参与者在完成事务回滚之后，向协调者发送Ack消息。
  4. 中断事务。协调者接收到所有参与者反馈的Ack消息后，完成事务中断。

简单地讲，二阶段提交将一个事务的处理过程分为了投票和执行两个阶段，其核心是对每个事务都采用先尝试后提交的处理方式，因此也可以将二阶段提交看作一个强一致性的算法。

<img src="img/Zookeeper/2.png" style="zoom: 50%;" />

###### 优缺点

二阶段提交协议的优点：原理简单，实现方便。

二阶段提交协议的缺点：同步阻塞、单点问题、脑裂、太过保守。

- 同步阻塞：二阶段提交协议存在的最明显也是最大的一个问题就是同步阻塞，这会极大地限制分布式系统的性能。在二阶段提交的执行过程中，所有参与该事务操作的逻辑都处于阻塞状态，也就是说，各个参与者在等待其他参与者响应的过程中，将无法进行其他任何操作。

- 单点问题：协调者的角色在整个二阶段提交协议中起到了非常重要的作用。一旦协调者出现问题，那么整个二阶段提交流程将无法运转，更为严重的是，如果协调者是在阶段二中出现问题的话，那么其他参与者将会一直处于锁定事务资源的状态中，而无法继续完成事务操作。

- 数据不一致：在二阶段提交协议的阶段二，即执行事务提交的时候，当协调者向所有的参与者发送Commit请求之后，发生了局部网络异常或者是协调者在尚未发送完Commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了Commit请求。于是，这部分收到了 Commit请求的参与者就会进行事务的提交，而其他没有收到Commit请求的参与者则无法进行事务提交，于是整个分布式系统便出现了数据不一致性现象。

- 太过保守：如果在协调者指示参与者进行事务提交询问的过程中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息的话，这时协调者只能依靠其自身的超时机制来判断是否需要中断事务，这样的策略显得比较保守。换句话说，二阶段提交协议没有设计较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。

##### 2.1.2 3PC

3PC，是Three-Phase Commit的缩写，即三阶段提交，是2PC的改进版，其将二阶段提交协议的“提交事务请求” 过程一分为二，形成了由 CanCommit，PreCommit 和 doCommit 三个阶段组成的事务处理协议。

<img src="img/Zookeeper/3.png" style="zoom: 80%;" />

###### 阶段一：CanCommit

1. 事务询问。协调者向所有的参与者发送一个包含事务内容的canCommit请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应。

2. 各参与者向协调者反馈事务询问的响应。参与者在接收到来自协调者的canCommit请求后，正常情况下，如果其自身认为可以顺利执行事务，那么会反馈Yes响应，并进入预备状态，否则反馈No响应。

###### 阶段二：PreCommit

- 执行事务预提交：假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务预提交。

  1. 发送预提交请求。协调者向所有参与者节点发出preCommit的请求，并进入Prepared阶段。

  2. 事务预提交。参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中。

  3. 各参与者向协调者反馈事务执行的响应。如果参与者成功执行了事务操作，那么就会反馈给协调者Ack响应，同时等待最终的指令：提交(commit)或中止(abort)。

- 中断事务：假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。

  1. 发送中断请求。协调者向所有参与者节点发出abort请求。

  2. 中断事务。无论是收到来自协调者的abort请求，或者是在等待协调者请求过程中出现超时，参与者都会中断事务。

###### 阶段三：doCommit

- 执行提交

  1. 发送提交请求。进入这一阶段，假设协调者处于正常工作状态，并且它接收到了来自所有参与者的Ack响应，那么它将从“预提交”状态转换到“提交”状态，并向所有的参与者发送doCommit请求。

  2. 事务提交。参与者接收到doCommit请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源。

  3. 反馈事务提交结果。参与者在完成事务提交之后，向协调者发送Ack消息。

  4. 完成事务。协调者接收到所有参与者反馈的Ack消息后，完成事务。

- 中断事务：进入这一阶段，假设协调者处于正常工作状态，并且有任意一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。

  1. 发送中断请求。协调者向所有的参与者节点发送abort请求。

  2. 事务回滚。参与者接收到abort请求后，会利用其在阶段二中记录的Undo信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。

  3. 反馈事务回滚结果。参与者在完成事务回滚之后，向协调者发送Ack消息。

  4. 中断事务。协调者接收到所有参与者反馈的Ack消息后，中断事务。

###### 优缺点

三阶段提交协议的优点：相较于二阶段提交协议，三阶段提交协议最大的优点就是降低了参与者的阻塞范围（因为加了第一阶段只确认一下能不能提交），并且能够在出现单点故障后继续达成一致（是指协调者在三阶段即使因为故障问题无法发出指令，参与者由于在二阶段已经完成了事务操作，因此会在超时后进行事务提交，而不会像2pc那样收不到二阶段的指令就一直处于等待状态）。

三阶段提交协议的缺点：三阶段提交协议在去除阻塞的同时也引入了新的问题，那就是在参与者接收到preCommit 消息后，如果网络出现分区，此时协调者所在的节点和参与者无法进行正常的网络通信，在这种情况下，该参与者依然会进行事务的提交，这必然出现数据的不一致性。

#### 2.2 Paxos 算法

Paxos算法是莱斯利•兰伯特(Leslie Lamport)于1990年提出的一种基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。

Paxos算法需要解决的问题就是如何在一个可能发生异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。

Paxos算法引入了“过半”的理念，通俗地讲就是少数服从多数的原则。同时，Paxos算法支持分布式节点角色之间的轮换，这极大地避免了分布式单点的出现，因此Paxos算法既解决了无限期等待问题，也解决了“脑裂”问题，是目前来说最优秀的分布式一致性协议之一。

##### 2.2.3 Paxos算法详解

###### 问题描述

假设有一组可以提出提案的进程集合，那么对于一个一致性算法来说需要保证以下几点:

- 在这些被提出的提案中，只有一个会被选定。

- 如果没有提案被提出，那么就不会有被选定的提案。

- 当一个提案被选定后，进程应该可以获取被选定的提案信息。

对于一致性来说，安全性(Safety)需求如下：

- 只有被提出的提案才能被选定(Chosen)。

- 只能有一个值被选定。

- 如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个。

Paxos算法的目标就是要保证最终有一个提案会被选定，当提案被选定后，进程最终也能获取到被选定的提案。

在该一致性算法中，有三种参与角色，我们用Proposer，Acceptor和Learner来表示。假设不同参与者之间可以通过收发消息来进行通信，那么：

- 每个参与者以任意的速度执行，可能会因为出错而停止，也可能会重启。同时，即使一个提案被选定后，所有的参与者也都有可能失败或重启，因此除非那些失败或重启的参与者可以记录某些信息，否则将无法确定最终的值。

- 消息在传输过程中可能会出现不可预知的延迟，也可能会重复或丢失，但是消息不会被损坏，即消息内容不会被篡改。

###### 提案的选定

要选定一个唯一提案的最简单方式莫过于只允许一个Accpetor存在，这样的话，Proposer 只能发送提案给该Accpetor，Acceptor会选择它接收到的第一个提案作为被选定的提案。这种解决方式尽管实现起来非常简单，但是却很难让人满意，因为一旦这个Accpetor 出现问题，那么整个系统就无法工作了。

因此，应该寻找一种更好的解决方式，例如可以使用多个Accpetor来避免Accpetor的单点问题。现在我们就来看看，在存在多个Acceptor的情况下，如何进行提案的选取：Proposer向一个Acceptor集合发送提案，同样，集合中的每个Acceptor都可能会批准(Accept)该提案，当有足够多的Acceptor批准这个提案的时候，我们就可以认为该提案被选定了。那么，什么是足够多呢？我们假定足够多的Acceptor是整个Acceptor集合的一个子集，并且让这个集合大得可以包含Acceptor集合中的大多数成员，因为任意两个包含大多数Acceptor的子集至少有一个公共成员（至少为一半+1）。另外我们再规定，每个Acceptor最多只能批准一个提案，那么就能保证只有一个提案被选定了。

###### 推导过程

在没有失败和消息丟失的情况下，如果我们希望即使在只有一个提案被提出的情况下，仍然可以选出一个提案，这就暗示了如下的需求。

```
P1: 一个Acceptor必须批准它收到的第一个提案。
```

在P1的基础上，再加上一个提案被选定需要由半数以上的Acceptor批准的需求暗示着一个Acceptor必须能够批准不止一个提案。

虽然允许多个提案被选定，但同时必须要保证所有被选定的提案都具有相同的Value值一一这是一个关于提案Value的约定，结合提案的编号，该约定可以定义如下：

```
P2：如果编号为M0、Value值为V0的提案（即[M0, V0]）被选定了，那么所有比编号M0更高的，且被选定的提案，其Value值必须也是V0。
```

因为提案的编号是全序的，条件P2就保证了只有一个Value值被选定这一关键安全性属性。同时，一个提案要被选定，其首先必须被至少一个Acceptor批准，因此我们可以通过满足如下条件来满足P2。

```
P2a：如果编号为M0、Value值为V0的提案（即[M0, V0]）被选定了，那么所有比编号M0更高的，且被Acceptor批准的提案，其Value值必须也是V0。
```

如果要同时满足P1和P2a，需要对P2a进行如下强化：

```
P2b：如果一个提案[M0, V0]被选定后，那么之后任何Proposer产生的编号更高的提案，其Value值都为V0。
```

因为编号为M0的提案已经被选定了，这就意味着肯定存在一个由半数以上的Acceptor组成的集合C, C中的每个Acceptor都批准了该提案。因为任何包含半数以上Acceptor的集合S都至少包含C中的一个成员，因此我们可以认为如果保持了下面P2c的不变性，那么编号为Mn的提案的Value也为V0。

```
P2c：对于任意的Mn和Vn，如果提案[Mn，Vn]被提出，那么肯定存在一个由半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个。
• S中不存在任何批准过编号小于Mn的提案的Acceptor。
• 选取S中所有Acceptor批准的编号小于Mn的提案，其中编号最大的那个提案其Value值是Vn。
```

###### Proposer生成提案

对于一个Proposer来说，获取那些已经被通过的提案远比预测未来可能会被通过的提案来得简单。因此，Proposer在产生一个编号为Mn的提案时，必须要知道当前某一个将要或已经被半数以上Acceptor批准的编号小于Mn但为最大编号的提案。并且，Proposer会要求所有的Acceptor都不要再批准任何编号小于Mn的提案——这就引出了如下的提案生成算法。

1. Proposer选择一个新的提案编号Mn，然后向某个Acceptor集合的成员发送请求，要求该集合中的Acceptor做出如下回应。

   - 向Proposer承诺，保证不再批准任何编号小于Mn的提案。
   - 如果Acceptor已经批准过任何提案，那么其就向Proposer反馈当前该Acceptor已经批准的编号小于Mn但为最大编号的那个提案的值。

   我们将该请求称为编号为Mn的提案的Prepare请求。

2. 如果Proposer收到了来自半数以上的Acceptor的响应结果，那么它就可以产生编号为Mn、Value值为Vn的提案，这里的Vn是所有响应中编号最大的提案的Value值。当然还存在另一种情况，就是半数以上的Acceptor都没有批准过任何提案，即响应中不包含任何的提案，那么此时Vn值就可以由Proposer任意选择。

在确定提案之后，Proposer就会将该提案再次发送给某个Acceptor集合，并期望获得它们的批准，我们称此请求为Accept请求。需要注意的一点是，此时接受Accept请求的Acceptor集合不一定是之前响应Prepare请求的Acceptor集合。因为任意两个半数以上的Acceptor集合，必定包含至少一个公共Acceptor。

###### Acceptor批准提案

一个Acceptor可能会收到来自Proposer的两种请求，分别是Prepare请求和Accept请求，对这两类请求做出响应的条件分别如下。

- Prepare请求：Acceptor可以在任何时候响应一个Prepare请求。

- Accept请求：在不违背Accept现有承诺的前提下，可以任意响应Accept请求。

因此，对Acceptor逻辑处理的约束条件，大体可以定义如下。

```
P1a：一个Acceptor只要尚未响应过任何编号大于Mn的Prepare请求，那么它就可以接受这个编号为Mn的提案。
```

从上面这个约束条件中，我们可以看出，P1a包含了P1。同时，值得一提的是，Paxos 算法允许Acceptor忽略任何请求而不用担心破坏其算法的安全性。

###### 算法优化

假设一个Acceptor收到了一个编号为Mn的Prepare请求，但此时该Acceptor已经对编号大于Mn的Prepare请求做出了响应，因此它肯定不会再批准任何新的编号为Mn的提案，那么很显然，Acceptor就没有必要对这个Prepare请求做出响应，于是Acceptor可以选择忽略这样的Prepare请求。同时，Acceptor也可以忽略掉那些它已经批准过的提案的Prepare请求。

通过这个优化，每个Acceptor只需要记住它已经批准的提案的最大编号以及它已经做出Prepare请求响应的提案的最大编号，以便在出现故障或节点重启的情况下，也能保证P2c的不变性。而对于Proposer来说，只要它可以保证不会产生具有相同编号的提案， 那么就可以丢弃任意的提案以及它所有的运行时状态信息。

###### 算法陈述

- 阶段一

  1. Proposer选择一个提案编号Mn，然后向Acceptor的某个超过半数的子集成员发送编号为Mn的Prepare请求。

  2. 如果一个Acceptor收到一个编号为Mn的Prepare请求，且编号Mn大于该Acceptor已经响应的所有Prepare请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给Proposer，同时该Acceptor会承诺不会再批准任何编号小于Mn的提案。

     举个例子来说，假定一个Acceptor已经响应过的所有Prepare请求对应的提案编号分别为1、2、…、5和7,那么该Acceptor在接收到一个编号为8的Prepare请求后，就会将编号为7的提案作为响应反馈给Proposer。

- 阶段二

  1. 如果Proposer收到来自半数以上的Acceptor对于其发出的编号为Mn的Prepare请求的响应，那么它就会发送一个针对[Mn,Vn]提案的Accept请求给Acceptor。注意，Vn的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它就是任意值。
  2. 如果Acceptor收到这个针对[Mn, VnJ提案的Accept请求，只要该Acceptor尚未对编号大于Mn的Prepare请求做出响应，它就可以通过这个提案。

###### 提案的获取

- 方案一：Learner获取一个已经被选定的提案的前提是，该提案已经被半数以上的Acceptor批准。因此，最简单的做法就是一旦Acceptor批准了一个提案，就将该提案发送给所有的Learner。很显然，这种做法虽然可以让Learner尽快地获取被选定的提案，但是却需要让每个Acceptor与所有的Learner逐个进行一次通信，通信的次数至少为二者个数的乘积。

- 方案二：另一种可行的方案是，我们可以让所有的Acceptor将它们对提案的批准情况，统一发送给一个特定的Learner(下文中我们将这样的Learner称为“主Learner")，在不考虑拜占庭将军问题的前提下，我们假定Learner之间可以通过消息通信来互相感知提案的选定情况。基于这样的前提，当主Learner被通知一个提案已经被选定时，它会负责通知其他的Learner。

  在这种方案中，Acceptor首先会将得到批准的提案发送给主Learner，再由其同步给其他Learner，因此较方案一而言，方案二虽然需要多一个步骤才能将提案通知到所有的Learner，但其通信次数却大大减少了，通常只是Acceptor和Learner的个数总和。但同时，该方案引入了一个新的不稳定因素：主Learner随时可能出现故障。

- 方案三：对方案二进行改进，可以将主Learner的范围扩大，即Acceptor可以将批准的提案发送给一个特定的Learner集合，该集合中的每个Learner都可以在一个提案被选定后通知所有其他的Learner。这个Learner集合中的Learner个数越多，可靠性就越好，但同时网络通信的复杂度也就越高。

### 第3章 Paxos的工程实践

#### 3.1 Chubby

##### 3.1.1 概述

Chubby是一个面向松耦合分布式系统的锁服务，通常用于为一个由大量小型计算机构成的松耦合分布式系统提供高可用的分布式锁服务。一个分布式锁服务的目的是允许它的客户端进程同步彼此的操作，并对当前所处环境的基本状态信息达成一致。针对这个目的，Chubby提供了粗粒度的分布式锁服务，开发人员不需要使用复杂的同步协议，而是直接调用Chubby的锁服务接口即可实现分布式系统中多个进程之间粗粒度的同步控制，从而保证分布式数据的一致性。

Chubby的客户端接口设计非常类似于UNIX文件系统结构，应用程序通过Chubby的客户端接口，不仅能够对Chubby服务器上的整个文件进行读写操作，还能够添加对文件节点的锁控制，并且能够订阅Chubby服务端发出的一系列文件变动的事件通知。

##### 3.1.2 应用场景

在Chubby的众多应用场景中，最为典型的就是集群中服务器的Master选举。例如在 Google 文件系统(Google File System,GFS)中使用 Chubby 锁服务来实现对 GFS Master 服务器的选举。而在BigTable中，Chubby同样被用于Master选举，并且借助Chubby，Master能够非常方便地感知到其所控制的那些服务器。

##### 3.1.3 设计目标

Chubby被设计成一个需要访问中心化节点的分布式锁服务，是因为锁服务具有以下4个传统算法库所不具有的优点。

- 对上层应用程序的侵入性更小：对于应用程序，尤其是上层的业务系统来说，在系统开发初期，开发人员并没有为 系统的高可用性做好充分的考虑。事实上，绝大部分的系统一开始都是从一个只需要支撑较小的负载，并且只需要保证大体可用的原型开始的，往往并没有在代码层面为分布式一致性协议的实现留有余地。当系统提供的服务日趋成熟，并且得到一定规模的用户认可之后，系统的可用性就会变得越来越重要了。于是，集群中副本复制和Master选举等一系列提高分布式系统可用性的措施，就会被加入到一个已有的系统中去。

  在这种情况下，尽管这些措施都可以通过一个封装了分布式一致性协议的客户端库来完成，但相比之下，使用一个分布式锁服务的接口方式对上层应用程序的侵入性会更小，并且更易于保持系统已有的程序结构和网络通信模式。

- 便于提供数据的发布与订阅：几乎在所有使用Chubby进行Master选举的应用场景中，都需要一种广播结果的机制，用来向所有的客户端公布当前的Master服务器。这就意味着Chubby应该允许其客户端在服务器上进行少量数据的存储与读取——也就是对小文件的读写操作。虽然这个特性也能够通过一个分布式命名服务来实现，但是根据实际的经验来看，分布式锁服务本身也非常适合提供这个功能，这一方面能够大大减少客户端依赖的外部服务，另一方面，数据的发布与订阅功能和锁服务在分布式一致性特性上是相通的。

- 开发人员对基于锁的接口更为熟悉：对于绝大部分的开发人员来说，在平常的编程过程中，他们对基于锁的接口都已经非常熟悉了。因此，Chubby为其提供了一套近乎和单机锁机制一致的分布式锁服务接口，这远比提供一个一致性协议的库来得更为友好。

- 更便捷地构建更可靠的服务：通常一个分布式一致性算法都需要使用Quorum机制来进行数据项值的选定。 Quorum机制是分布式系统中实现数据一致性的一个比较特殊的策略，它指的是在一个由若干个机器组成的集群中，在一个数据项值的选定过程中，要求集群中存在过半的机器达成一致，因此Quorum机制也被称作“过半机制”。在Chubby中通常使用5台服务器来组成一个集群单元(cell)，根据Quorum机制，只要整个集群中有3台服务器是正常运行的，那么整个集群就可以对外提供正常的服务。相反的，如果仅提供一个分布式一致性协议的客户端库，那么这些高可用性的系统部署都将交给开发人员自己来处理，这无疑提高了成本。

在Chubby的设计过程中，提出了以下几个设计目标。

- 提供一个完整的、独立的分布式锁服务，而非仅仅是一个一致性协议的客户端库：Chubby 对于使用它的应用程序的侵入性非常低，应用程序不需要修改已有程序的结构即可使用分布式一致性特性。例如，对于“Master选举同时将Master信息登记并广播”的场景，应用程序只需要向Chubby请求一个锁，并且在获得锁之后向相应的锁文件写入Master信息即可，其余的客户端就可以通过读取这个锁文件来获取Master信息。

- 提供粗粒度的锁服务：Chubby锁服务针对的应用场景是客户端获得锁之后会进行长时间持有（数小时或 数天），而非用于短暂获取锁的场景。针对这种应用场景，当锁服务短暂失效时（例如服务器宕机），Chubby需要保持所有锁的持有状态，以避免持有锁的客户端出现问题。这和细粒度锁的设计方式有很大的区别，细粒度锁通常设计为锁服务一旦失效就释放所有锁，因为细粒度锁的持有时间很短，相比而言放弃锁带来的代价较小。

- 在提供锁服务的同时提供对小文件的读写功能：Chubby提供对小文件的读写服务，以使得被选举出来的Master可以在不依赖额外服务的情况下，非常方便地向所有客户端发布自己的状态信息。具体的，当一个客 户端成功获取到一个Chubby文件锁而成为Master之后，就可以继续向这个文件里写入Master信息，其他客户端就可以通过读取这个文件得知当前的Master信息。

- 高可用、高可靠：在Chubby的架构设计中，允许运维人员通过部署多台机器（一般是5台机器）来组成一个Chubby集群，从而保证集群的高可用。基于对Paxos算法的实现，对于一个由5台机器组成的Chubby集群来说，只要保证存在3台正常运行的机器，整个集群对外服务就能保持可用。

  另外，由于Chubby支持通过小文件读写服务的方式来进行Master选举结果的发布与订阅，因此在Chubby的实际应用过程中，必须能够支撑成百上千个Chubby客户端对同一个文件进行监视和读取。

- 提供事件通知机制：在实际使用过程中，Chubby客户端需要实时地感知到Master的变化情况，当然这可以通过让客户端反复的轮询来实现，但是在客户端规模不断增大的情况下，客户端主动轮询的实时性效果并不理想，且对服务器性能和网络带宽压力都非常大。因此，Chubby需要有能力将服务端的数据变化情况(例如文件内容变更)以事件的形式通知到所有订阅的客户端。


##### 3.1.4 Chubby技术架构

###### 系统结构

Chubby的整个系统结构主要由服务端和客户端两部分组成，客户端通过RPC调用与服务端进行通信。

<img src="img/Zookeeper/5.png" style="zoom:80%;" />

一个典型的Chubby集群，或称为Chubby cell，通常由5台服务器组成。这些副本服务器采用Paxos协议，通过投票的方式来选举产生一个获得过半投票的服务器作为Master。一旦某台服务器成为了Master，Chubby就会保证在一段时期内不会再有其他服务器成为Master，这段时期被称为Master租期(Master lease)。在运行过程中，Master服务器会通过不断续租的方式来延长Master租期，而如果Master服务器出现故障，那么余下的服务器就会进行新一轮的Master选举，最终产生新的Master服务器，开始新的 Master 租期。

集群中的每个服务器都维护着一份服务端数据库的副本，但在实际运行过程中，只有Master服务器才能对数据库进行写操作，而其他服务器都是使用Paxos协议从Master服务器上同步数据库数据的更新。

现在，我们再来看下Chubby的客户端是如何定位到Master服务器的。Chubby客户端通过向记录有Chubby服务端机器列表的DNS来请求获取所有的Chubby服务器列表，然后逐个发起请求询问该服务器是否是Master。在这个询问过程中，那些非Master的服务器，则会将当前Master所在的服务器标识反馈给客户端，这样客户端就能够非常快速地定位到Master服务器了。

一旦客户端定位到Master服务器之后，只要该Master正常运行，那么客户端就会将所有的请求都发送到该Master服务器上。针对写请求，Chubby Master会采用一致性协议将其广播给集群中所有的副本服务器，并且在过半的服务器接受了该写请求之后，再响应给客户端正确的应答。而对于读请求，则不需要在集群内部进行广播处理，直接由Master服务器单独处理即可。

在Chubby运行过程中，服务器难免会发生故障。如果当前的Master服务器崩溃了，那么集群中的其他服务器会在Master租期到期后，重新开启新一轮的Master选举。通常，进行一次Master选举大概需要花费几秒钟的时间。而如果是集群中任意一台非Master服务器崩溃，那么整个集群是不会停止工作的，这个崩潰的服务器会在恢复之后自动加入到Chubby集群中去。新加入的服务器首先需要同步Chubby最新的数据库数据，完成数据同步之后，新的服务器就可以加入到正常的Paxos运作流程中与其他服务器副本一起协同工作。

如果集群中的一个服务器发生崩溃并在几小时后仍无法恢复正常，那么就需要加入新的机器，并同时更新DNS列表。Chubby服务器的更换方式非常简单，只需要启动Chubby服务端程序，然后更新DNS上的机器列表（即使用新机器的IP地址替换老机器的IP地址）即可。在Chubby运行过程中，Master服务器会周期性地轮询DNS列表，因此其很快就会感知到服务器地址列表的变更，然后Master就会将集群数据库中的地址列表做相应的变更，集群内部的其他副本服务器通过复制方式就可以获取到最新的服务器地址列表了。

###### 目录与文件

Chubby对外提供了一套与Unix文件系统非常相近但是更简单的访问接口。Chubby的数据结构可以看作是一个由文件和目录组成的树，其中每一个节点都可以表示为一个使用斜杠分割的字符串，典型的节点路径表示如下：

```
/ls/foo/wombat/pouch
```

其中，ls是所有Chubby节点所共有的前缀，代表着锁服务，是Lock Service的缩写；foo则指定了Chubby集群的名字，从DNS可以査询到由一个或多个服务器组成该Chubby集群；剩余部分的路径/yvombat/pouch则是一个真正包含业务含义的节点名字，由Chubby服务器内部解析并定位到数据节点。

Chubby的命名空间，包括文件和目录，我们称之为节点。在同一个Chubby集群数据库中，每一 个节点都是全局唯一的。和Unix系统一样，每个目录都可以包含一系列的子文件和子目录列表，而每个文件中则会包含文件内容。当然，Chubby并非模拟一个完整的文件系统，因此没有符号链接和硬连接的概念。

Chubby上的每个数据节点都分为持久节点和临时节点两大类，其中持久节点需要显式地调用接口 API来进行删除，而临时节点则会在其对应的客户端会话失效后被自动删除。

另外，Chubby上的每个数据节点都包含了少量的元数据信息，其中包括用于权限控制的访问控制列表（ACL）信息。同时，每个节点的元数据中还包括4个单调递增的64位编号。

###### 锁与锁序列器

在分布式系统中，锁是一个非常复杂的问题，由于网络通信的不确定性，导致在分布式系统中锁机制变得非常复杂，消息的延迟或是乱序都有可能会引起锁的失效。一个典型的分布式锁错乱案例是，一个客户端G获取到了互斥锁L，并且在锁L的保护下发出请求R，但请求R迟迟没有到达服务端（可能出现网络延时或反复重发等），这时应用程序会认为该客户端进程已经失败，于是便会为另一个客户端C2分配锁L，然后再重新发起之前的请求R，并成功地应用到了服务器上。此时，不幸的事情发生了，客户端G发起的请求R在经过一波三折之后也到达了服务端，此时，它有可能会在不受任何锁控制的情况下被服务端处理，从而覆盖了客户端C2的操作，于是导致系统数据出现不一致。

在Chubby中，任意一个数据节点都可以充当一个读写锁来使用：一种是单个客户端以 排他（写）模式持有这个锁，另一种则是任意数目的客户端以共享（读）模式持有这个锁。同时，在Chubby的锁机制中需要注意的一点是，Chubby舍弃了严格的强制锁，客户端可以在没有获取任何锁的情况下访问Chubby的文件，也就是说，持有锁F既不是访问文件F的必要条件，也不会阻止其他客户端访问文件F。

在Chubby中，主要采用锁延迟和锁序列器两种策略来解决上面我们提到的由于消息延迟和重排序引起的分布式锁问题。

其中锁延迟是一种比较简单的策略。具体的，如果一个客户端以正常的方式主动释放了一个锁，那么Chubby服务端将会允许其他客户端能够立即获取到该锁。而如果一个锁是因为客户端的异常情况（如客户端无响应）而被释放的话，那么Chubby服务器会为该锁保留一定的时间，我们称之为“锁延迟“（lock-delay），在这段时间内，其他客户端无法获取这个锁。锁延迟措施能够很好地防止一些客户端由于网络闪断等原因而与服务器暂时断开的场景出现。

Chubby提供的另一种方式是使用锁序列器，当然该策略需要Chubby的上层应用配合在代码中加入相应的修改逻辑。任何时候，锁的持有者都可以向Chubby请求一个锁序列器，其包括锁的名字、锁模式（排他或共享模式），以及锁序号。当客户端应用程序在进行一些需要锁机制保护的操作时，可以将该锁序列器一并发送给服务端。Chubby服务端接收到这样的请求后，会首先检测该序列器是否有效，以及检查客户端是否处于恰当的锁模式；如果没有通过检查，那么服务端就会拒绝该客户端请求。

###### Chubby中的事件通知机制

为了避免大量客户端轮询Chubby服务端状态所带亲的压力，Chubby提供了事件通知机制。Chubby的客户端可以向服务端注册事件通知，当触发这些事件的时候，服务端就会向客户端发送对应的事件通知。在Chubby的事件通知机制中，消息通知都是通过异步的方式发送给客户端的，常见的Chubby事件如下。

- 文件内容变更：例如，BigTable集群使用Chubby锁来确定集群中的哪台BigTable机器是Master；获得锁的BigTable Master会将自身信息写入Chubby上对应的文件中。BigTable集群中的其他客户端可以通过监视这个Chubby文件的变化来确定新的BigTable Master机器。

- 节点删除：当Chubby上指定节点被删除的时候，会产生“节点删除”事件，这通常在临时节点中比较常见，可以利用该特性来间接判断该临时节点对应的客户端会话是否有效。

- 子节点新增、删除：当Chubby上指定节点的子节点新增或是减少时，会产生“子节点新增、删除”事件。

- Master服务器转移：当Chubby服务器发生Master转移时，会以事件的形式通知客户端。

###### Chubby中的缓存

为了提高Chubby的性能，同时也是为了减少客户端和服务端之间频繁的读请求对服务端的压力，Chubby除了提供事件通知机制之外，还在客户端中实现了缓存，会在客户端对文件内容和元数据信息进行缓存。使用缓存机制在提高系统整体性能的同时，也为系统带来了一定的复杂性，其中最主要的问题就是应该如何保证缓存的一致性。在 Chubby中，通过租期机制来保证缓存的一致性。

Chubby缓存的生命周期和Master租期机制紧密相关，Master会维护每个客户端的数据缓存情况，并通过向客户端发送过期信息的方式来保证客户端数据的一致性。在这种机制下，Chubby就能够保证客户端要么能够从缓存中访问到一致的数据，要么访问出错，而一定不会访问到不一致的数据。具体的，每个客户端的缓存都有一个租期，一旦该租期到期，客户端就需要向服务端续订租期以继续维持缓存的有效性。当文件数据或元数据信息被修改时，Chubby服务端首先会阻塞该修改操作，然后由Master向所有可能缓存了该数据的客户端发送缓存过期信号，以使其缓存失效，等到Master在接收到所有相关客户端针对该过期信号的应答（应答包括两类，一类是客户端明确要求更新缓存，另一类则是客户端允许缓存租期过期）后，再继续进行之前的修改操作。

通过上面这个缓存机制的介绍，Chubby的缓存数据保证了强一致性。尽管要保证严格的数据一致性对于性能的开销和系统的吞吐影响很大，但由于弱一致性模型在实际使用过程中极容易出现问题，因此Chubby在设计之初就决定了选择强一致性模型。

###### 会话和会话激活（KeepAlive）

Chubby客户端和服务端之间通过创建一个TCP连接来进行所有的网络通信操作，我们将这一连接称为会话（Session）。会话是有生命周期的，存在一个超时时间，在超时时间内，Chubby客户端和服务端之间可以通过心跳检测来保持会话的活性，以使会话周期得到延续，我们将这个过程称为KeepAlive （会话激活）。如果能够成功地通过KeepAlive过程将Chubby会话一直延续下去，那么客户端创建的句柄、锁和缓存数据等都依然有效。

- KeepAlive 请求：Master在接收到客户端的KeepAlive请求时，首先会将该请求阻塞住，并等到该客户端的当前会话租期即将过期时，才为其续租该客户端的会话租期，之后再向客户端响应这个KeepAlive请求，并同时将最新的会话租期超时时间反馈给客户端。Master对于会话续租时间的设置，默认是12秒，但这不是一个固定的值，Chubby会根据实际的运行情况，自行调节该周期的长短。举个例子来说，如果当前Master处于高负载运行状态的话，那 么Master会适当地延长会话租期的长度，以减少客户端KeepAlive请求的发送频率。客 户端在接收到来自Master的续租响应后，会立即发起一个新的KeepAlive请求，再由Master进行阻塞。因此我们可以看出，在正常运行过程中，每个Chubby客户端总是会有一个KeepAlive请求阻塞在Master服务器上。

  除了为客户端进行会话续租外，Master还将通过KeepAlive响应来传递Chubby事件通知和缓存过期通知给客户端。具体的，如果Master发现服务端已经触发了针对该客户端的事件通知或缓存过期通知，那么会提前将KeepAlive响应反馈给客户端。

- 会话超时：Chubby的客户端也会维持一个和Master端近似相同的会话租期。为什 么是近似相同呢？这是因为客户端必须考虑两方面的因素：一方面，KeepAlive响应在网络传输过程中会花费一定的时间；另一方面，Master服务端和Chubby客户端存在时钟不一致性现象。因此在Chubby会话中，存在Master端会话租期和客户端本地会话租期。

  如果Chubby客户端在运行过程中，按照本地的会话租期超时时间，检测到其会话租期已经过期却尚未接收到Master的KeepAlive响应，那么这个时候，它将无法确定Master服务端是否已经中止了当前会话，我们称这个时候客户端处于“危险状态”。此时，Chubby 客户端会清空其本地缓存，并将其标记为不可用。同时，客户端还会等待一个被称作“宽限期”的时间周期，这个宽限期默认是45秒。如果在宽限期到期前，客户端和服务端之间成功地进行了KeepAlive，那么客户端就会再次开启本地缓存，否则，客户端就会认为当前会话已经过期了，从而中止本次会话。

  当客户端进入上述提到的危险状态时，Chubby的客户端库会通过一个“jeopardy”事件来通知上层应用程序。如果恢复正常，客户端同样会以一个“safe”事件来通知应用程序可以继续正常运行了。但如果客户端最终没能从危险状态中恢复过来，那么客户端会以一个“expired”事件来通知应用程序当前Chubby会话已经超时。Chubby通过这些不同的事件类型通知，能够很好地辅助上层应用程序在不明确Chubby会话状态的情况下，根据不同的事件类型来做出不同的处理：等待或重启。有了这样的机制保证之后，对于那些在短时间内Chubby服务不可用的场景下，客户端应用程序可以选择等待，而不是重启，这对于那些重启整个应用程序需要花费较大代价的系统来说非常有帮助。

###### Chubby Master故障恢复

Chubby的Master服务器上会运行着会话租期计时器，用来管理所有会话的生命周期。如果在运行过程中Master出现了故障，那么该计时器会停止，直到新的Master选举产生后，计时器才会继续计时，也就是说，从旧的Master崩溃到新的Master选举产生所花费的时间将不计入会话超时的计算中，这等价于延长了客户端的会话租期。如果新的Master在短时间内就选举产生了，那么客户端就可以在本地会话租期过期前与其创建连接。而如果Master的选举花费了较长的时间，就会导致客户端只能清空本地的缓存，并进入宽限期进行等待。

<img src="img/Zookeeper/6.png" style="zoom: 50%;" />

一开始在旧的Master服务器上维持了会话租期“lease M1”，在客户端上维持了对应的“lease C1”，同时客户端的KeepAlive请求1一直被Master阻塞着。在一段时间之后，Master向客户端反馈了KeepAlive响应2，同时开始了新的会话租期“lease M2”，而客户端在接收到该KeepAlive响应之后，立即发送了新的KeepAlive请求3，并同时也开始了新的会话租期”lease C2“，至此，客户端和服务端Master之间的所有交互都是正常的。但是随后，Master发生了故障，从而无法反馈客户端的KeepAlive请求3。在这个过程中，客户端检测到会话租期“lease C2”已经过期，它会清空本地缓存，并进入宽限期。在这段时间内，客户端无法确定Master上的会话周期是否也已经过期，因此它不会销毁它的本地会话，而是将所有应用程序对它的API调用都阻塞住，以避免在这个期间进行的API调用导致数据不一致现象。同时，在客户端宽限期开始时，Chubby客户端会向上层应用程序发送一个“jeopardy”事件。一段时间之后，Chubby服务端选举产生了新的Master，并为该客户端初始化了新的会话租期“lease M3”。当客户端向新的Master发送KeepAlive请求4时，Master检测到该客户端的Master周期号(Master epoch number)已经过期，因此会在KeepAlive响应5中拒绝这个客户端请求，并将最新的Master周期号发送给客户端。之后，客户端会携带上新的Master周期号，再次发送KeepAlive请求6给Master，最终，整个客户端和服务端之间的会话就会再次恢复正常。

在Master转换的这段时间内，只要客户端的宽限期足 够长，那么客户端应用程序就可以在没有任何察觉的情况下，实现Chubby的故障恢复, 但如果客户端的宽限期设置得比较短，那么Chubby客户端就会丢弃当前会话，并将这个异常情况通知给上层应用程序。

一个新的Chubby Master服务器选举产生之后，会进行如下几个主要处理。

1. 新的Master选举产生后，首先需要确定Master周期。Master周期用来唯一标识一 个Chubby集群的Master统治轮次，以便区分不同的Master。一旦新的Master周期确定下来之后，Master就会拒绝所有携带其他Master周期编号的客户端请求，同时告知其最新的Master周期编号，例如上述提到的KeepAlive请求4。需要注意的一点是，只要发生Master重新选举，就一定会产生新的Master周期，即使是在选举前后Master都是同一台机器的情况下也是如此。

2. 选举产生的新Master能够立即对客户端的Master寻址请求进行响应，但是不会立即开始处理客户端会话相关的请求操作。

3. Master根据本地数据库中存储的会话和锁信息，来构建服务器的内存状态。

4. 到现在为止，Master已经能够处理客户端的KeepAlive请求了，但依然无法处理其他会话相关的操作。

5. Master会发送一个“Master故障切换”事件给每一个会话，客户端接收到这个事件后，会清空它的本地缓存，并警告上层应用程序可能已经丢失了别的事件，之后再向Master反馈应答。

6. 此时，Master会一直等待客户端的应答，直到每一个会话都应答了这个切换事件。

7. 在Master接收到了所有客户端的应答之后，就能够开始处理所有的请求操作了。

8. 如果客户端使用了一个在故障切换之前创建的句柄，Master会重新为其创建这个句柄的内存对象，并执行调用。而如果该句柄在之前的Master周期中已经被关闭 了，那么它就不能在这个Master周期内再次被重建了。这一机制就确保了即使由于网络原因使得Master接收到那些延迟或重发的网络数据包，也不会错误地重建一个已经关闭的句柄。

##### 3.1.5 Paxos协议实现

Chubby服务端的基本架构大致分为三层：

- 最底层是容错日志系统(Fault-Tolerant Log)，通过Paxos算法能够保证集群中所有机器上的日志完全一致，同时具备较好的容错性。

- 日志层之上是Key-Value类型的容错数据库(Fault-Tolerant DB)，其通过下层的日志来保证一致性和容错性。

- 存储层之上就是Chubby对外提供的分布式锁服务和小文件存储服务。

其整体架构如图所示。

![img](img/Zookeeper/7.png)

Paxos算法的作用就在于保证集群内各个副本节点的日志能够保持一致。Chubby事务日志中的每一个Value对应Paxos算法中的一个Instance，由于Chubby需要对外提供不间断的服务，因此事务日志会无限增长，于是在整个Chubby运行过程中，会存在多个Paxos Instance。同时，Chubby会为每一个Paxos Instance都按序分配一个全局唯一的Instance编号，并将其顺序写入到事务日志中去。

在此基础上就可以在上层实现一个一致的状态机副本，“数据快照和事务日志回放”机制：集群中的某台机器在宕机重启以后，为了恢复状态机的状态，最简单的方法就是将已经记录的所有事务日志重新执行一遍。但这会有一个明显的问题，就是如果机器上的事务日志已经积累了很多，那么恢复的时间就会非常长，因此需要定期对状态机数据做一个数据快照并将其存入磁盘，然后就可以将数据快照点之前的事务日志清除。

通常副本节点在进行宕机后的恢复过程中，会出现磁盘未损坏和损坏两种情况。前者最为常见，一般通过磁盘上保存的数据库快照和事务日志就可以恢复到之前某个时间点的 状态，之后再向集群中其他正常运行的副本节点索取宕机后缺失的部分数据变更记录，这样即可实现宕机后的数据恢复。另外一种则是磁盘损坏，无法直接从本地数据恢复的情况。针对这种异常情况，就需要从其他副本节点上索取全部的状态数据。

#### 3.2 Hypertable

##### 3.2.1概述

Hypertable是一个使用C++语言开发的开源、高性能、可伸缩的数据库，采用与HBase非常相似的分布式模型，其目的是要构建一个针对分布式海量数据的高并发数据库。

目前Hypertable X支持最基本的添、删、改、查功能，对于事务处理和关联査询等关系型数据库的高级特性都尚未支持。同时，就少量数据记录的査询性能和吞吐量而言，Hypertable可能也不如传统的关系型数据库。和传统关系型数据库相比，Hypertable最大的优势在于以下几点。

- 支持对大量并发请求的处理。

- 支持对海量数据的管理。

- 扩展性良好，在保证可用性的前提下，能够通过随意添加集群中的机器来实现水平扩容。

- 可用性极高，具有非常好的容错性，任何节点的失效，既不会造成系统瘫痪也不会影响数据的完整性。

Hypertable的整体架构如图所示。

<img src="img/Zookeeper/8.png" style="zoom: 50%;" />

Hypertable 的核心组件包括 Hyperspace，RangeServer，Master 和 DFS Broker 四部分。

- Hyperspace是Hypertable中最重要的组件之一，其提供了对分布式锁服务的支持以及对元数据的管理，是保证Hypertable数据一致性的核心。Hyperspace类似于Google BigTable系统中的Chubby，在这里我们可以认为它是一个文件存储系统，主要用来存储一些元数据信息，同时提供分布式锁服务，另外还负责提供高效、可靠的主机选举服务。

- RangeServer是实际对外提供服务的组件单元，负责数据的读取和写入。在Hypertable的设计中，对每一张表都按照主键进行切分，形成多个Range（类似于关系型数据库中的分表），每个Range由一个RangeServer （RangeServer调用DFSBroker来进行数据的读写）负责管理。在Hypertable中，通常会部署多个RangeServer，毎个RangeServer都负责管理部分数据，由Master来负责进行RangeServer的集群管理。

- Master是元数据管理中心，管理包括创建表、删除表或是其他表空间变更在内的所有元数据操作，同时负责检测RangeServer的工作状态，一旦某一个RangeServer宕机或是重启，能够自动进行Range的重新分配，从而实现对RangeServer集群的管理和负载均衡。

- DFS Broker则是底层分布式文件系统的抽象层，用于衔接上层Hypertable和底层文件存储。所有对文件系统的读写操作，都是通过DFS Broker来完成的。目前已经可以接入Hypertable中的分布式文件系统包括HDFS、MapR、Ceph和KFS等，针对任何其他新的文件系统，只需要实现一个对应的DFS Broker，就可以将其快速接入到整个 Hypertable 系统中。

##### 算法实现

###### Active Server

Hyperspace通常以一个服务器集群的形式部署，一般由5~11台服务器组成，在运行过程中，会从集群中选举产生一个服务器作为Active Server，其余的服务器则是Standby Server，同时，Active Server和Standby Server之间会进行数据和状态的实时同步。

在Hypertable启动初始化阶段，Master模块会连接上Hyperspace集群中的任意一台服务器，如果这台Hyperspace服务器恰好处于Active状态，那么便完成了初始化连接，如果连接上的Hyperspace服务器处于Standby状态，那么该Hyperspace服务器会在此次连接创建后，将当前处于Active状态的Hyperspace服务器地址发送给Master模块，Master模块会重新与该Active Hyperspace服务器建立连接，并且之后对Hyperspace的所有操作请求都会发送给这个Hyperspace服务器。换句话说，只有Active Hyperspace才能真正地对外提供服务。

###### 事务请求处理

在Hyperspace集群中，还有一个非常重要的组件，就是BDB。BDB服务也是采用集群部署的，也存在Master的角色，是Hyperspace底层实现分布式数据一致性的精华所在（Hyperspace和BDB一一对应）。

在Hyperspace对外提供服务时，任何对于元数据的操作，Master模块都会将其对应的事务请求发送给Hyperspace服务器。在接收到该事务请求后，Hyperspace服务器就会向BDB集群中的Master服务器发起事务操作。BDB服务器在接收到该事务请求后，会在集群内部发起一轮事务请求投票流程，一旦BDB集群内部过半的服务器成功应用了该事务操作，就会反馈Hyperspace服务器更新已经成功，再由Hyperspace响应上层的Master模块。

###### Active Hyperspace 选举

当某台处于Active状态的Hyperspace服务器出现故障时，集群中剩余的服务器会自动重新选举出新的Active Hyperspace，这一过程称为Hyperspace集群的Active选举。Active 选举过程的核心逻辑就是根据所有服务器上事务日志的更新时间来确定哪个服务器的数据最新——事务日志更新时间越新，那么这台服务器被选举为Active Hyperspace的可能性就越大，因为只有这样，才能避免集群中数据不一致情况的发生。完成Active Hyperspace选举之后，余下所有的服务器就需要和Active Hyperspace服务器进行数据同步，即所有Hyperspace服务器对应的BDB数据库的数据都需要和Master BDB保持一致。

### 第4章 ZooKeeper 与 Paxos

#### 4.1 初识 ZooKeeper

##### 4.1.1 ZooKeeper 介绍

###### ZooKeeper是什么

ZooKeeper是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。ZooKeeper可以保证如下分布式一致性特性。

- 顺序一致性：从同一个客户端发起的事务请求，最终将会严格地按照其发起顺序被应用到 ZooKeeper 中去。
- 原子性：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说, 要么整个集群所有机器都成功应用了某一个事务，要么都没有应用，一定不会出现集群中部分机器应用了该事务，而另外一部分没有应用的情况。
- 单一视图(Single System Image)：无论客户端连接的是哪个ZooKeeper服务器，其看到的服务端数据模型都是一致的。
- 可靠性：一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来，除非有另一个事务又对其进行了变更。
- 实时性：通常人们看到实时性的第一反应是，一旦一个事务被成功应用，那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是，ZooKeeper仅仅保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。

###### ZooKeeper的设计目标

ZooKeeper致力于提供一个高性能、高可用，且具有严格的顺序访问控制能力(主要是写操作的严格顺序性)的分布式协调服务。高性能使得ZooKeeper能够应用于那些对系统呑吐有明确要求的大型分布式系统中，高可用使得分布式的单点问题得到了很好的解决，而严格的顺序访问控制使得客户端能够基于ZooKeeper实现一些复杂的同步原语。下面我们来具体看一下ZooKeeper的四个设计目标。

- 目标一：简单的数据模型

  ZooKeeper使得分布式程序能够通过一个共享的、树型结构的名字空间来进行相互协调。这里所说的树型结构的名字空间，是指ZooKeeper服务器内存中的一个数据模型，其由一系列被称为ZNode的数据节点组成，总的来说，其数据模型类似于一个文件系统，而ZNode之间的层级关系，就像文件系统的目录结构一样。不过和传统的磁盘文件系统不同的是，ZooKeeper将全量数据存储在内存中，以此来实现提高服务器吞吐、减少延迟 的目的。

- 目标二：可以构建集群

  一个ZooKeeper集群通常由一组机器组成，一般3〜5台机器就可以组成一个可用的ZooKeeper集群。组成ZooKeeper集群的每台机器都会在内存中维护当前的服务器状态，并且每台机器之间都互相保持着通信。只要集群中存在超过一半的机器能够正常工作， 那么整个集群就能够正常对外服务。

  ZooKeeper的客户端程序会选择和集群中任意一台机器共同来创建一个TCP连接，而一旦客户端和某台ZooKeeper服务器之间的连接断开后，客户端会自动连接到集群中的其他机器。

- 目标三：顺序访问

  对于来自客户端的每个更新请求，ZooKeeper都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序，应用程序可以使用ZooKeeper的这个特性来实现更高层次的同步原语。

- 目标四：高性能

  由于ZooKeeper将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景。

##### 4.1.3 ZooKeeper的基本概念

###### 集群角色

通常在分布式系统中，构成一个集群的每一台机器都有自己的角色，最典型的集群模式就是Master/Slave模式（主备模式）。在这种模式中，我们把能够处理所有写操作的机器称为Master机器，把所有通过异步复制方式获取最新数据，并提供读服务的机器称为Slave机器。

而在ZooKeeper中，这些概念被颠覆了。它没有沿用传统的Master/Slave概念，而是引入了 Leader，Follower和Observer三种角色。ZooKeeper集群中的所有机器通过一个Leader选举过程来选定一台被称为“Leader”的机器，Leader服务器为客户端提供读和写服务。Follower和Observer都能够提供读服务，唯一的区别在于，Observer机器不参与Leader选举过程，也不参与写操作的“过半写成功”策略，因此Observer可以在不影响写性能的情况下提升集群的读性能。

###### 会话 (Session)

Session是指客户端会话。在 ZooKeeper中，一个客户端连接是指客户端和服务器之间的一个TCP长连接。ZooKeeper 对外的服务端口默认是2181，客户端启动的时候，首先会与服务器建立一个TCP连接，从第一次连接建立开始，客户端会话的生命周期也开始了，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端 主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。

###### 数据节点（Znode）

在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在 ZooKeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。ZooKeeper 将所有数据存储在内存中，数据模型是一棵树（ZNodeTree），由斜杠（/）进行分割的路径，就是一个Znode，例如/foo/path1。每个ZNode上都会保存自己的数据内容，同时还会保存一系列属性信息。

在ZooKeeper中，ZNode可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在 ZooKeeper上。而临时节点的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。一旦节点被标记上这个属性，那么在这个节点被创建的时候，ZooKeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。

###### 版本

在前面我们已经提到，ZooKeeper的每个ZNode上都会存储数据，对应于每个ZNode，ZooKeeper都会为其维护一个叫作Stat的数据结构，Stat中记录了这个ZNode的三个数据版本，分别是version（当前ZNode的版本）、eversion（当前ZNode子节点的版本）和aversion（当前ZNode的ACL版本）。

###### Watcher

Watcher(事件监听器)，是ZooKeeper中的一个很重要的特性。ZooKeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是ZooKeeper实现分布式协调服务的重要特性。

###### ACL

ZooKeeper采用ACL(Access Control Lists)策略来进行权限控制，类似于UNIX文件系统的权限控制。ZooKeeper定义了如下5种权限。

- CREATE：创建子节点的权限。

- READ：获取节点数据和子节点列表的权限。

- WRITE：更新节点数据的权限。

- DELETE：删除子节点的权限。

- ADMIN：设置节点ACL的权限。

其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制。

#### 4.2 ZooKeeper 的 ZAB 协议

##### 4.2.1 ZAB 协议

ZAB协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的原子广播协议。Paxos算法是一种通用的分布式一致性算法。

基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各副本之间数据的一致性。具体的， ZooKeeper使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用ZAB的原子广播协议，将服务器数据的状态变更以事务Proposal的形式广播到所有的副本进程上去。ZAB协议的这个主备模型架构保证了同一时刻集群中只能够有一个主进程来广播服务器的状态变更，因此能够很好地处理客户端大量的并发请求。

另一方面，考虑到在分布式环境中，顺序执行的一些状态变更其前后会存在一定的依赖关系，有些状态变更必须依赖于比它早生成的那些状态变更，例如变更C需要依赖变更A和变更B。这样的依赖关系也对ZAB协议提出了一个要求：ZAB协议必须能够保证一个全局的变更序列被顺序应用，也就是说，ZAB协议需要保证如果一个状态变更已经被处理了，那么所有其依赖的状态变更都应该已经被提前处理掉了。

最后，考虑到主进程在任何时候都有可能出现崩溃退出或重启现象，因此，ZAB协议还需要做到在当前主进程出现上述异常情况的时候，依旧能够正常工作。

ZAB协议的核心是定义了对于那些会改变ZooKeeper服务器数据状态的事务请求的处理方式，即：所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，而余下的其他服务器则成为Follower服务器。Leader服务器负责将一个客户端事务请求转换成一个事务Proposal（提议），并将该Proposal分发给集群中所有的Follower服务器。之后Leader服务器需要等待所有Follower服务器的反馈，一旦超过半数的Follower服务器进行了正确的反馈后，那么Leader就会再次向所有的Follower服务器分发Commit消息，要求其将前一个Proposal进行提交。

##### 4.2.2 协议介绍

ZAB协议包括两种基本的模式，分别是崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当Leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入恢复模式并选举产生新的Leader服务器。当选举产生了新的Leader服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。

当一台同样遵守ZAB协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。

Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他 机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。

###### 消息广播

ZAB协议的消息广播过程使用的是一个原子广播协议，类似于一个二阶段提交过程。针对客户端的事务请求，Leader服务器会为其生成对应的事务Proposal，并将其发送给集群中其余所有的机器，然后再分别收集各自的选票，最后进行事务提交。

<img src="img/Zookeeper/9.png" style="zoom:50%;" />

在ZAB协议的二阶段提交过程中，移除了中断逻辑，所有的Follower服务器要么正常反馈Leader提出的事务Proposal，要么就抛弃Leader服务器。同时，ZAB协议将二阶段提交中的中断逻辑移除意味着我们可以在过半的Follower服务器已经反馈Ack之后就开始提交事务Proposal了，而不需要等待集群中所有的Follower服务器都反馈响应。当然，在这种简化了的二阶段提交模型下，是无法处理Leader服务器崩溃退出而带来的数据不一致问题的，因此在ZAB协议中添加了另一个模式，即采用崩溃恢复模式来解决这个问题。另外，整个消息广播协议是基于具有FIFO特性的TCP协议来进行网络通信的，因此能够很容易地保证消息广播过程中消息接收与发送的顺序性。

在整个消息广播过程中，Leader服务器会为每个事务请求生成对应的Proposal来进行广播，并且在广播事务Proposal之前，Leader服务器会首先为这个事务Proposal分配一个全局单调递增的唯一ID,我们称之为事务ID（即ZXID）。由于ZAB协议需要保证每一个消息严格的因果关系，因此必须将每一个事务Proposal按照其ZXID的先后顺序来进行排序与处理。

具体的，在消息广播过程中，Leader服务器会为每一个Follower服务器都各自分配一个单独的队列，然后将需要广播的事务Proposal依次放入这些队列中去，并且根据FIFO策略进行消息发送。每一个Follower服务器在接收到这个事务Proposal之后，都会首先将其以事务日志的形式写入到本地磁盘中去，并且在成功写入后反馈给Leader服务器一个Ack响应。当Leader服务器接收到超过半数Follower的Ack响应后，就会广播一个Commit消息给所有的Follower服务器以通知其进行事务提交，同时Leader自身也会完成对事务的提交，而每一个Follower服务器在接收到Commit消息后，也会完成对事务的提交。

###### 崩溃恢复

在ZAB协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出一个新的Leader服务器。因此，ZAB协议需要一个高效且可靠的Leader选举算法，从而确保能够快速地选举出新的Leader。同时，Leader选举算法不仅仅需要让Leader自己知道其自身已经被选举为Leader，同时还需要让集群中的所有其他机器也能够快速地感知到选举产生的新的Leader服务器。

###### 基本特性

在崩溃恢复过程中，可能会出现的两个数据不一致性的隐患及针对这些情况ZAB协议所需要保证的特性。

- ZAB协议需要确保那些已经在Leader服务器上提交的事务最终被所有服务器都提交。

- ZAB协议需要确保丢弃那些只在Leader服务器上被提出的事务。

假设一个事务在Leader服务器上被提交了，并且已经得到过半Follower服务器的Ack反馈，但是在它将Commit消息发送给所有Follower机器之前，Leader服务器挂了。

<img src="img/Zookeeper/10.png" style="zoom: 67%;" />

需要确保S2和S3将P1和P2都提交，并且丢弃P3。

结合上面提到的这两个崩溃恢复过程中需要处理的特殊情况，就决定了ZAB协议必须设计这样一个Leader选举算法：能够确保提交已经被Leader提交的事务Proposal，同时丢弃已经被跳过的事务Proposal。针对这个要求，如果让Leader选举算法能够保证新选举出来的Leader服务器拥有集群中所有机器最高编号（即ZXID最大）的事务Proposal，那么就可以保证这个新选举出来的Leader一定具有所有已经提交的提案。更为重要的是，如果让具有最高编号事务Proposal的机器来成为Leader，就可以省去Leader服务器检查Proposal的提交和丢弃工作的这一步操作了。

###### 数据同步

所有正常运行的服务器，要么成为Leader，要么成为Follower并和Leader保持同步。Leader服务器需要确保所有的Follower服务器能够接收到每一条事务Proposal，并且能够正确地将所有已经提交了的事务Proposal应用到内存数据库中去。具体的，Leader服务器会为每一个Follower服务器都准备一个队列，并将那些没有被各Follower服务器同步的事务以Proposal消息的形式逐个发送给Follower服务器，并在每一个Proposal消息后面紧接着再发送一个Commit消息，以表示该事务已经被提交。等到Follower服务器将所有其尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库中后，Leader服务器就会将该Follower服务器加入到真正的可用Follower列表中， 并开始之后的其他流程。

ZAB协议是如何处理那些需要被丢弃的事务Proposal的：在ZAB协议的事务编号ZXID设计中，ZXID是一个64位的 数字，其中低32位可以看作是一个简单的单调递增的计数器，针对客户端的每一个事务请求，Leader服务器在产生一个新的事务Proposal的时候，都会对该计数器进行加1操作；而高32位则代表了Leader周期epoch的编号，每当选举产生一个新的Leader服务器，就会从这个Leader服务器上取出其本地日志中最大事务Proposal的ZXID，并从 该ZXID中解析出对应的epoch值，然后再对其进行加1操作，之后就会以此编号作为新的epoch，并将低32位置0来开始生成新的ZXID。ZAB协议中的这一通过epoch编号来区分Leader周期变化的策略，能够有效地避免不同的Leader服务器错误地使用相同的ZXID编号提出不一样的事务Proposal的异常情况，这对于识别在Leader崩溃恢复 前后生成的Proposal非常有帮助，大大简化和提升了数据恢复流程。

基于这样的策略，当一个包含了上一个Leader周期中尚未提交过的事务Proposal的服务器启动时，其肯定无法成为Leader，因为当前集群中一定包含一个Quorum集合（半数以上），该集合中的机器一定包含了更高epoch的事务Proposal，因此这台机器的事务Proposal肯定不是最高，也就无法成为Leader了。当这台机器加入到集群中，以Follower角色连接上Leader服务器之后，Leader服务器会根据自己服务器上最后被提交的Proposal来和Follower服务器的Proposal进行比对，比对的结果当然是Leader会要求Follower进行一个回退操作——回退到一个确实已经被集群中过半机器提交的最新的事务Proposal。（S1连上集群后需要丢弃P3。）

##### 4.2.3 深入ZAB协议

###### 算法描述

整个ZAB协议主要包括消息广播和崩溃恢复两个过程，进一步可以细分为三个阶段，分别是发现(Discovery)，同步(Synchronization)和广播(Broadcast)阶段。组成ZAB协议的每一个分布式进程，会循环地执行这三个阶段，我们将这样一个循环称为一个主进程周期。

| 术语名      | 说 明                                                        |
| ----------- | ------------------------------------------------------------ |
| $F._p$      | Follower f处理过的最后一个事务Proposal                       |
| $F._{zxid}$ | Follower f处理过的历史事务Proposal中最后一个事务Proposal的事务标识ZXID |
| $h_f$       | 每一个Follower f通常都已经处理(接受)了不少事务Proposal，并且会有一个针对已经处理过的事务的集合，将其表示为$h_f$，表示Follower f已经处理过的事务序列 |
| $I_e$       | 初始化历史记录，在某一个主进程周期epoch e中，当准Leader完成阶段一之后， 此时它的$h_f$就被标记为$I_e$。 |

###### 阶段一：发现

阶段一主要就是Leader选举过程，用于在多个分布式进程中选举出主进程，准Leader L 和Follower F的工作流程分别如下。

- 步骤F.1.1 Follower F将自己最后接受的事务Proposal的epoch值CEPOCH(F.p)发送给准 Leader L。

- 步骤L.1.1 当接收到来自过半Follower的CEPOCH(F.p)消息后，准Leader L会生成NEWEPOCH(e‘)消息给这些过半的Follower。关于这个epoch值e’，准Leader L会从所有接收到的CEPOCH(F.p)消息中选取出最大的epoch值，然后对其进行加1操作，即为e‘。

- 步骤F.1.2 当Follower接收到来自准Leader L的NEWEPOCH(e')消息后，如果其检测到当前的CEPOCH(F.p)值小于e’，那么就会将CEPOCH(F.p)赋值为e‘，同时向这个准Leader L反馈Ack消息。在这个反馈消息(ACK-E(F.p, hf)) 中，包含了当前该Follower的epoch CEPOCH(F.p)，以及该Follower的历史事务Proposal集合：hf。

当Leader L接收到来自过半Follower的确认消息Ack之后，Leader L就会从这过半服务器中选取出一个Follower F,并使用其作为初始化事务集合Ie’。

关于这个Follower F的选取，对于Quorum中其他任意一个Follower F‘，F需要满足以下两个条件中的一个：

- CEPOCH (F'.p) < CEPOCH (F.p)（F的leader周期更大）

- (CEPOCH (F’.p) = CEPOCH (F.p)) & ( F‘.zxid < F.zxid 或 F'.zxid = F.zxid)（周期相同但F的事务编号更大）

至此，ZAB协议完成阶段一的工作流程。

###### 阶段二：同步

在完成发现流程之后，就进入了同步阶段。在这一阶段中，Leader L和Follower F的工作流程分别如下。

- 步骤L.2.1 Leader L会将e’和Ie‘以NEWLEADER(e,le’)消息的形式发送给所有Quorum中的Follower。

- 步骤F.2.1 当Follower接收到来自Leader L的NEWLEADER(e’,le’)消息后，如果 Follower发现CEPOCH (F.p)≠e‘ ，那么直接进入下一轮循环，因为此时Follower发现自己还在上一轮，或者更上轮，无法参与本轮的同步。

  如果CEPOCH (F.p) = e’，那么Follower就会执行事务应用操作。具体的，对于每一个事务 Proposal：<v,z> ∈ Ie‘，Follower 都会接受 <e',< v,z>>。最后，Follower会反馈给Leader，表明自己已经接受并处理了所有Ie‘中的事务 Proposal。

- 步骤L.2.2 当Leader接收到来自过半Follower针对NEWLEADER(e‘,le’)的反馈消息后，就会向所有的Follower发送Commit消息。至此Leader完成阶段二。

- 步骤F.2.2 当Follower收到来自Leader的Commit消息后，就会依次处理并提交所有在le’中未处理的事务。至此Follower完成阶段二。

###### 阶段三：广播

完成同步阶段之后，ZAB协议就可以正式开始接收客户端新的事务请求，并进行消息广播流程。

- 步骤L.3.1 Leader L接收到客户端新的事务请求后，会生成对应的事务Proposal，并根据ZXID的顺序向所有Follower发送提案<e‘,<v,z>>，其中epoch(z)=e'。

- 步骤F.3.1 Follower根据消息接收的先后次序来处理这些来自Leader的事务Proposal，并将他们追加到hf中去，之后再反馈给Leader。

- 步骤L.3.1 当Leader接收到来自过半Follower针对事务Proposal < e',< v,z >> 的Ack消息后，就会发送Commit<e’,<v,z>>消息给所有的Follower，要求它们进行事务的提交。

- 步骤F.3.2 当Follower F接收到来自Leader的Commit < e',< v,z >>消息后，就会开始提交事务Proposal <e‘,<v,z>>。需要注意的是，此时该Follower F必定已经提交了事务 Proposal <v',z'>，其中 <v',z'>∈hf，z'< z。

<img src="img/Zookeeper/11.png" style="zoom: 67%;" />

整个过程中各进程之间的消息收发情况，各消息说明依次如下：

- CEPOCH：Follower进程向准Leader发送自己处理过的最后一个事务Proposal的epoch值。

- NEWEPOCH：准Leader进程根据接收的各进程的epoch，来生成新一轮周期的epoch值。

- ACK-E：Follower进程反馈准Leader进程发来的NEWEPOCH消息。

- NEWLEADER：准Leader进程确立自己的领导地位，并发送NEWLEADER消息给各进程。

- ACK-LD：Follower进程反馈Leader进程发来的NEWLEADER消息。

- COMMIT-LD：要求Follower进程提交相应的历史事务Proposal。

- PROPOSE：Leader进程生成一个针对客户端事务请求的Proposal。

- ACK：Follower进程反馈Leader进程发来的PROPOSAL消息。

- COMMIT：Leader发送COMMIT消息，要求所有进程提交事务PROPOSE。

在正常运行过程中，ZAB协议会一直运行于阶段三来反复地进行消息广播流程。如果出现Leader崩溃或其他原因导致Leader缺失，那么此时ZAB协议会再次进入阶段一，重新选举新的Leader。

###### 运行分析

在ZAB协议的设计中，每一个进程都有可能处于以下三种状态之一。

- LOOKING：Leader选举阶段

- FOLLOWING：Follower服务器和Leader保持同步状态

- LEADING：Leader服务器作为主进程领导状态

组成ZAB协议的所有进程启动的时候，其初始化状态都是LOOKING状态，此时进程组中不存在Leader。所有处于这种状态的进程，都会试图去选举出一个新的Leader。随后，如果进程发现已经选举出新的Leader了，那么它就会马上切换到FOLLOWING状态，并开始和Leader保持同步。这里，我们将处于FOLLOWING状态的进程称为Follower，将处于LEADING状态的进程称为Leader。当检测出Leader已经崩溃或者是放弃了领导地位时，其余的Follower进程就会转换到LOOKING状态，并开始进行新一轮的Leader选举。因此在ZAB协议运行过程中，每个进程都会在LEADING、FOLLOWING和LOOKING状态之间不断地转换。

Leader的选举过程发生在前面两个阶段。只有在完成了阶段二，即完成各进程之间的数据同步之后，准Leader进程才能真正成为新的主进程周期中的Leader。具体的，我们将一个可用的Leader定义如下：

- 如果一个准Leader Le接收到来自过半的Follower进程针对Le的NEWLEADER (e,Ie)反馈消息，那么Le就成为了周期e的Leader。

完成Leader选举以及数据同步之后，ZAB协议就进入了原子广播阶段。在这一阶段中，Leader会以队列的形式为每一个与自己保持同步的Follower创建一个操作队列。同一时刻，一个Follower只能和一个Leader保持同步，Leader进程与所有的Follower进程之间都通过心跳检测机制来感知彼此的情况。如果Leader能够在超时时间内正常收到心跳检测，那么Follower就会一直与该Leader保持连接。而如果在指定的超时时间内Leader无法从过半的Follower进程那里接收到心跳检测，或者是TCP连接本身断开了，那么Leader就会终止对当前周期的领导，并转换到LOOKING状态，所有的Follower也会选择放弃这个Leader，同时转换到LOOKING状态。之后，所有进程就会开始新一轮的Leader选举，并在选举产生新的Leader之后开始新一轮的主进程周期。

### 第5章 使用 ZooKeeper

#### 5.1 部署与运行

##### 5.1.2 集群与单机

###### 集群模式

1. 准备Java运行环境。确保你已经安装了 Java 1.6或更高版本的JDK。

2. 下载ZooKeeper安装包。解压到一个目录，在下文中使用%ZK_HOME%代表该目录。

3. 配置文件zoo.cfg。

   初次使用ZooKeeper，需要将%ZK_HOME%/conf目录下的zoo_sample.cfg文件重命名为zoo.cfg，并且按照如下代码进行简单配置即可：

   ```
   tickTime=2000
   dataDir=/var/lib/zookeeper/
   clientPort=2181
   initLimit=5
   syncLimit=2
   server.1=IP1:2888:3888
   server.2=IP2:2888:3888
   server.3=IP3:2888:3888
   ```

   - 在集群模式下，集群中的每台机器都需要感知到整个集群是由哪几台机器组成的，在配置文件中，可以按照这样的格式进行配置，每一行都代表一个机器配置：

     server.id=host:port:port

     其中，id被称为Server ID，用来标识该机器在集群中的机器序号。同时，在每台ZooKeeper机器上，我们都需要在数据目录（即dataDir参数指定的那个目录）下创建一个文件，该文件只有一行内容，并且是一个数字，即对应于每台机器的Server ID数字。

   - 在ZooKeeper的设计中，集群中所有机器上zoo.cfg文件的内容都应该是一致的。因此最好使用SVN或是GIT把此文件管理起来，确保每个机器都能共享到一份相同的配置。

   - 上面也提到了，myid文件中只有一个数字，即一个Server ID。例如，server.1 的文件内容就是“1” 。注意，请确保每个服务器的myid文件中的数字不同，并且和自己所在机器的zoo.cfg中server.id=host:port:port的id值一致。另外，id的范围是1~255。

4. 创建myid文件。

   在dataDir所配置的目录下，创建一个名为myid的文件，在该文件的第一行写上一个数字，和zoo.cfg中当前机器的编号对应上。

5. 按照相同的步骤，为其他机器都配置上zoo.cfg和myid文件。

6. 启动服务器。

   至此，所有的选项都已经基本配置完毕，可以使用%ZK_HOME%/bin目录下的zkServer.sh脚本进行服务器的启动，如下：

    ```
   $sh zkServer.sh startJMX enabled by defaultUsing config: /opt/zookeeper-3.4.3/bin/../conf/zoo.cfgStarting zookeeper ... STARTED
    ```

7. 验证服务器。

启动完成后，可以使用如下命令来检査服务器启动是否正常：

```
$ telnet 127.0.0.1 2181Trying 127.0.0.1...Connected to localhost.localdomain (127.0.0.1).Escape character is '^]'.statZookeeper version: 3.4.3-1240972, built on 02/06/2012 10:48 GMTClients:/127.0.0.1:50257[0](queued=0,recved=l,sent=0)Latency min/avg/max: 0/1/4489Received: 844689Sent: 993100Outstanding: 0Zxid: 0x600084344Mode: leaderNode count: 37
```

上面就是通过Telnet方式，使用Stat命令进行服务器启动的验证，如果出现和上面类似的输出信息，就说明服务器已经正常启动了。

###### 单机模式

单机模式只是一种特殊的集群模式而已一一只有一台机器的集群，和集群模式的部署步骤基本一致， 只是在zoo.cfg文件的配置上有些差异。由于现在我们是单机模式，整个ZooKeeper集群中只有一台机器，所以需要对zoo.cfg做如下修改：

```
tickTime=2000dataDir=/var/lib/zookeeper/clientPort=2181initLimit=5syncLimit=2server.1=IP1:2888:3888
```

和集群模式唯一的区别就在机器列表上，在单机模式的zoo.cfg文件中，只有server.1这一项。修改完这个文件后，就可以启动服务器了。同样，验证服务器启动情况，然后得到如下的输出信息：

```
$ telnet 127.0.0.1 2181Trying 127.0.0.1...Connected to localhost.localdomain (127.0.0.1).Escape character is '^]'.statZookeeper version: 3.4.3-1240972, built on 02/06/2012 10:48 GMTClients:/127.0.0.1:50257[0](queued=0,recved=l,sent=0)Latency min/avg/max: 0/1/4489Received: 844689Sent: 993100Outstanding: 0Zxid: 0x600084344Mode: standaloneNode count: 37
```

###### 伪集群模式

和其他分布式系统（如Hadoop）一样，ZooKeeper也允许你在一台机器上完成一个伪集群的搭建。

所谓的伪集群，用一句话说就是，集群所有的机器都在一台机器上，但是还是以集群的特性来对外提供服务。这种模式和集群模式非常类似，只是把zoo.cfg做了如下修改：

```
tickTime=2000dataDir=/var/lib/zookeeper/clientPort=2181initLimit=5syncLimit=2server.1=IP1:2888:3888server.2=IP1:2889:3889server.3=IP1:2890:3890
```

##### 5.1.3 运行服务

###### 启动服务

首先我们来看下如何启动ZooKeeper服务。常见的启动方式有两种。

- Java命令行

  使用Java命令来运行JAR包，具体方法是在 ZooKeeper 3.4.3发行版本的%ZK_HOME%目录下执行如下命令：

  ```
  $java -cp zookeeper-3.4.3.jar:lib/ slf4j-api-1.6.1.jar:lib/slf4j-log4j12-1.6.1.jar:lib/log4j-1.2.15.jar:conf org.apache.zookeeper.server.quorum.QuorumPeerMain conf/zoo.cfg
  ```

  通过运行上面这个命令，ZooKeeper的主入口QuorumPeerMain类就会启动ZooKeeper服务器，同时，随着ZooKeeper服务的启动，其内部的JMX也会被启动，方便管理员在JMX管理控制台上进行一些对ZooKeeper的监控与操作。

- 使用ZooKeeper自带的启动脚本来启动ZooKeeper

  | 脚 本     | 说 明                                                 |
  | --------- | ----------------------------------------------------- |
  | zkCleanup | 清理ZooKeeper历史数据，包括事务日志文件和快照数据文件 |
  | zkCli     | ZooKeeper的一个简易客户端                             |
  | zkEnv     | 设置ZooKeeper的环境变量                               |
  | zkServer  | ZooKeeper服务器的启动、停止和重启脚本                 |

###### 停止服务

停止ZooKeeper服务最常用的方法就是使用上面介绍的zkServer脚本的Stop命令来完成，如下：

```
$ sh zkServer.sh stopJMX enabled by defaultUsing config: /opt/zookeeper-3.4.3/bin/../conf/zoo.cfgStopping zookeeper ... STOPPED
```

###### 常见异常

在启动的时候，通常会碰到一些异常。

- 端口被占用：java.net.BindException: **Address already in use**

- 磁盘没有剩余空间：java.io.lOException: **No space left on device**

- 无法找到myid文件：ERROR [main:QuorumPeerMain@85] **Invalid config, exiting abnormally**

  Caused by: java.lang.IllegalArgumentException: **/tmp/zookeeper/myid file is missing**

- 集群中其他机器Leader选举端口未开：

  WARN [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@368]

  **Cannot open channel to 2 at election address /122.228.242.241:3888**

#### 5.2 客户端脚本

进入ZooKeeper的bin目录之后，直接执行如下命令：

```
$ sh zkCli.sh
```

当看到如下输出信息时，表示已经成功连接上本地的ZooKeeper服务器了：

```
WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0]
```

注意，上面的命令没有显式地指定ZooKeeper服务器地址，那么默认是连接本地的ZooKeeper服务器。如果希望连接指定的ZooKeeper服务器，可以通过如下方式实现：

```
$ sh zkCli.sh -server ip:port
```

##### 5.2.1 创建

使用create命令，可以创建一个ZooKeeper节点。用法如下：

```
create [-s] [-e] path data acl
```

其中，-s或-e分别指定节点特性：顺序或临时节点。默认情况下，即不添加-s或-e参数创建的是持久节点。acl它是用来进行权限控制的，缺省情况下，不做任何权限控制。

执行如下命令：

```
[zk: localhost:2181(CONNECTED) 0] create /zk-book 123 create /zk-book
```

执行完上面的命令，就在ZooKeeper的根节点下创建了一个叫作/zk-book的节点，并且节点的数据内容是“123”。

##### 5.2.2 读取

与读取相关的命令包括ls命令和set命令。

###### ls

使用ls命令，可以列出ZooKeeper指定节点下的所有子节点。当然，这个命令只能看到指定节点下第一级的所有子节点。用法如下：

```
ls path [watch]
```

其中，path表示的是指定数据节点的节点路径。

执行如下命令：

```
[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper]
```

第一次部署的ZooKeeper集群，默认在根节点”/“下面有一个叫作/zookeeper的保留节点。

###### get

使用get命令，可以获取ZooKeeper指定节点的数据内容和属性信息。用法如下：

```
get path [watch]
```

执行如下命令：

```
[zk: localhost:2181(C0NNECTED) 0] get /zk-book123cZxid = Oxactime = Thu Jun 20 10:35:00 UTC 2013mZxid = Oxamtime = Thu Jun 20 10:35:00 UTC 2013pZxid = Oxaeversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 0
```

从上面的输出信息中，我们可以看到，第一行是节点/zk-book的数据内容，其他几行则是创建该节点的事务ID (cZxid)、最后一次更新该节点的事务ID (mZxid)和最后一次更新该节点的时间(mtime)等属性信息。

##### 5.2.3 更新

使用set命令，可以更新指定节点的数据内容。用法如下：

```
set path data [version]
```

其中，data就是要更新的新内容。注意，set命令后面还有一个version参数，在 ZooKeeper中，节点的数据是有版本概念的，这个参数用于指定本次更新操作是基于ZNode的哪一个数据版本进行的。

执行如下命令：

```
[zk: localhost:2181(CONNECTED) 3] set /zk-book 456cZxid = Oxactime = Thu Jun 20 10:35:00 UTC 2013mZxid = 0x11mtime = Fri Jun 21 01:05:49 UTC 2013pZxid = 0xaeversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 0
```

执行完以上命令后，节点/zk-book的数据内容就已经被更新成“456” 了。在输出信息中，dataVersion的值由原来的0变成了1，这是因为刚才的更新操作导致该节点的数据版本也发生了变更。

##### 5.2.4 删除

使用delete命令，可以删除ZooKeeepr上的指定节点。用法如下：

```
delete path [version]
```

此命令中的version参数和set命令中的version参数的作用是一致的。

执行如下命令：

```
[zk: localhost:2181(CONNECTED) 2] delete /zk-book
```

执行完以上命令后，就可以把/zk-book这个节点成功删除了。但是这里要注意的一点是，要想删除某一个指定节点，该节点必须没有子节点存在。

#### 5.3 Java客户端API使用

ZooKeeper作为一个分布式服务框架，主要用来解决分布式数据一致性问题，它提供了简单的分布式原语，并且对多种编程语言提供了 API。

##### 5.3.1 创建会话

客户端可以通过创建一个ZooKeeper(org.apache.zookeeper.ZooKeeper)实例来连接ZooKeeper服务器。ZooKeeper的4种构造方法如下。

```java
ZooKeeper(String connectstring, int sessionTimeout, Watcher watcher);ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, booleancanBeReadOnly);ZooKeeper(String connectstring, int sessionTimeout, Watcher watcher, long sessionId, byte[] sessionPasswd);ZooKeeper(String connectstring, int sessionTimeout, Watcher watcher, long sessionId, byte[] sessionPasswd, boolean canBeReadOnly);
```

| 参数名                     | 说 明                                                        |
| -------------------------- | ------------------------------------------------------------ |
| connectstring              | **指ZooKeeper服务器列表，由英文状态逗号分开的host:port字符串组成**， 每一个都代表一台ZooKeeper机器，例如，192.168.1.1:2181，192.168.1.2:2181，192.168.1.3:2181，这样就为客户端指定了三台服务器地址。另外，**也可以在connectString中设置客户端连接上ZooKeeper后的根目录**，方法是在host:port字符串之后添加上这个根目录，例如，192.168.1.1: 2181，192.168.1.2:2181, 192.168.1.3:2181/zk- book，这样就指定了该客户端连接上ZooKeeper服务器之后，所有对ZooKeeper的操作，都会基于这个根目录。例如，客户端对/foo/bar的操作，都会指向节点/zk-book/foo/bar一一这个目录也叫Chroot，即客户端隔离命名空间。 |
| sessionTimeout             | **指会话的超时时间，是一个以“毫秒”为单位的整型值。**在ZooKeeper中有会话的概念、在一个会话周期内，ZooKeeper客户端和服务器之间会通过心跳检测机制来维持会话的有效性，一旦在sessionTimeout时间内没有进行有效的心跳检测，会话就会失效。 |
| watcher                    | ZooKeeper允许客户端在构造方法中传入一个接口Watcher(org.apache.zookeeper. Watcher)的实现类对象来作为默认的**Watcher事件通知处理器**。当然，该参数可以设置为null以表明不需要设置默认的 Watcher处理器。 |
| canBeReadOnly              | **用于标识当前会话是否支持"read-only (只读)”模式**。默认情况下，在ZooKeeper集群中，一个机器如果和集群中过半及以上机器失去了网络连接，那么这个机器将不再处理客户端请求(包括读写请求)。但是在某些使用场景下，当ZooKeeper服务器发生此类故障的时候，我们还是希望ZooKeeper服务器能够提供读服务(当然写服务肯定无法提供)一一这就是ZooKeeper的“read-only”模式。 |
| sessionId 和 sessionPasswd | **分别代表会话ID和会话秘钥。**这两个参数能够唯一确定一个会话，同时客户端使用这两个参数可以**实现客户端会话复用，从而达到恢复会话的效果。**具体使用方法是，第一次连接上ZooKeeper服务器时，通过调用ZooKeeper对象实例的以下两个接口，即可获得当前会话的ID和秘钥：long getSessionId() 和 byte[] getSessionPasswd() 获取到这两个参数值之后，就可以在下次创建ZooKeeper对象实例的时候传入构造方法了。 |

注意，ZooKeeper客户端和服务端会话的建立是一个异步的过程，也就是说在程序中，构造方法会在处理完客户端初始化工作后立即返回，在大多数情况下，此时并没有真正建立好一个可用的会话，在会话的生命周期中处于“CONNECTING”的状态。

当该会话真正创建完毕后，ZooKeeper服务端会向会话对应的客户端发送一个事件通知，以告知客户端，客户端只有在获取这个通知之后，才算真正建立了会话。

###### 创建一个最基本的**ZooKeeper**会话实例

```java
package book.chapter05.$5_3_1;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.Keeperstate;import org.apache.zookeeper.ZooKeeper;public class ZooKeeper_Constructor_Usage_Simple implements Watcher { 	private static CountDownLatch connectedSemaphore = new CountDownLatch(1);	public static void main(String[] args) throws Exception{	ZooKeeper zookeeper =		new ZooKeeper("domain1.book.zookeeper:2181",			5000, //			new ZooKeeper_Constructor_Usage_Simple());	System.out.printin(zookeeper.getState());    try {    	connectedSemaphore.await();    } catch (InterruptedException e) {}    Systern.out.printIn("ZooKeeper session established.");}	/**实现了 Watcher 接口，重写了 process方法，该方法负责处理来自ZooKeeper服务端的Watcher通知，在	 *收到服务端发来的SyncConnected事件之后，解除主程序在CountDownLatch上的等待阻塞。至此，客户端会		*话创建完毕。*/	public void process(WatchedEvent event) {		System.out.println("Receive watched event： " + event);		if (Keeperstate.SyncConnected == event.getState()) { 										connectedSemaphore.countDown();		}	}}
```

###### 创建一个复用sessionld和sessionPasswd的**ZooKeeper**对象实例

```java
package book.chapter05.$5_3_1;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.Keeperstate;import org.apache.zookeeper.ZooKeeper;public class ZooKeeper_Constructor_Usage_With_SID_PASSWD implements Watcher { 	private static CountDownLatch connectedSemaphore = new CountDownLatch(1); 	public static void main(String[] args) throws Exception{		ZooKeeper zookeeper = new ZooKeeper("domain1.book.zookeeper:2181",        	5000, //			new ZooKeeper_Constructor_Usage_With_SID_PASSWD()); 					connectedSemaphore.await();        long sessionId = zookeeper.getSessionId();        byte[] passwd = zookeeper.getSessionPasswd();		//Use illegal sessionId and sessionPasswd 		zookeeper = new ZooKeeper("domain1.book.zookeeper:2181",        	5000, //         	new ZooKeeper_Constructor_Usage_With_SID_PASSWD(),//         	1l,//			"test".getBytes());		//Use correct sessionId and sessionPassWd		zookeeper = new ZooKeeper("domain1.book.zookeeper:2181",			5000, //			new ZooKeeper_Constructor_Usage_With_SID_PASSWD()，// 			sessionId,// 			passwd);		Thread.sleep(Integer.MAX_VALUE);	}	public void process(WatchedEvent event) {		System.out.println("Receive watched event: " + event);		if (Keeperstate.SyncConnected == event.getState()) ( 				connectedSemaphore.countDown();		}	}}
```

##### 5.3.2 创建节点

客户端可以通过ZooKeeper的API来创建一个数据节点，有如下两个接口：

```java
String create(final String path, byte data[], List<ACL> acl, CreateMode createMode)//同步接口void create(final String path, byte data[], List<ACL> acl, CreateMode createMode, Stringcallback cb, Object ctx)//异步接口
```

| 参数名     | 说 明                                                        |
| ---------- | ------------------------------------------------------------ |
| path       | **需要创建的数据节点的节点路径**，例如，/zk-book/foo         |
| data[]     | 一个字节数组，是**节点创建后的初始内容**                     |
| acl        | **节点的ACL策略**                                            |
| createMode | **节点类型**，是一个枚举类型，通常有4种可选的节点类型：持久(PERSISTENT)，持久顺序(PERSISTENT_SEQUENTIAL)，临时(EPHEMERAL)，临时顺序(EPHEMERAL_SEQUENTIAL) |
| cb         | 注册一个**异步回调函数**。开发人员需要实现StringCallback接口，主要是对下面这个方法的重写：void processResult(int rc, String path, Object ctx, String name)，**当服务端节点创建完毕后，ZooKeeper客户端就会自动调用这个方法**，这样就可以处理相关的业务逻辑了 |
| ctx        | **用于传递一个对象，可以在回调方法执行的时候使用**，通常是放一个上下文(Context)信息 |

无论是同步还是异步接口，ZooKeeper都不支持递归创建，即无法在父节点不存在的情况下创建一个子节点。另外，如果一个节点已经存在了，那么创建同名节点的时候，会抛出NodeExistsException异常。

目前，ZooKeeper的节点内容只支持字节数组(byte[])类型，也就是说，ZooKeeper 不负责为节点内容进行序列化，开发人员需要自己使用序列化工具将节点内容进行序列化和反序列化。

###### 使用同步API创建一个节点

```java
package book.chapter05.$5_3_2;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class ZooKeeper_Create_API_Sync_Usage implements Watcher {	private static CountDownLatch connectedSemaphore = new CountDownLatch(1);	public static void main(String[] args) throws Exception{ 		ZooKeeper zookeeper = new ZooKeeper("domain1.book.zookeeper:2181",        	5000, //            new ZooKeeper_Create_API_Sync_Usage());		connectedSemaphore.await();		String path1 = zookeeper.create("/zk-test-ephemeral-",			"".getBytes(),			Ids.OPEN_ACL_UNSAFE,            CreateMode.EPHEMERAL);         System.out.printin("Success create znode: " + path1);        		String path2 = zookeeper.create("/zk-test-ephemeral-",			"".getBytes(),			Ids.OPEN_ACL_UNSAFE,			CreateMode.EPHEMERAL_SEQUENTIAL);		System.out.printin("Success create znode: " + path2);	}	public void process(WatchedEvent event) {		if (KeeperState.SyncConnected = event.getState()) { 			connectedSemaphore.countDown();		}	)}
```

###### 使用异步API创建一个节点

```java
import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class ZooKeeper_Create_API_ASync_llsage implements Watcher {	private static CountDownLatch connectedSemaphore = new CountDownLatch(1);	public static void main(String[] args) throws Exception{		ZooKeeper zookeeper = new ZooKeeper("domain1.book.zookeeper:2181",        	5000, //         	new ZooKeeper_Create_API_ASync_Usage());		connectedSemaphore.await();		zookeeper.create("/zk-test-ephemeral-",        	"".getBytes(),            Ids.OPEN_ACL_UNSAFE,            CreateMode.EPHEMERAL,            new IStringCallback(),            "I am context.");				zookeeper.create("/zk-test-ephemeral-", 			"".getBytes(), 			Ids.OPEN_ACL_UNSAFE, 			CreateMode.EPHEMERAL, 			new IStringCallback(), 			"I am context.");				zookeeper.create("/zk-test-ephemeral-",        	"".getBytes(),			Ids.OPEN_ACL_UNSAFE,            CreateMode.EPHEMERAL_SEQUENTIAL,            new IStringCallback(),            "I am context.");		Thread.sleep(Integer.MAX_VALUE);	}	public void process(WatchedEvent event) {		if (Keeperstate.SyncConnected = event.getState()) { 			connectedSemaphore.countDown();		}	}}class IStringCallback implements AsyncCallback.StringCallback{	public void processResult(int rc, String path, Object ctx, String name) { 					System.out.println("Create path result: [" + rc + ", " + path + ", " + ctx + ", real path name: " + name);	}}
```

| 参数名 | 说 明                                                        |
| ------ | ------------------------------------------------------------ |
| rc     | Result Code，**服务端响应码**。客户端可以从这个响应码中识别出API调用的结果，常见的响应码如下。0(Ok)：接口调用成功。-4(ConnectionLoss)：客户端和服务端连接已断开。-110(NodeExists)：指定节点已存在。-112(SessionExpired)：会话已过期。 |
| path   | 接口调用时**传入API的数据节点的节点路径**参数值              |
| ctx    | 接口调用时**传入API的Ctx参数值**                             |
| name   | **实际在服务端创建的节点名。**在清单5-5中，第三次创建节点时，由于创建的节点类型是顺序节点，因此在服务端没有真正创建好顺序节点之前，客户端无法知道节点的完整节点路径。于是，在回调方法中，服务端会返回这个数据节点的完整节点路径。 |

##### 5.3.3 删除节点

客户端可以通过ZooKeeper的API来删除一个节点，有如下两个接口 ：

```java
public void delete(final String path, int version)public void delete(final String path, int version, VoidCallback cb, Object ctx)
```

| 参数名  | 说 明                                                        |
| ------- | ------------------------------------------------------------ |
| path    | 指定数据节点的节点路径，即API调用的目的是删除该节点          |
| version | 指定节点的数据版本，即表明本次删除操作是针对该数据版本进行的 |
| cb      | 注册一个异步回调函数                                         |
| ctx     | 用于传递上下文信息的对象                                     |

在ZooKeeper中，只允许删除叶子节点。也就是说，如果一个节点存在至少一个子节点的话，那么该节点将无法被直接删除，必须先删除掉其所有子节点。

##### 5.3.4 读取数据

读取数据，包括子节点列表的获取和节点数据的获取。

###### getChildren

客户端可以通过ZooKeeper的API来获取一个节点的所有子节点，有如下8个接口可供使用：

```java
List<String>getChildren(final String path, Watcher watcher)List<String>getChildren(String path, boolean watch)void getChildren(final String path, Watcher watcher, ChildrenCallback cb, Object ctx)void getChildren(String path, boolean watch, ChildrenCallback cb, Object ctx) List<String>getChildren(final String path, Watcher watcher, Stat stat)List<String>getChildren(String path, boolean watch, Stat stat)void getChildren(final String path, Watcher watcher, Children2Callback cb, Object ctx)void getChildren(String path, boolean watch, Children2Callback cb, Object ctx)
```

| 参数名  | 说 明                                                        |
| ------- | ------------------------------------------------------------ |
| path    | ****指定数据节点的节点路径****，即API调用的目的是获取该节点的子节点列表 |
| watcher | 注册的Watcher。一旦在本次子节点获取之后，子节点列表发生变更的话，那么就会向客户端发送通知。该参数允许传入null。 |
| watch   | 表明是否需要注册一个Watcher。如果这个参数是true，那么ZooKeeper客户端会自动使用上文中提到的那个默认Watcher；如果是false，表明不需要注册Watcher |
| cb      | 注册一个异步回调函数                                         |
| ctx     | 用于传递上下文信息的对象                                     |
| stat    | 指定数据节点的节点状态信息。用法是在接口中传入一个旧的stat变量，该stat变量会在方法执行过程中，被来自服务端响应的新stat对象替换 |

首先，我们来看看注册Watcher。如果ZooKeeper客户端在获取到指定节点的子节点列表后，还需要订阅这个子节点列表的变化通知，那么就可以通过注册一个Watcher来实现。当有子节点被添加或是删除时，服务端就会向客户端发送一个NodeChildrenChanged(EventType.NodeChildrenChanged)类型的事件通知。需要注意的是，在服务端发送给客户端的事件通知中，是不包含最新的节点列表的，客户端必须主动重新进行获取。通常客户端在收到这个事件通知后，就可以再次获取最新的子节点列表了。

再来看看用于描述节点状态信息的对象：stat。stat对象中记录了一个节点的基本属性信息，例如节点创建时的事务ID(cZxid)、最后一次修改的事务ID(mZxid)和节点数据内容的长度(dataLength)等。有时候，我们不仅需要获取节点最新的子节点列表，还要获取这个节点最新的节点状态信息。对于这种情况，我们可以将一个旧的stat变量传入API接口，该stat变量会在方法执行过程中，被来自服务端响应的新stat对象替换。

###### 使用同步API获取子节点列表

```java
package book.chapter05.$5_3_4;import java.util.List;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class ZooKeeperGetChildren_API_Sync_Usage implements Watcher {	private static CountDownLatch connectedSemaphore = new CountDownLatch(1); 	private static ZooKeeper zk = null;	public static void main(String[] args) throws Exception{		String path = "/zk-book";		zk = new ZooKeeper("domain1.book.zookeeper:2181",			5000, //			new ZooKeeper_GetChildren_API_Sync_Usage());		connectedSemaphore.await();				zk.create(path, 			"",getBytes(),			Ids.OPEN_ACL_UNSAFE,            GreateMode.PERSISTENT);		zk.create(path+"/c1", 			"".getBytes(),			Ids.0PEN_ACL_UNSAFE,            CreateMode.EPHEMERAL);            		List<String> childrenList = zk.getChildren(path, true);		System.out.println(childrenList);		zk.create(path+"/c2", 			"".getBytes(),			Ids.OPEN_ACL_UNSAFE,            CreateMode.EPHEMERAL);		Thread.sleep(Integer.MAX_VALUE);	}	public void process(WatchedEvent event) {		if (Keeperstate.SyncConnected = event.getState()) {			if (EventType.None = event.getType() && null = event.getPath()) { 						connectedSemaphore.countDown();			} else if (event.getType() == EventType.NodeChildrenChanged) { 				try {					System.out.println(						"ReGetChild:"+zk.getChildren(event.getPath(),true));				} catch (Exception e) {}			}		}	}}
```

###### 使用异步API获取子节点列表

```JAVA
package book.chapter05.$5_3_4;import java.util.List;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType;import org.apache.zookeeper.Watcher.Event.Keeperstate;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;public class ZooKeeper_GetChildren_API_ASync_Usage implements Watcher {	private static CountDownLatch connectedSemaphore = new CountDownLatch(1); 	private static ZooKeeper zk = null;	public static void main(String[] args) throws Exception{		String path = "/zk-book";		zk = new ZooKeeper("domain1.book.zookeeper:2181",			5000, //			new ZooKeeper_GetChildren_API_ASync_Usage());		connectedSemaphore.await();				zk.create(path,        	"".getBytes(),			Ids.OPEN_ACL_UNSAFE,            CreateMode.PERSISTENT);                    zk.create(path+"/c1",        	"".getBytes(),        	Ids.OPEN_ACL_UNSAFE,            CreateMode.EPHEMERAL);        zk.getChildren(path, true, new IChildren2Callback(), null);                zk.create(path+"/c2",         	"".getBytes(),   	    	Ids.OPEN_ACL_UNSAFE,    	    	CreateMode.EPHEMERAL);        Thread.sleep(Integer.MAX_VALUE);	}	public void process(WatchedEvent event) {		if (Keeperstate.SyncConnected == event.getState()) {			if (EventType.None = event.getType() && null == event.getPath()) { 						connectedSemaphore.countDown();			} else if (event.getType() = EventType.NodeChildrenChanged) { 				try {					System.out.printin(					"ReGetChild:"+zk.getChildren(event.getPath(),true));				} catch (Exception e) {}			}		}	}}class IChildren2Callback implements AsyncCallback.Children2Callback{	public void processResult(int rc, String path, Object ctx, List<String> children, Stat stat) {	System.out.println("Get Children znode result: [response code: " + rc + ", param path: " + path + ", ctx: " + ctx + ", children list: " + children + ", stat: " + stat);	}}
```

###### getData

客户端可以通过ZooKeeper的API来获取一个节点的数据内容，有如下4个接口：

```java
byte[] getData(final String path, Watcher watcher, Stat stat)byte[] getData(String path, boolean watch, Stat stat)void getData(final String path, Watcher watcher, DataCallback cb, Object ctx)void getData(String path, boolean watch, DataCallback cb, Object ctx)
```

| 参数名  |                                                              |
| ------- | ------------------------------------------------------------ |
| path    | 指定数据节点的节点路径，即API调用的目的是获取该节点的数据内容 |
| watcher | 注册的Watcher，一旦之后节点内容有变更，就会向客户端发送通知。该参数允许传入null。 |
| stat    | 指定数据节点的节点状态信息。用法是在接口中传入一个旧的stat变量，该stat变量会在方法执行过程中，被来自服务端响应的新stat对象替换。 |
| watch   | 表明是否需要注册一个Watcher。如果这个参数是true，那么ZooKeeper客户端会自动使用上文中提到的那个默认Watcher；如果是false，表明不需要注册Watcher。 |
| cb      | 注册—个异步回调函数。                                        |
| ctx     | 用于传递上下文信息的对象。                                   |

###### 使用同步API获取节点数据内容

```java
package book.chapter05.$5_3_4:import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.CreateMode;import org,apochc.zookeeper.WatchedCvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType;import org.apache.zookeeper.Watcher.Event.Keeperstate;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;public class GetData_API_Sync_Usage implements Watcher {	private static CountDownLatch connectedSemaphore = new CountDownLatch(1); 	private static ZooKeeper zk = null;	private static Stat stat = new Stat();	public static void main(String[] args) throws Exception {		String path = "/zk-book";		zk = new ZooKeeper("domain1.book.zookeeper:2181",			5000, //			new GetData_API_Sync_Usage());		connectedSemaphore.await();				zk.create(path,        	"123".getBytes(),            Ids.OPEN_ACL_UNSAFE,            GreateMode.EPHEMERAL);        System.out.printIn(new String(zk.getData(path, true, stat)));        System.out.println(stat.getCzxid()+","+stat.getMzxid()+","+stat.getVersion());                zk.setData(path, "123".getBytes(), -1);        Thread.sleep(Integer.MAX_VALUE);	}    public void process(WatchedEvent event) {        if (Keeperstate.SyncConnected = event.getState()) (	        if (EventType.None == event.getType() && null == event.getPath()) { 	        	connectedSemaphore.countDown();            } else if (event.getType() == EventType.NodeDataChanged) {                 try {                   System.out.printIn(new String(zk.getData(event.getPath(), true, stat)));                    System.out.println(stat.getCzxld()+","+                        stat.getMzxid()+","+                        stat.getVersion());                } catch (Exception e) {}            }        }	}}
```

###### 使用异步API获取节点数据内容

```java
package book.chapter05.$5_3_4;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.AsyncCalIback;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WauhedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType;import org.apache.zookeeper.Watcher,Event.Keeperstate：import org.apache.zookeeper.ZooDefs.Ids： import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;public class GetData_API_ASync_Usage implements Watcher {	private static CountDownLatch connectedSemaphore = new CountDownLatch(1); 	private static ZooKeeper zk;	public static void main(String[] args) throws Exception {		String path = "/zk-book";		zk = new ZooKeeper("domain1.book.zookeeper:2181",			5000, //			new GetData_API_ASync_Usage());        connectedSemaphore.await();		zk.create(path,        	"123".getBytes(),            Ids.OPEN_ACL_UNSAFE,            CreateMode.EPHEMERAL);		zk.getData(path, true, new IDataCallback(), null);		zk.setData(path,        	"123".getBytes(),            -1);		Thread.sleep(Integer.MAX_VALUE);}	public void process(WatchedEvent event) (		if (KeeperState.SyncConnected = event.getState()) {			if (EventType.None = event.getType() && null = event.getPath()) { 				connectedSemaphore.countDown();			} else if (event.getType() = EventType.NodeDataChanged) { 				try {					zk.getData(event.getPath(), true, new IDataCallback(), null);				} catch (Exception e) {}			}		}	}}class IDataCallback implements AsyncCallback.DataCallback{	public void processResult(int rc, String path, Object ctx, byte[] data. Stat stat) {		System.out.println(rc + ", " + path + ", " + new String(data)); 				System.out.println(stat.getCzxid()+","+			stat.getMzxid()+","+			stat.getVersion());	}}
```

##### 5.3.5 更新数据

客户端可以通过ZooKeeper的API来更新一个节点的数据内容，有如下两个接口 ：

```java
Stat setData(final String path, byte[] data, int version)void setData(final String path, byte[] data, int version, StatCallback cb, Object ctx)
```

| 参数名  | 说 明                                                        |
| ------- | ------------------------------------------------------------ |
| path    | 指定数据节点的节点路径，即API调用的目的是更新该节点的数据内容 |
| data    | 一个字节数组，即需要使用该数据内容来覆盖节点现在的数据内容   |
| version | 指定节点的数据版本，即表明本次更新操作是针对该数据版本进行的 |
| cb      | 注册一个异步回调函故                                         |
| ctx     | 用于传递上下文信息的时象                                     |

###### 使用同步API更新节点数据内容

```java
package book.chapter05.$5_3_5：import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType;import org.apache.zookeeper.Watcher.Event.Keeperstate:import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper:import org.apache.zookeeper.data.Stat;public class SetData_API_Sync_Usage implements Watcher {	private static CountDownLatch connectedSemaphore = new CountDownLatch(1); 	private static ZooKeeper zk;	public static void main(String[] args) throws Exception {		String path = "/zk-book";		zk = new ZooKeeper("donain1.book.zookeeper:2181",		5000, //		new SetData_API_Sync_Usage());        connectedSemaphore.await();        zk.create(path,            "123".getBytes(),            Ids.OPEN_ACL_UNSAFE,            CreateMode.EPHEMERAL);        zk.getData(path, true, null):        Stat stat = zk.setData(path,            "456".getBytes(), -1);        System.out.println(stat.getCzxid()+","+            stat,getMzxid()+","+            stat.getVersion());        Stat stat2 = zk.setData(path,            "456".getBytes(),            stat.getVersion());        System.out.println(stat2.getCzxid()+","+            stat2.getMzxid()+","+             stat2.getVersion());        try {            zk.setData(path, "456".getBytes(), stat.getVersion());//注意用第一次的stat变量的版本号，已经过期        } catch (KeeperException e) {            System.out.println("Error: " + e.code() + "," + e.getMessage());         }        Thread.sleep(Integer.MAX_VALUE);    }    @Override     public void process(WatchedEvent event) {    	if (Keeperstate.SyncConnected = event,getState()) {    		if (EventType.None == event.getType() && null == event.getPath())) {     			connectedSemaphore.countDown();    		}    	}    }}
```

###### 使用异步API更新节点数据内容

```java
package book.chapters.$5_3_5;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.CreateMode;import org,apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType;import org.apache.zookeeper.Watcher.Event.Keeperstate;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;public class SetData_API_ASync_Usage implements Watcher (	private static CountDownLatch connectedSemaphore = new CountDownLatch(1); 	private static ZooKeeper zk;	public static void main(String[] args) throws Exception {		String path = "/zk-book";		zk = new ZooKeeper("domain1.book.zookeeper:2181",			5000, //			new SetData_API_ASync_Usage());		connectedSemaphore.await();		zk.create(path,        	"123".get8ytes(),            Ids.OPEN_ACL_UNSAFE,            CreateMode.EPHEMERAL);		zk.setData(path, 			"456".getBytes(),            -1,            new IStatCallback(),            null);		Thread.sleep(Integer.MAX_VALUE);	}	@Override	public void process(WatchedEvent event) (		if (KeeperState.SyncConnected == event.getState()) (			if (EventType.None = event.getType() && null = event.getPath()) ( 				connectedSemaphore.countDown();			}		}	}}class IStatCallback implements AsyncCallback.StatCallback{ 	public void processResult(int rc, String path, Object ctx, Stat stat) { 		if (rc == 0) {			System.out.printin("SUCCESS");		}	}}
```

##### 5.3.6 检测节点是否存在

客户端以通过ZooKeeper的API来删除一个节点，有如下4个接口:

```java
public Stat exists(final String path. Watcher watcher)public Stat exists(String path, boolean watch)public void exists(final String path, Watcher watcher, StatCallback cb, Object ctx)public void exists(String path, boolean watch, StatCallback cb, Object ctx)
```

| 参教名  | 说 明                                                        |
| ------- | ------------------------------------------------------------ |
| path    | 捐定数据节点的节点路径，即API调用的目的是检测该节点是否存在  |
| watcher | 注册的Walther。用于监听以下三类事件：节点被创建；节点被删除；节点被更新 |
| watch   | 指定是否复用ZooKeeper中默认的Watcher                         |
| cb      | 注册一个异步回调函数                                         |
| ctx     | 用于传递上下文信息的对象                                     |

###### 使用同步API检测节点是否存在

```java
package book.chapter05.$5_3_6：import java.util.concurrent.CountDownLatch; import org.apache.zookeeper.CreateMode; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType; import org.apache.zookeeper.Watcher.Event.KeeperState; import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class Exist_API_Sync_Usage implements Watcher {	private static CountDownLatch connectedSemaphore = new CountDownLatch(1); 	private static ZooKeeper zk;	public static void main(String[] args) throws Exception {		String path = "/zk-book";		zk = new ZooKeeper("domain1.book.zookeeper:2181",			5000, //			new Exist_API_Sync_Usage());		connectedSemaphore.await();				zk.exists(path, true);		zk.create(path, "".getBytes(), Ids.0PEN_ACL_UNSAFE, CreateMode.PERSISTENT);		zk.setData(path, "123".getBytes(), -1);		zk.create(path+"/c1", "".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);		zk.delete(path+"/c1", -1);		zk.delete(path, -1);		Thread.sleep(Integer.MAX_VALUE);	}	@Override	public void process(WatchedEvent event) {		try (			if (KeeperState.SyncConnected == event.getState()) (				if (EventType.None == event.getType() && null == event.getPath())					connectedSemaphore.countDown();				} else if (EventType.NodeCreated == event.getType()) { 								System.out.printIn("Node(" + event.getPath() + ")Created"); 						zk.exists(event.getPath(), true);				} else if (EventType.NodeDeleted == event.getType()) { 					System.out.println("Node(" + event.getPath() + ")Deleted"); 					zk.exists(event.getPath(), true);				} else if (EventType.NodeDataChanged == event.getType()) { 					System.out.println("Node(" + event.getPath() + ")DataChanged");					zk.exists(event.getPath(), true);		} catch (Exception e) {}	}}
```

针对节点/zk-book (初始状态，服务器上是不存在该节点的)，我们先后进行了如下操作。

1. 通过exists接口来检测是否存在指定节点，同时注册了一个Watcher。

2. 创建节点/zk-book，此时服务端马上会向客户端发送一个事件通知：NodeCreated。客户端在收到该事件通知后，**再次调用exists接口，同时注册Watcher**。

3. 更新该节点的数据。这个时候，服务端又会向客户端发送一个事件通知：NodeDataChanged。客户端在收到该事件通知后，继续调用exists接口，同时注册Watcher.

4. 创建子节点 /zk-book/c1。

5. 删除子节点/zk-book/c1。

6. 删除节点/zk-book。此时客户端会收到服务端的事件通知：NodeDeleted。

从上面6个操作步骤以及服务端对应的通知发送中，我们可以得出如下结论。

- 无论指定节点是否存在，通过调用exists接口都可以注册Watcher。

- exists接口中注册的Watcher，能够对节点创建，节点删除和节点数据更新事件进行监听。

- 对于指定节点的子节点的各种变化，都不会通知客户端。

##### 5.3.7 权限控制

为了避免存储在ZooKeeper服务器上的数据被其他进程干扰或人为操作修改，需要对 ZooKeeper上的数据访问进行权限控制(Access Control)。ZooKeeper通过设置服务器上数据节点的ACL，来控制客户端对该数据节点的访问权限：如果一个客户端符合该ACL控制，那么就可以对其进行访问，否则将无法操作。针对这样的控制机制，ZooKeeper提供了多种权限控制模式 (Scheme)，分别是world、auth、digest、ip和super。

开发人员如果要使用ZooKeeper的权限控制功能，需要在完成ZooKeeper会话创建后，给该会话添加上相关的权限信息(Authlnfo)。ZooKeeper客户端提供了相应的API接口来进行权限信息的设置，如下：

```
addAuthInfo(String scheme, byte[] auth)
```

| scheme | 权限控制模式，分别为world、auth、digest、ip和super。 |
| ------ | ---------------------------------------------------- |
| auth   | 具体的权限信息                                       |

###### 使用包含权限信息的**ZooKeeper**会话创建数据节点

```java
package book.chapter05.$5_3_7;import org.apache.zookeeper.CreateMode;Import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class AuthSample {	final static String PATH = "/zk-book-auth_tesf";	public static void main(String[] args) throws Exception {		ZooKeeper zookeeper = new ZooKeeper("domain1.book.zookeeper:2181", 50000, null);		zookeeper.addAuthlnfo("digest", "foo:true".getBytes());		zookeeper.create(PATH, "init".getBytes(), Ids.CREATOR_ALL_ACL, 					CreateMode.EPHEMERAL);		Thread.sleep(Integer.MAX_VALUE);	}}
```

###### 删除节点接口的权限控制

```java
package book.chapter05.$5_3_7;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.ZooOefs.Ids;import org.apache.zookeeper.ZooKeeper;public class AuthSample_Delete {	final static String PATH = "/zk-book-auth_tesf";	final static String PATH2 = "/zk-book-auth_test/child":	public static void main(String[] args) throws Exception (		ZooKeeper zookeeper1 = new ZooKeeper("domain1.book.zookeeper:2181", 5000, null);		zookeeper1.addAuthInfo("digest", "foo:true".getBytes());		zookeeper1.create(PATH,	"init".getBytes(), Ids.CREATOR_ALL_ACL,			CreateMode.PERSISTENT);		zookeeper1.create(PATH2, "init".getBytes(),	Ids.CREATOR_ALL_ACL,			CreateMode.EPHEMERAL);				try (			ZooKeeper zookeeper2 = new ZooKeeper("domain1.book.zookeeper:2181", 						50000, null);			zookeeper2.delete(PATH2, -1);		} catch (Exception e) {			System.out.println("删除节点失败："+ e.getMessage());				ZooKeeper zookeeper3 = new ZooKeeper("domain1.book.zookeeper:2181", 50000, null);		zookeeper3.addAuthInfo("digest", "foo:true".getBytes()); 		zookeeper3.delete(PATH2, -1);		System.out.println("成功删除节点："+ PATH2);				ZooKeeper zookeeper4 = new ZooKeeper("domain1.book.zookeeper:2181", 50000, null);		zookeeper4.delete(PATH, -1);		System.out.println(“成功删除节点："+ PATH);	}}运行程序，输出结果如下：删除节点失败：KeeperErrorCode = NoAuth for /zk-book-auth_test/child成功删除节点：/zk-book-auth_test/child成功删除节点：/zk-book-auth_test
```

第一次，我们使用没有包含权限信息的客户端会话进行数据节点删除操作，显然，程序运行过程中抛出了异常信息：KeeperErrorCode = NoAuth for /zk-book-auth_test/child。而在第二次接口调用中，由于使用了正确的权限信息，因此成功删除了数据节点。

第三次节点删除操作中，我们使用的是没有包含权限信息的客户端会活，但最终却成功删除了数据节点。从这个 例子中，我们可以看到，删除节点接口的权限控制比较特殊。当客户端对一个数据节点添加了权限信息后，对于删除操作而言，其作用范围是其子节点。也就是说，当我们对一个数据节点添加权限信息后，依然可以自由地删除这个节点，但是对于这个节点的子节点，就必须使用相应的权限信息才能够删除掉它。

#### 5.4 开源客户端

##### 5.4.2 Curator

Curator是Netflix公司开源的一套ZooKeeper客户端框架，解决了很多ZooKeeper客户端非常底层的细节开发工作，包括连接重连、反复注册Watcher和NodeExistsException异常等，目前已经成为Apache的顶级项目，是全世界范围内使用最广泛的ZooKeeper客户端之一。

###### 创建会话

1. 使用CuratorFrameworkFactory这个工厂类的两个静态方法来创建一个客户端：

   ```java
   static CuratorFramework newClient(String connectString, RetryPolicy retryPolicy);
   static CuratorFramework newtClient(String connectString,
                                      int sessionTimeoutMs, int connectionTimeoutMs,
                                      RetryPolicy retryPolicy);
   ```

2. 通过调用CuratorFramework中的start()方法来启动会话。

| 参数名              | 说明                                                         |
| ------------------- | ------------------------------------------------------------ |
| connectString       | 指ZooKeeper服务器列表。由英文状态逗号分开的host:port字符串组成。每一个都代表一台ZooKeeper机器。 |
| retryPolicy         | 重试策略。默认有四种实现，分别是Exponential Backoff Retry、RetryNTimes、RetryOneTime、RetryUntilElapsed |
| sessionTimeoutMs    | 会话超时时间，单位为毫秒，默认是60000ms                      |
| connectionTimeoutMs | 连接创建超时时间，单位为毫秒，默认是15000ms                  |

在重试策略上，Curator通过一个接口RetryPolicy来让用户实现自定义的重试策略。在RetryPolicy接口中只定义了 一个方法：

```java
boolean allowRetry(int retryCount, long elapsedTimeMs, RetrySleeper sleeper)
```

| 参数名        | 说明                                                         |
| ------------- | ------------------------------------------------------------ |
| retryCount    | 已经重试的次数，如果是第一次重试，那么该参数为0              |
| elapsedTimeMs | 从第一次重试开始已经花费的时间，单位为毫秒                   |
| sleeper       | 用于sleep指定时间，Curator建议不使用Thread.SIeep来进行sleep操作 |

- 使用Curator创建会话

  ```java
  package book.chapter05.$5_4_2;
  import org.apache.curator.RetryPolicy;
  import org.apache.curator.framework.CuratorFramework;
  import org.apache.curator.framework.CuratorFrameworkFactory;
  import org,apache.curator.retry.ExponentialBackoffRetry;
  public class Create_Session_Sample {
      public static void main(String[] args) throws Exception{
          RetryPolicy retryPolicy = new ExponentialBackoffRetry(1900, 3);
          CuratorFramework client =
              CuratorFrameworkFactory.newClient("dotnainl.book.zookeeper:2181",
                                                5000, 3000, retryPolicy);
          client.start();
          Thread.sleep(Integer.MAX_VALUE);
      }
  }
  ```

  ExponentialBackoffRetry重试策略是Curator默认提供的几种试策略之一，其构造方法如下：

  ```java
  ExponentialBackoffRetry(int baseSleepTimeMs, int maxRetries);
  ExponentialBackoffRetry(int baseSleepTimeMs, int maxRetries, int maxSleepMs);
  ```

  | 参数名          | 说明          |
  | --------------- | ------------- |
  | baseSleepTimeMs | 初始sleep时间 |
  | maxRetries      | 最大重试次数  |
  | maxSleepMs      | 最大sleep时间 |

  ExponentialBackoffRetry的重试策略设计如下：

  给定一个初始sleep时间baseSleepTimeMs，在这个基础上结合重试次数，通过以下公式计算出当前需要sleep的时间：

  ​	当前 sleep 时间=baseSleepTimeMs * Math.max(1, random.nextlnt(1 << (retryCount + 1)))

  可以看出随着重试次数的増加，计算出的sleep时间会越来越大。如果该sleep时间在maxSleepMs的范围之内，那么就使用该sleep时间，否则使用maxSleepMs。另外，maxRetries参数控制了最大重试次数，以避免无限制的重试。

- 使用Fluent风格的API接口来创建会话

  ```java
  package book.chapter05.$5_4_2;
  import org.apache.curator.RetryPolicy; 
  import org.apache.curator.framework.CuratorFramework;
  import org.apache.curator.framework.CuratorFrameworkFactory;
  import org.apache.curator.retry.ExponentialBackoffRetry;
  public class Create_Session_Sample_fluent {	
      public static void main(String[] args) throws Exception{		
          RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); 					CuratorFramework client = CuratorFrameworkFactory.builder()			
              .connectstring("domain1.book.zookeeper:2181")				
              .sessionTimeoutMs(5000)
              .retryPolicy(retryPolicy)
              .build();
          client.start();
          Thread.sleep(Integer.MAX_VALUE);
      }
  }
  ```

- 使用Curator创建含隔离命名空间的会话

  为了实现不同的ZooKeeper业务之间的隔离，往往会为毎个业务分配一个独立的命名空间，即指定一个ZooKeeper根路径。例如，下面所示的代码片段中定义了某个客户端的独立命名空间为/base，那么该客户端对ZooKeeper上数据节点的任何操作，都是基于该相对目录进行的：

  ```java
  CuratorFrameworkFactory.builder()	.connectString("domain1.book.zookeeper:2181")	.sessiontimeoutMs(5000)	.retryPolicy(retryPolicy)    .namespace("base")	.build();
  ```

###### 创建节点

```java
CuratorFramework	--public CreateBuilder create();CreateBuilder	--public ProtectACLCreateModePathAndBytesable<String>creatingParentsIfNeeded(); CreateModable	--public T withMode(CreateMode mode);PathAndBytesable<T>	--public T forPath(String path, byte[] data) throws Exception;	--public T forPath(String path) throws Exception;
```

- 创建一个节点，初始内变为空

  ```
  client.create().forPath(path)
  ```

  注意，如果没有设置节点属性，那么Curator默认创建的是持久节点，内容默认为空。

- 创建一个节点，附带初始内容

  ```
  client.create().forPath(path, "init".getBytes());
  ```

- 创建一个临时节点，初始内容为空

  ```
  client.create().withMode(CreateMode.EPHEMERAL).forPath(path);
  ```

- 创建一个临时节点，并自动递归创建父节点

  ```
  client.create()	.creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(path);
  ```

  由于在ZooKeeper中规定所有非叶子节点必须为持久节点，调用上面这个API之后，只有path参数对应的数据节点是临时步点，其父节点均为持久节点。

###### 删除节点

```
CuratorFramework	--public DeleteBuilder delete()；Versionable<T>	--public T withVersion(int version):DeleteBuilder	--public DeleteBuilderBase guaranteed()；PathAndBytesable<T>	--public T forPath(String path, byte[] data) throws Exception;	--public T forPath(String path) throws Exception;
```

- 删除一个节点

  ```
  client.delete().forPath(path);
  ```

  注意使用该接口只能删除叶子节点。

- 删除一个节点，并递归删除其所有子节点

  ```
  client.delete().deletingChildrenIfNeeded().forPath(path);
  ```

- 删除一个节点，强制指定版本进行删除

  ```
  client.delete().withVersion(version).forPath(path);
  ```

- 删除一个节点，强制保证删除

  ```
  client.delete().guaranteed().forPath(path);
  ```

  注意，guaranteed()接口是一个保障措施，只要客户端会话有效，那么Curator会在后台持续进行删除操作直到节点删除成功。

###### 读取数据

```
CuratorFramework
	--public GetDataBuilder getData();
Statable<T>
	--public T storingStatIn(Stat stat);
Pathable<T>
	--public T forPath(String path) throws Exception;
```

- 读取一个节点的数据内容

  ```
  client.getData().forPath(path);
  ```

  注意该接口调用后的返回值是byte[]。

- 读取一个节点的数据内容，同时获取到该节点的stat

  ```
  client.getData().storingStatIn(stat).forPath(path);
  ```

  Curator通过传入一个旧的Stat变量的方式来存储服务端返回的最新的节点状态信息。

###### 更新数据

```
CuratorFramework
	--public SetDataBuilder setData();
Versionable<T>
    --public T withVersiondnt version): 
PathAndBytesable<T>
    --public T forPath(String path, byte[] data) throws Exception;
    --public T forPath(String path) throws Exception;
```

- 更新一个节点的数据内容

  ```
  client.setData().forPath(path);
  ```

  调用该接口后，会返回一个stat对象。

- 更新一个节点的数据内容，强制指定版本进行更新

  ```
  client.setDataO.withVersion(version).forPath(path);
  ```

  注意，withVersion 接口就是用来实现 CAS (Compare and Swap)的，version (版本信息)通常是从一个旧的stat对象中获取到的。

###### 异步接口

Curator中引入了Backgroundcallback接口，用来处理异步接口调用之后服务端返回的结果信息，接口定义如下。

```
public interface BackgroundCallback{
	public void processResult(CuratorFramework client, CuratorEvent event)
    	throws Exception;
}
```

| 参数名 | 说明           |
| ------ | -------------- |
| client | 当前客户端实例 |
| event  | 服务端事件     |

CuratorEvent定义了ZooKeeper服务端发送到客户端的一系列事件参数，其中比较重要的有事件类型和响应码两个参数。

- 事件类型(CuratorEventType)

  getType()，代表本次拥的类型，主要有 CREATE、DELETE、EXISTS、GET_DATA、SET_DATA、CHILDREN、 SYNC、GET_ACL、WATCHED 和 CLOSING。

- 响应码(int)

  响应码用于标识事件的结果状态，所有响应码都被定义在org.apache.zookeeper.KeeperException.Code 类中。比较常见的响应码有 0(Ok)、-4(ConnectionLoss)、-110(NodeExists) 和 -112(SessionExpired)等，分别代表接口调用成功、客户端与服务端连接已断开、指定节点已存在和会话已过期等。

```
Backgroundable<T>
    --public T inBackground();
    --public T inBackground(Object context);
    --public T inBackground(BackgroundCallback callback);
    --public T inBackground(Backgroundcallback callback, Object context);
    --public T inBackground(BackgroundCallback callback, Executor executor);
    --public T inBackground(BackgroundCallback callback, Object context. Executor executor);
```

在ZooKeeper中，所有异步通知事件处理都是由EventThread这个线程来处理的——EventThread线程用于串行处理所有的事件通知。EventThread的“串行处理机制”在绝大部分应用场景下能够保证对事件处理的顺序性，但这个特性也有其弊端，就是一旦碰上一个复杂的处理单元，就会消耗过长的处理时间，从而影响对其他事件的处理。因此在上面的inBackground接口中，允许用户传入一个Executor实例，这样一来，就可以把那些比较复杂的事件处理放到一个专门的线程池中去，如Executors. newFixedThreadPool(2).

```java
package book.chapter05.$5_4_2;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.api.BackgroundCallback;
import org.apache.curator.framework.api.CuratorEvent;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.apache.zookeeper.CreateMode;

public class CreateNodeBackground_Sample {
    static String path = "/zk-book";
    static CuratorFramework client = CuratorFrameworkFactory.builder()
        .connectString("domain1.book.zookeeper:2181")
        .sessionTimeoutMs(5000)
        .retryPolicy(new ExponentialBackoffRetry(1000, 3))
        .build();
    static CountDownLatch semaphore = new CountDownLatch(2);
    static ExecutorService tp = Executors.newFixedThreadPool(2);
	
	public static void main(String!) args) throws Exception {
    	client.start();
		System.out.printIn("Main thread: " + Thread.currentThread().getName()); 
		//此处传入了自定义的Executor
		client.create().creatingParentsIfNeeded()
			.withMode(CreateMode.EPHEMERAL)
			.inBackground(new BackgroundCallback() {
				@Override
				public void processResult(CuratorFramework client, CuratorEvent event) 						throws Exception {
					System.out.printIn("event[code: " + event.getResultCode() + ", type: " + event.getType() + "]");
					System.out.printIn("Thread of processResult: " + Thread.currentThread ().getName());
					semaphore.countDown();
				}
			}, tp)
			.forPath(path, "init".getBytes());
    	//此处没有传入自定义的Executor
    	client.create().creatingParentsIfNeeded()
    		.withMode(CreateMode.EPHEMERAL)
    		.inBackground(new BackgroundCallback() {
    			@Override
	    		public void processResult(CuratorFramework client, CuratorEvent event) 						throws Exception {
				    System.out.println("event[code: " + event.getResultCode() + ", type: " + event.getType() + "]");
    				System.out.printIn("Thread of processResult: " + Thread.currentThread ().getName());
    				semaphore.countDown();
    			}
			})
			.forPath(path, "init".getBytes());
        semaphore.await();
        tp.shutdown();
    }
}
```

###### 事件监听

ZooKeeper原生支持通过注册Watcher来进行事件监听，但是其使用并不是特别方便，需要开发人员自己反复注册Watcher，比较繁琐。Curator引入了 Cache来实现对 ZooKeeper服务端事件的监听。Cache是Curator中对事件监听的包装，其对事件的监听其实可以近似看作是一个本地缓存视图和远程ZooKeeper视图的对比过程。同时Curator 能够自动为开发人员处理反夏注册监听，从而大大简化了原生API开发的繁琐过程。Cache分为两类监听类型：节点监听和子节点监听。

- NodeCache用于监听指定ZooKeeper数据节点本身的变化，其构造方法有如下两个：

  ```
  public NodeCache(CuratorFramework client. String path);
  public NodeCache(CuratorFramework client, String path, boolean dataIsCompressed);
  ```

  | 参数名           | 说明               |
  | ---------------- | ------------------ |
  | client           | Curator客户端实例  |
  | path             | 数据节点的节点路径 |
  | dataIsCompressed | 是否进行数据压缩   |

  同时，NodeCache定义了事件处理的回调接口 NodeCacheListener。

  ```
  public interface NodeCacheListener{
  	public void nodeChanged() throws Exception;
  }
  ```
  
  ```java
  package book.chapterOS.$5_4_2;
  import org.apache.curator.framework.CuratorFramework;
  import org.apache.curator.framework.CuratorFrameworkFactory;
  import org.apache.curator.framework.recipes.cache.NodeCache;
  import org.apache.curator.framework.recipes.cache.NodeCacheListener;
  import org.apache.curator.retry.ExponentialBackoffRetry;
  import org.apache.zookeeper.CreateMode;
  public class NodeCache_Sample {
  	static String path = "/zk-book/nodecache";
  	static CuratorFramework client = CuratorFrameworkFactory.builder() 
  		.connectstring("domain1.book.zookeeper:2181") 
  		.sessionTimeoutMs(5000)
  		.retryPolicy(new ExponentialBackoffRetry(1000, 3)) 
  		.build();
  	public static void main(String[] args) throws Exception {
      	ciient.start();
  		client.create()
  			.creatingParentsIfNeeded()
  			.withMode(CreateMode.EPHEMERAL)
  			.forPath(path, "init".getBytes());
  		
  		final NodeCache cache = new NodeCache(client,path,false);
          cache.start(true);
  		cache.getListenable().addListener(new NodeCacheListener() {
          	@Override
  			public void nodeChanged() throws Exception {
  				System.out.printin("Node data update, new data: " + 
  					new String(cache.getCurrentData().getData()));
  			}
  		});
  		
  		client.setData().forPath(path, "u".getBytes());
  		Thread.sleep(1000);
  		client.delete().deletingChildrenIfNeeded().forPath(path);
          Thread.sleep(Integer.MAX_VALUE);
      }
  }
  ```
  
  start方法有个boolean类型的参数，默认是false，如果设置为true，那么NodeCache在第一次启动的时候就会立刻从ZooKeeper上读取对应节点的数据内容，并保存在 Cache 中。
  
  NodeCache不仅可以用于监听数据节点的内容变更，也能监听指定节点是否存在。如果原本节点不存在，那么Cache就会在节点被创建后触发NodeCacheListener。但是，如果该数据节点被删除，那么Curator就无法触发NodeCacheListener了。
  
- PathChildrenCache用于监听指定ZooKeeper数据节点的子节点变化情况。

  PathChildrenCache有如下几个构造方法的定义:

  ```
  public PathChildrenCache(CuratorFramework client, String path, boolean cacheData); public PathChildrenCache(CuratorFramework client, String path, boolean cacheData, 		ThreadFactory threadFactory);
  public PathChildrenCache(CuratorFramework client, String path, boolean cacheData, 		boolean dataIsCompressed, ThreadFactory threadFactory);
  public PathChildrenCache(CuratorFramework client, String path, boolean cacheData, 		boolean dataIsCompressed, final ExecutorService executorService);
  putlic PathChildrenCache(CuralorFramewort client, String path, boolean cacheData,
  	boolean dataIsCompressed, final CloseableExecutorSetvice executorSenzice);
  ```

  | 参数名           | 说明                                                         |
  | ---------------- | ------------------------------------------------------------ |
  | client           | Curator 客户端实例                                           |
  | path             | 数据节点的节点路径                                           |
  | datalsCompressed | 是否进行数据压缩                                             |
  | cacheData        | 用于配置是否把节点内容缓存起来，如果true，那么客户端在收到节点列表变更的同时，也能够获取到节点的数据内容；如果配置为false，则无法获取到节点的数据内容 |
  | threadFactory    | 利用这两个参数，开发者可以通过构造一个专门的线程池来处       |
  | executorService  | 理事件通知                                                   |

  PathChildrenCache 定义了事件处理的回调接口PathChildrenCacheListener，其定义如下。

  ```
  public interface PathChildrenCacheListener{
  	public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) 			throws Exception；
  }
  ```

  PathChildrenCacheEvent 类中定义了所有的事件类型。主要包括新增子节点(CHILD_ADDED)，子节点数据变更(CHILD_UPDATED)和子节点删除(CHILD_REMOVED)三类。

  ```java
  package book.chapter05.$5_4_2:
  import org.apache.curator.framework.CuratorFramework;
  import org.apache.curator.framework.CuratorFrameworkFactory;
  import org.apache.curator.framework.recipes.cache.PathChildrenCache;
  import org.apache,curator.framework.recipes.cache.PathChildrenCache.StartMode;
  import org.apache.curator.framework.recipes.cache.PathChildrenCacheEvent;
  import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;
  import org.apache.curator.retry.ExponentialBackoffRetry;
  import org.apache.zookeeper.CreateMode;
  public class PathChildrenCache_Sample {
      static String path = "/zk-book";
      static CuratorFramework client = CuratorFrameworkFactory.builder() 						.connectstring("domain1.book.zookeeper:2181")
      	.retryPolicy(new ExponentialBackoffRetry(1000, 3))
      	.sessionTimeoutMs(5000)
      	.build();
  	public static void main(String[] args) throws Exception ( 
  		client.start();
  		PathChildrenCache cache = new PathChildrenCache(client, path, true); 				cache.start(StartMode.POST_INITIALIZED_EVENT);
  		cache.getListenable().addListener(new PathChildrenCacheListener() { 
  			public void childEvent(CuratorFramework client,
  				PathChildrenCacheEvent event) throws Exception {
  				switch (event.getType()) {
  					case CHILD_ADDED:
  						System.out.println("CHILD_ADOED," + 													event.getData().getPath());break;
  					case CHILD_UPDATED:
  						System.out.println("CHILD_UPDATED," + 
  							event.getData().getPath());break;
  					case CHILD_REMOVED:
  						System.out.println("CHILD_REMOVED," + 													event.getData().getPath());break;
  					default: break;
  		});
  		
  		client.create().withMode(CreateMode.PERSISTENT).forPath(path);
  		Thread.sleep(1000);
  		client.create().withMode(CreateMode.PERSISTENT).forPath(path+"/c1");
  		Thread.sleep(1000);
  		client.delete().forPath(path+"/c1");
  		Thread.sleep(1000);
  		client.delete().forPath(path);
  		Thread.sleep(Integer.MAX_VALUE);
  	}
  }
  ```

  对于节点/zk-book本身的变更，并没有通知到客户端。

  另外，和其他ZooKeeper客户端产品一样，Curator也无法对二级子节点逬行事件监听。也就是说，如果使用PathChildrenCache对/zk-book进行监听，那么当 /zk-book/c1/c2 节点被创建或删除的时候，是无法触发节点变更事件的。

###### Master选举

在分布式系统中，经常会碰到这样的场景：对于一个复杂的任务，仅需要从集群中选举出一台进行处理即可。诸如此类的分布式问题，我们统称为"Master选举”。借助 ZooKeeper，我们可以比较方便地实现Master选举的功能，其大体思路非常简单：**选择一个根节点，例如/master_select，多台机器同时向该节点创建一个子节点 /master_select/lock，利用ZooKeeper的特性，最终只有一台机器能够创建成功，成功的那台机器就作为Master。**

Curator也是基于这个思路，但是它将节点创建、事件监听和自动选举过程进行了封装。开发人员只需要调用简单的API即可实现Master选举。

```java
package book.chapter95.S5_4_2;
import org.apache.curator,framework,CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.leader.Leaderselector;
import org.apache.curator.framework.recipes.leader.LeaderSelectorListenerAdapter;
import org.apache.curator.retry.ExponentialBackoffRetry;
public class Recipes_MasterSelect {
	static String master_path = "/curator_recipes_master_path";
    static CuratorFramework client = CuratorFrameworkFactory.builder() 							.connectString("domain1.book.zookeeper:2181")
		.retryPolicy(new ExponentialBackoffRetry(1000, 3)).build();
	public static void main(String[] args) throws Exception {
    	client.start();
		LeaderSelector selector = new Leaderselector(client, master_path,
        	new LeaderSelectorListenerAdapter() {
				public void takeLeadership(CuratorFramework client) throws Exception {
					System.out.printin("成为Master角色");
					Thread.sleep(3000);
					System.out.println("完成Master操作，释放Master权利"); 
				} 
			});
        selector.autoRequeue();
        selector.start();
		Thread.sleep(Integer.MAX_VALUE);
	}
}
```

在创建Leaderselector实例的时候，还会传入一个监听器：LeaderselectorListenerAdapter，这需要开发人员自行实现，Curator会在成功获取Master权利的时候回调该监听器。其定义如下。

```java
public interface LeaderSelectorListener extends ConnectionStateListener{
	public void takeLeadership(CuratorFramework client) throws Exception;
}

public abstract class LeaderSelectorListenerAdapter implements LeaderSelectorListener{ 		@Override
	public void statethanged(Curatorhramework client, Connectlonbtate newState){ 
		if ((newState == Connectionstate.SUSPENDED) || (newState == Connectionstate.LOST) ){
			throw new CancelLeadershipException();
		}
	}
}
```

LeaderSelectorListener接口中最主要的方法就是takeLeadership方法，Curator会在竞争到Master后自动调用该方法，开发者可以在这个方法中实现自己的业务逻辑。需要注意的一点是，一旦执行完takeLeadership方法，Curator就会立即释放Master权利，然后重新开始新一轮的Master选举。

###### 分布式锁

在分布式环境中，为了保证数据的一致性，经常在程序的某个运行点(例如，减库存操作或流水号生成等)需要进行同步控制。

```java
package book.chapterQS.$5_4_2:
import java.text.SimpleDateFormat:
import java.util.Date;
import java-util.concurrent.CountOownLatch;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.locks.InterProcessMutex;
import org.apache.curator.retry.ExponentlalBackoffRetry;

public class Recipes_Lock (
    static String lock_path = "/curator_recipes_lock_path";
    static CuratorFramework client = CuratorFrameworkFactory.builder()
		.connectString("domain1.book.zookeeper:2181")
		.retryPolicy(new ExponentialBackoffRetry(1000, 3)).build();
	public static void main(StrIng[] args) throws Exception {
		client.start();
		final InterProcessMutex lock = new InterProcessMutex(client, lock_path); 
		final CountDownLatch down = new CountDownLatch(1);
		for(int i = 0; i < 30; i++)(
			new Thread(new Runnable() {
				public void run() (
                    try (
                    	down.await()：
                    	lock.acquire();
                    } catch (Exception e) {}
					SimpleDateFormat sdf = new SimpleDateFormat("HH:mm:ss|SSS"); 
					String orderNo = sdf.format(new Date());
					System.out.printin("生成的订单号是: " + orderNo);
					try (
						lock.release();
					} catch (Exception e) {}
				}
			}).start();
		}
		down.countDown();
	}
}
```

其核心接口如下：

```
public interface InterProcessLock
    -public void acquire() throws Exception;
    -public void release() throws Exception;
```

这两个接口分别用来实现分布式锁的获取与释放过程。

###### 分布式计数器

有了上述分布式锁实现的基础之后，我们就很容易基于其实现一个分布式计数器。分布式计数器的一个典型场景是统计系统的在线人数。基于ZooKeeper的分布式计数器的实现思路也作常简单：**指定一个ZooKeeper数据节点作为计数器，多个应用实例在分布式锁的控制下，通过更新该数据节点的内容来实现计数功能。**

Curator同样将这一系列逻辑封装在了DistributedAtomicInteger类中，从其类名我们可以看出这是一个可以在分布式环境中使用的原子整型。

```java
package book.chapter05.$5_4_2:
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.atomic.AtomicValue;
import org.apache.curator.framework.recipes.atomic.DistributedAtomicInteger;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.apache.curator.retry.RetryNTimes;

public class Recipes_DistAtomicInt {
	static String distatomicint_path = "/curator_recipes_distatondcint_path";
    static CuratorFramework client = CuratorFrameworkFactory.builder()
		.connectString("domain1.book.zookeeper:2181")
		.retryPolicylnew ExponentialBackoffRetry(1000, 3)).build();
	public static void main(String[] args ) throws Exception { 
		client.start();
		DistributedAtomicInteger atomicInteger =
			new DistributedAtomicInteger(client, distatomlcxnt_path,
				new RetryNTimes(3, 1000));
		AtomicValue<Integer> rc = atomicInteger.add(8);
		System.out.println("Result: " + rc.succeeded());
	}
}
```

###### 分布式Barrier

Barrier是一种用来控制多线程之间同步的经典方式，在JDK中也自带了CyclicBarrier实现。

```
package book.chapter05.$5_4_2:
import java.io.lOException;
import java.util.concurrent.CyclicBarrier;
import java.util.concurrent.ExecutorService；
import java.util.concurrent.Executors;
public class Recipes_CyclicBarrier {
	public static CyclicBarrier barrier = new CyclicBarrier(3);
	pubLic static void main(String[] args)throws IOException, InterruptedException {
		ExecutorService executor= Executors.newFixedThreadPooL(3);
        executor.submit(new Thread(new Runner("1号选手"))); 
        executor.submit(new Thread(new Runner("2号选手"))); 
        executor.submit(new Thread(new Runner("3号选手"))); 
        executor.shutdown();
    }
}
class Runner implements Runnable {
    private String name;
    public Runner(String name){
    	this.name = name;
    }
    pubLic void run(){
    	System.out.println(name ＋"准备好了.");
        try{
        	Recipes_CyclicBarrier.barrier.await();
        } catch (Exception e){}
    	System.out.println(name ＋"起跑!");
	]
}
```

Curator 中提供的 DistributedBarrier 就是用来实现分布式 Barrier 的。

```java
package book.chapter05.$5_4_2;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import orq.apache.curator.framework.recipes.barriers.DistributedBarrier;
import org.apache.curator.retry.ExponentialBackoffRetry;

public class Recipes_Barrier {
	static String barrier_path = "/curator_recipes_barrier_path";
    static DistributedBarrier barrier;
	pubLic static void main(String[] args) throws Exception {
		for(int i = 0; i < 5; i++){
			new Thread(new Runnable() {
				public void run() {
					try{
						CuratorFramework client = CuratorFrameworkFactory.buitder()
							.connectString("domain1.book.zookeeper:2181") 											.retryPolicy(new ExponentialBackoffRetry(1000,3)).build();
						client.start();
						barrier = new DistributedBarrier(client, barrier_path); 								System.out.printLn(Thread.currentThread().getName()+
							"号 barrier 设置");
						barrier.setBarrier();
                        barrier.wait0nBarrier();
                        System.err.println("启动...");}
                    catch (Exception e){}
                }
            }).start();
		Thread.sleep(2000);
        barrier.removeBarrier();//由主线程控制释放
    }
}
```

```java
package book.chapter05.$5_4_2;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.barriers.DistributedDoubleBarrier;
import org.apache.curator.retry.ExponentialBackoffRetry;

public class Recipes_Barrier2 {
	static String barrier_path = "/curator_recipes_barrier_path";
	pubLic static void main(String[] args) throws Exception {
		for(int i = 0; i < 5; i++){
			new Thread(new Runnable() {
				public void run() {
					try{
						CuratorFramework client = CuratorFrameworkFactory.buitder()
							.connectString("domain1.book.zookeeper:2181") 											.retryPolicy(new ExponentialBackoffRetry(1000,3)).build();
						client.start();
						DistributedDoubleBarrier barrier = 
							new DistributedDoubleBarrier(client, barrier_path, 5);
                        Thread.sleep(Math.round(Math.random()*3000));     
                        System.out.println(Thread.currentThread().getName()+
							"号进入barrier");
						barrier.enter();
                        System.err.println("启动...");
                        Thread.sleep(Math.round(Math.random()*3000));
                        barrier.leave();//进入线程达到5后自动释放
						System.out.printLn("退出...");
						} catch (Exception e){}
                }
            }).start();
    	}
    }
}
```

###### EnsurePath

EnsurePath 提供了一种能够确保数据节点存在的机制，多用于这样的业务场景中∶上层业务希望对一个数据节点进行一些操作，但是操作之前需要确保该节点存在。基于ZooKeeper 提供的原始 API 接口，为解决上述场景的问题，开发人员需要首先对该节点进行一个判断，如果该节点不存在，那么就需要创建节点。而与此同时，在分布式环境中，在 A 机器试图进行节点创建的过程中，由于并发操作的存在，另一台机器，如 B 机器，也在同时创建这个节点，于是 A机器创建的时候，可能会抛出诸如"节点已经存在"的异常。因此开发人员还必须对这些异常进行单独的处理，逻辑通常非常琐碎。

EnsurePath 正好可以用来解决这些烦人的问题，它采取了静默的节点创建方式，其内部实现就是试图创建指定节点，如果节点已经存在，那么就不进行任何操作，也不对外抛出异常，否则正常创建数据节点。

```java
package book.chapter05.$5_4_2;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.apache.curator,utils.EnsurePath;
public class EnsurePathDemo {
    static String path = "/zk-book/cl";
    static CuratorFramework client = CuratorFrameworkFactory.builder()
		.connectString("domain1.book.zookeeper:2181") 
		.sessionTimeoutMs(5000)
		.retryPolicy(new ExponentialBackoffRetry(1000,3)).build();
	public static void main(String[]args)throws Exception {(
        cLient.start();
        cLient.usingNamespace("zk-book");
        EnsurePath ensurePath = new EnsurePath(path); 						
        ensurePath.ensure(cLient.getZookeeperCLient()); 
        ensurePath.ensure(cLient.getZookeeperClient());
        EnsurePath ensurePath2 = client.newNamespaceAwareEnsurePath("/c1"); 	
        ensurePath2.ensure(client.getZookeeperClient());
    }
}
```

###### TestingServer&TestingCluster

为了便于开发人员进行ZooKeeper的开发与测试，Curator提供了一种启动简易ZooKeeper服务的方法——TestingServer。TestingServer 允许开发人员非常方便地启动一个标准的 ZooKeeper 服务器，并以此来进行一系列的单元测试。

```java
package book.chapter05.$5_4_2; 
import java.io.File;
import org.apache.curator.framework.CuratorFramework; 
import org.apache.curator.framework.CuratorFrameworkFactory; 
import org.apache.curator.retry.ExponentiaLBackoffRetry; 
import org.apache.curator.test.TestingServer;
public class TestingServer Sample {
    static String path = "/zookeeper";
	public static void main(String[] args) throws Exception {
		TestingServer server = new TestingServer(2181, new File("/home/admin/zk-book-data"));
		CuratorFramework cLient = CuratorFrameworkFactory.builder()
			.connectString(server.getConnectString()).sessionTimeoutMs(5000)
			.retryPoLicy(new ExponentialBackoffRetry(1000,3)) .build(); 
		client.start();
		System.out.printIn( cLient.getChildren(.forPath( path ));
        server.close();
    }
}
```

Curator 也提供了启动 ZooKeeper 集群的工具类。TestingCluster是一个可以模拟ZooKeeper集群环境的 Curator工具类，能够便于开发人员在本地模拟由n台机器组成的集群环境。

```java
package book.chapter65.$5 4 2;
import org.apache.curator.test.TestingCLuster;
import org.apache.curator.test.TestingZooKeeperServer;
public class TestingCluster_Sample {
	public static void main(String[] args)throws Exception { 
		TestingCluster cluster = new TestingCluster(3);
        cluster.start();
        Thread.sleep(2000);
		TestingZooKeeperServer leader = null;
		for(TestingZooKeeperServer zs:cluster.getServers()){
			System.out.print(zs.getInstanceSpec().getServerId()+"."); 		
			System.out.print(zs.getQuorumPeer().getServerState()+".");
			System.out.printIn(zs.getInstanceSpec()
				.getDataDirectory().getAbsolutePath());

            if(zs.getQuorumPeer().getServerState().equaLs("Leading")){
                leader = zs;
            }
		leader.kill();

		System.out.println( ".-After leader kill;" );
        for(TestingZooKeeperServer zs: cluster.getServers()){
	        System.out.print(zs.getInstanceSpec().getServerId()+"."); 			
	        System.out.print(zs.getQuorumPeer().getServerState()+"-");
        	System.out.printLn(zs.getInstanceSpec()
        		.getDataDirectory().getAbsolutePath());
       	}
        cluster.stop();
    }
}
运行程序，输出结果如下∶
1-following-C:\Users\nieader\AppData\Local\Temp\1404044185801-0 
2-following-C:\Users\nileader\AppData\LocaL\Temp\1404044185814-0
3-Leading-C:\Users\nileader\AppData\Local\Temp\1404044185823-0
--After leader kill;
1-leaderelection-C:\Users\nileader\AppData\Local\Temp\1404044185801-0
2-Leaderelection-C:\Users\nileader\AppData\Local\Temp\1404044185814-0
3-leaderelection-C:\Users\nileader\AppData\Local\Temp\1404844185823-0
```

### 第6章 ZooKeeper 的典型应用场景

#### 6.1典型应用场景及实现

##### 6.1.1 数据发布/订阅

数据发布/订阅（PublishSubseribe）系统，即所谓的配置中心，顾名思义就是发布者将数据发布到 ZooKeeper 的一个或一系列节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新。

发布/阅系统一般有两种设计模式，分别是推（Push）模式和拉（Pull）模式。在推模式中，服务端主动将数据更新发送给所有订阅的客户端，而拉模式则是由客户端主动发起请求来获取最新数据，通常客户端都采用定时进行轮询拉取的方式。ZooKeeper 采用的是推拉相结合的方式∶客户端向服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送 Watcher 事件通知，客户端接收到这个消息通知之后，需要主动到服务端获取最新的数据。

如果将配置信息存放到 ZooKeeper 上进行集中管理，那么通常情况下，应用在启动的时候都会主动到 ZooKeeper 服务端上进行一次配置信息的获取，同时，在指定节点上注册一个 Watcher 监听，这样一来，但凡配置信息发生变更，服务端都会实时通知到所有订阅的客户端，从而达到实时获取最新配置信息的目的。

在我们平常的应用系统开发中，经常会碰到这样的需求：系统中需要使用一些通用的配置信息，例如机器列表信息、运行时的开关配置、数据库配置信息等。这些全局配置信息通常具备以下 3 个特性：

- 数据量通常比较小。

- 数据内容在运行时会发生动态变化。
- 集群中各机器共享，配置一致。

对于这类配置信息，一般的做法通常可以选择将其存储在本地配置文件或是内存变量中。但是，一旦机器规模变大，且配置信息变更越来越频繁后，我们发现依靠现有的这两种方式解决配置管理就变得越来越困难了。我们既希望能够快速地做到全局配置信息的变更，同时希望变更成本足够小，因此我们必须寻求一种更为分布式化的解决方案。

###### 配置管理

- 配置存储

  在进行配置管理之前，首先我们需要将初始化配置存储到 ZooKeeper 上去。一般情况下，我们可以在 ZooKeeper 上选取一个数据节点用于配置的存储，例如/app1/database_config。将需要集中管理的配置信息写入到该数据节点中去。例如

  ```
  #DBCP
  dbcp.driverClassName=com.mysqL.jdbc.Driver
  dbcp.dbDBCUrl=jdbc:mysgl://1.1.1.1:3306/taokeeper 
  dbcp.characterEncoding=GBK 
  dbcp. username=xiaoming
  dbcp.password=123456 
  dbcp.maxActive=30 
  dbcp.maxIde=10 
  dbcp.maxWait=10000
  ```

- 配置获取

  集群中每台机器在启动初始化阶段，首先会从上面提到的 ZooKeeper 配置节点上读取数据库信息，同时，客户端还需要在该配置节点上注册一个数据变更的 Watcher监听，一旦发生节点数据变更，所有订阅的客户端都能够获取到数据变更通知。
- 配置变更

  在系统运行过程中。可能会出现需要进行数据库切换的情况，这个时候就需要进行配置变更。借助 ZooKeeper，我们只需要对 ZooKeeper 上配置节点的内容进行更新， ZooKeeper 就能够帮我们将数据变更的通知发送到各个客户端，每个客户端在接收到这个变更通知后，就可以重新进行最新数据的获取。

##### 6.1.2 负载均衡

负载均衡（Load Balance）是一种相当常见的计算机网络技术，用来对多个计算机（计算机集群），网络连接、CPU、磁盘驱动器或其他资源进行分配负载，以达到优化资源使用、最大化吞吐率、最小化响应时间和避免过载的目的。

###### 一种动态的 DNS 服务

通常情况下，我们可以向域名注册服务商申请域名注册，但是这种方式最大的缺陷在于只能注册有限的域名。

因此，在实际开发中，往往使用本地 HOST绑定来实现域名解析的工作。可以很容易解决域名紧张的问题，然而一旦机器规模变大后，就常常会碰到这样的情况∶我们在应用上线的时候，需要在应用的每台机器上去绑定域名，但是在机器规模相当庞大的情况下，这种做法就相当不方便。另外，如果想要临时更新城名，还需要到每个机器上去逐个进行变更，要消耗大量时间，因此完全无法保证实时性。

- 域名配置

  和配置管理一样，我们首先需要在 ZooKeeper 上创建一个节点来进行域名配置，例如 /DDNS/app1/server.app1.company1.com（以下简称"域名节点"）。

  ```
  IP 地址和端口配置
  #单个IP∶PORT 
  192.168.0.1:8080
  #多个IP∶PORT
  192.168.0.1:8080,192.168.0.2:8080
  ```

- 域名解析

  在传统的 DNS 解析中，我们都不需要关心域名的解析过程，所有这些工作都交给了操作系统的域名和 IP 地址映射机制（本地 HOST 绑定）或是专门的域名解析服务器（由域名注册服务商提供）。因此，在这点上，DDNS 方案和传统的域名解析有很大的区别——在 DDNS 中，域名的解析过程都是由每一个应用自己负责的。通常应用都会首先从域名节点中获取一份 IP地址和端口的配置，进行自行解析。同时，每个应用还会在域名节点上注册一个数据变更 Watcher 监听，以便及时收到域名变更的通知。

- 域名变更

  在运行过程中，难免会碰上域名对应的 IP 地址或是端口变更，这个时候就需要进行域名变更操作。在 DDNS 中，我们只需要对指定的域名节点进行更新操作，ZooKeeper 就会向订阅的客户端发送这个事件通知，应用在接收到这个事件通知后，就会再次进行域名配置的获取。

##### 6.1.3 命名服务

命名服务（Name Service）也是分布式系统中比较常见的一类场景。在分布式系统中，被命名的实体通常可以是集群中的机器。提供的服务地址或远程对象等——这些我们都可以统称它们为名字（Name），其中较为常见的就是一些分布式服务框架（如 RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据指定名字来获取资源的实体、服务地址和提供者的信息等。

通过调用ZooKeeper 节点创建的 API接口可以创建一个顺序节点，并且在 API返回值中会返回这个节点的完整名字。利用这个特性，我们就可以借助 ZooKeeper 来生成全局唯一的 ID 了。

<img src="img/Zookeeper/12.PNG" style="zoom: 67%;" />

对于一个任务列表的主键，使用 ZooKeeper 生成唯一ID的基本步骤：

1. 所有客户端都会根据自己的任务类型，在指定类型的任务下面通过调用create()接口来创建一个顺序节点，例如创建"job-"节点。
2. 节点创建完毕后，create()接口会返回一个完整的节点名，例如"job-00000003"。
3. 客户端拿到这个返回值后，拼接上 type 类型，例如"type2-job-0000000003"，这就可以作为一个全局唯一的ID 了。

##### 6.1.4 分布式协调/通知

分布式协调/通知服务是分布式系统中不可缺少的一个环节，是将不同的分布式组件有机结合起来的关键所在。对于一个在多台机器上部署运行的应用而言，通常需要一个协调者（Coordinator）来控制整个系统的运行流程，例如分布式事务的处理、机器间的互相协调等。同时，引入这样一个协调者，便于将分布式协调的职责从应用中分离出来，从而可以大大减少系统之间的耦合性，而且能够显著提高系统的可扩展性。

ZooKeeper 中特有的 Watcher 注册与异步通知机制，能够很好地实现分布式环境下不同机器，甚至是不同系统之间的协调与通知，从而实现对数据变更的实时处理。基于 ZooKeeper 实现分布式协调与通知功能，通常的做法是不同的客户端都对 ZooKeeper上同一个数据节点进行Watcher 注册，监听数据节点的变化（包括数据节点本身及其子节点），如果数据节点发生变化，那么所有订阅的客户端都能够接收到相应的 Watcher 通知，并做出相应的处理。

###### 一种通用的分布式系统机器间通信方式

- 心跳检测

  机器间的心跳检测机制是指在分布式环境中，不同机器之间需要检测到彼此是否在正常运行，例如 A 机器需要知道 B 机器是否正常运行。在传统的开发中，我们通常是通过主机之间是否可以相互PING通来判断，更复杂一点的话，则会通过在机器之间建立长连接，通过 TCP 连接固有的心跳检测机制来实现上层机器的心跳检测，这些确实都是一些非常常见的心跳检测方法。

  基于 ZooKeeper 的临时节点特性，可以让不同的机器都在 ZooKeeper 的一个指定节点下创建临时子节点，不同的机器之间可以根据这个临时节点来判断对应的客户端机器是否存活。通过这种方式，检测系统和被检测系统之间并不需要直接相关联，而是通过 ZooKeeper 上的某个节点进行关联，大大减少了系统耦合。

- 工作进度汇报

  在一个常见的任务分发系统中，通常任务被分发到不同的机器上执行后，需要实时地将自己的任务执行进度汇报给分发系统。这个时候就可以通过 ZooKeeper 来实现。在 ZooKeeper 上选择一个节点，每个任务客户端都在这个节点下面创建临时子节点，这样便可以实现两个功能∶

  - 通过判断临时节点是否存在来确定任务机器是否存活；

  - 各个任务机器会实时地将自己的任务执行进度写到这个临时节点上去，以便中心系统能够实时地获取到任务的执行进度。

- 系统调度

  使用 ZooKeeper，能够实现另一种系统调度模式∶ 一个分布式系统由控制台和一些客户端系统两部分组成，控制台的职责就是需要将一些指令信息发送给所有的客户端，以控制它们进行相应的业务逻辑。后台管理人员在控制台上做的一些操作，实际上就是修改了 ZooKeeper 上某些节点的数据，而 ZooKeeper 进一步把这些数据变更以事件通知的形式发送给了对应的订阅客户端。

  总之，使用 ZooKeeper 来实现分布式系统机器间的通信，不仅能省去大量底层网络通信和协议设计上重复的工作，更为重要的一点是大大降低了系统之间的耦合，能够非常方便地实现异构系统之间的灵活通信。

##### 集群管理

所谓集群管理，包括集群监控与集群控制两大块，前者侧重对集群运行时状态的收集，后者则是对集群进行操作与控制。在日常开发和运维过程中，我们经常会有类似于如下的需求。

- 希望知道当前集群中究竟有多少机器在工作。
- 对集群中每台机器的运行时状态进行数据收集。
- 对集群中机器进行上下线操作。

ZooKeeper 具有以下两大特性。

- 客户端如果对 ZooKeeper 的一个数据节点注册 Watcher 监听，那么当该数据节点的内容或是其子节点列表发生变更时，ZooKeeper 服务器就会向订阅的客户端发送变更通知。

- 对在 ZooKeeper 上创建的临时节点，一旦客户端与服务器之间的会话失效，那么该临时节点也就被自动清除。

利用 ZooKeeper 的这两大特性，就可以实现另一种集群机器存活性监控的系统。例如，监控系统在 /clusterServers 节点上注册一个 Watcher 监听，那么但凡进行动态添加机器的操作，就会在/clusterServers 节点下创建一个临时节点：/clusterServers/[Hostname]。这样一来，监控系统就能够实时检测到机器的变动情况，至于后续处理就是监控系统的业务了。

###### 分布式日志收集系统

- 注册收集器机器

  使用 ZooKeeper 来进行日志系统收集器的注册，典型做法是在 ZooKeeper 上创建一个节点作为收集器的根节点，例如/logs/collector（下文我们以"收集器节点"代表该数据节点），每个收集器机器在启动的时候，都会在收集器节点下创建自己的节点，例如/logs/collector/[Hostname]。

- 任务分发

  待所有收集器机器都创建好自己对应的节点后，系统根据收集器节点下子节点的个数，将所有日志源机器分成对应的若干组，然后将分组后的机器列表分别写到这些收集器机器创建的子节点（例如 /logs/collector/host1）上去。这样一来，每个收集器机器都能够从自己对应的收集器节点上获取日志源机器列表，进而开始进行日志收集工作。

- 状态汇报

  完成收集器机器的注册以及任务分发后，我们还要考虑到这些机器随时都有挂掉的可能。因此，针对这个问题，我们需要有一个收集器的状态汇报机制：每个收集器机器在创建完自己的专属节点后，还需要在对应的子节点上创建一个状态子节点，例如/logs/collector/host1/status，每个收集器机器都需要定期向该节点写入自己的状态信息。我们可以把这种策略看作是一种心跳检测机制，通常收集器机器都会在这个节点中写入日志收集进度信息。日志系统根据该状态子节点的最后更新时间来判断对应的收集器机器是否存活。

- 动态分配

  如果收集器机器挂掉或是扩容了，就需要动态地进行收集任务的分配。在运行过程中，日志系统始终关注着/logs/collector这个节点下所有子节点的变更，一旦检测到有收集器机器停止汇报或是有新的收集器机器加入，就要开始进行任务的重新分配。无论是针对收集器机器停止汇报还是新机器加入的情况，日志系统都需要将之前分配给该收集器的所有任务进行转移。为了解决这个问题，通常有两种做法。

  - 全局动态分配

    日志系统需要根据新的收集器机器列表，立即对所有的日志源机器重新进行一次分组，然后将其分配给剩下的收集器机器。

  - 局部动态分配

    全局动态分配方式虽然策略简单，但是存在一个问题：一个或部分收集器机器的变更，就会导致全局动态任务的分配，影响面比较大，因此风险也就比较大。所谓局部动态分配，顾名思义就是在小范围内进行任务的动态分配。在这种策略中，每个收集器机器在汇报自己日志收集状态的同时，也会把自己的负载汇报上去。请注意，这里提到的负载并不仅仅只是简单地指机器 CPU 负载（Load），而是一个对当前收集器任务执行的综合评估，这个评估算法和 ZooKeeper 本身并没有太大的关系，这里不再赘述。
    
    在这种策略中，如果一个收集器机器挂了，那么日志系统就会把之前分配给这个机器的任务重新分配到那些负载较低的机器上去。同样，如果有新的收集器机器加入，会从那些负载高的机器上转移部分任务给这个新加入的机器。

##### 6.1.6 Master 选举

在分布式系统中，Master 往往用来协调集群中其他系统单元，具有对分布式系统状态变更的决定权。例如，在一些读写分离的应用场景中，客户端的写请求往往是由 Master来处理的；而在另一些场景中，Master 则常常负责处理一些复杂的逻辑，并将处理结果同步给集群中其他系统单元。

利用 ZooKeeper 的强一致性，能够很好地保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，即 ZooKeeper 将会保证客户端无法重复创建一个已经存在的数据节点。也就是说，如果同时有多个客户端请求创建同一个节点，那么最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很容易地在分布式环境中进行 Master 选举了。

在这个系统中，首先会在 ZooKeeper 上创建一个日期节点，例如"2013-09-20"。客户端集群每天都会定时往 ZooKeeper 上创建一个临时节点，例如/master_election/2013-09-20/binding。在这个过程中，只有一个客户端能够成功创建这个节点，那么这个客户端所在的机器就成为了Master。同时，其他没有在 ZooKeeper 上成功创建节点的客户端，都会在节点/master_election/2013-09-20 上注册一个子节点变更的 Watcher，用于监控当前的 Master 机器是否存活，一旦发现当前的 Master 挂了，那么其余的客户端将会重新进行 Master 选举。

##### 6.1.7 分布式锁

分布式锁是控制分布式系统之间同步访问共享资源的一种方式。如果不同的系统或是同一个系统的不同主机之间共享了一个或一组资源，那么访问这些资源的时候，往往需要通过一些互斥手段来防止彼此之间的干扰，以保证一致性，在这种情况下，就需要使用分布式锁了。

###### 排他锁

排他锁（Exclusive Locks，简称 X 锁），又称为写锁或独占锁，是一种基本的锁类型。如果事务T1对数据对象 O1加上了排他锁，那么在整个加锁期间，只允许事务 T1对 O1进行读取和更新操作，其他任何事务都不能再对这个数据对象进行任何类型的操作——直到T1释放了排他锁。

排他锁的核心是如何保证当前有且仅有一个事务获得锁，并且锁被释放后，所有正在等待获取锁的事务都能够被通知到。

- 定义锁：通过 ZooKeeper 上的临时节点来表示一个锁，例如/exclusive_lock/lock节点就可以被定义为一个锁。

- 获取锁：在需要获取排他锁时，所有的客户端都会试图通过调用 create()接口，在/exclusive_lock节点下创建临时子节点/exclusive_lock/lock。ZooKeeper 会保证在所有的客户端中，最终只有一个客户端能够创建成功，那么就可以认为该客户端获取了锁。同时，所有没有获取到锁的客户端就需要到/exclusive_lock 节点上注册一个子节点变更的 Watcher 监听，以便实时监听到 lock 节点的变更情况。

- 释放锁：/exclusive_lock/lock 是一个临时节点，因此在以下两种情况下，都有可能释放锁。

  - 当前获取锁的客户端机器发生宕机，那么 ZooKeeper 上的这个临时节点就会被移除。
  - 正常执行完业务逻辑后，客户端就会主动将自己创建的临时节点删除。

  无论在什么情况下移除了 lock 节点，ZooKeeper 都会通知所有在/exclusive_lock 节点上注册了子节点变更 Watcher 监听的客户端。这些客户端在接收到通知后，再次重新发起分布式锁获取，即重复"获取锁"过程。

###### 共享锁

共享锁（Shared Locks，简称 S 锁），又称为读锁，同样是一种基本的锁类型。如果事务 T1对数据对象 O1加上了共享锁，那么当前事务只能对 O1进行读取操作，其他事务也只能对这个数据对象加共享锁——直到该数据对象上的所有共享锁都被释放。

共享锁和排他锁最根本的区别在于，加上排他锁后，数据对象只对一个事务可见，而加上共享锁后，数据对所有事务都可见。

- 定义锁：和排他锁一样，同样是通过 ZooKeeper 上的数据节点来表示一个锁，是一个类似于 "/shared_lock/[Hostname]-请求类型-序号"的临时顺序节点，例如/shared_lock/192.168.0.1-R-00000001，那么，这个节点就代表了一个共享锁。

- 获取锁：在需要获取共享锁时，所有客户端都会到/shared_lock 这个节点下面创建一个临时顺序节点，如果当前是读请求，那么就创建例如/shared_lock/192.168.0.1-R-00000001 的节点；如果是写请求，那么就创建例如/shared_lock/192.168.0.1-W-000000007 的节点。

- 判断读写顺序：根据共享锁的定义，不同的事务都可以同时对同一个数据对象进行读取操作，而更新操作必须在当前没有任何事务进行读写操作的情况下进行。基于这个原则，我们来看看如何通过 ZooKeeper 的节点来确定分布式读写顺序，大致可以分为如下 4个步骤。

  1. 创建完节点后，获取/shared_lock节点下的所有子节点，并对该节点注册子节点变更的Watcher 监听。

  2. 确定自己的节点序号在所有子节点中的顺序。

  3. 对于读请求∶如果没有比自己序号小的子节点，或是所有比自己序号小的子节点都是读请求，那么表明自己已经成功获取到了共享锁，同时开始执行读取逻辑。如果比自己序号小的子节点中有写请求，那么就需要进入等待。

     对于写请求∶如果自己不是序号最小的子节点，那么就需要进入等待。
     
  4. 接收到 Watcher 通知后，重复步骤1。

- 释放锁：释放锁的逻辑和排他锁一致。

- 羊群效应：在整个分布式锁的竞争过程中，大量的"Watcher 通知"和"子节点列表获取"两个操作重复运行，并且绝大多数的运行结果都是判断出自己并非是序号最小的节点，从而继续等待下一次通知——这个看起来显然不怎么科学。客户端无端地接收到过多和自己并不相关的事件通知，如果在集群规模比较大的情况下，不仅会对ZooKeeper 服务器造成巨大的性能影响和网络冲击，更为严重的是，如果同一时间有多个节点对应的客户端完成事务或是事务中断引起节点消失，ZooKeeper 服务器就会在短时间内向其余客户端发送大量的事件通知——这就是所谓的羊群效应。上面这个 ZooKeeper 分布式共享锁实现中出现羊群效应的根源在于，没有找准客户端真正的关注点。我们再来回顾一下上面的分布式锁竞争过程，它的核心逻辑在于∶判断自己是否是所有子节点中序号最小的。于是，很容易可以联想到，每个节点对应的客户端只需要关注比自己序号小的那个相关节点的变更情况就可以了——而不需要关注全局的子列表变更情况（比自己小的节点可以读则自己就可以读了）。

- 改进后的共享锁：每个锁竞争者，只需要关注/shared_lock节点下序号比自己小的那个节点是否存在即可

  1. 客户端调用 create()方法创建一个类似于"/shared_lock/[Hostname]-请求类型-序号"的临时顺序节点。

  2. 客户端调用 getChildren()接口来获取所有已经创建的子节点列表，注意，这里不注册任何 Watcher。

  3. 如果无法获取共享锁，那么就调用exist()来对比自己小的那个节点注册 Watcher。

     读请求∶向比自己序号小的最后一个写请求节点注册 Watcher 监听。

     写请求∶向比自己序号小的最后一个节点注册 Watcher 监听。

  4. 等待 Watcher 通知，继续进入步骤 2。

##### 6.1.8 分布式队列

分布式队列，简单地讲分为两大类，一种是常规的先入先出队列，另一种则是要等到队列元素集聚之后才统一安排执行的 Barrier 模型。

###### FIFO：先入先出

所有客户端都会到/queue_fifo 这个节点下面创建一个临时顺序节点，例如queue_fifo/192.168.0.1-0000001。

创建完节点之后，根据如下 4 个步骤来确定执行顺序。

1. 通过调用 getChildren()接口来获取/queue_fifo 节点下的所有子节点，即获取队列中所有的元素。
2. 确定自己的节点序号在所有子节点中的顺序。
3. 如果自己不是序号最小的子节点，那么就需要进入等待，同时向比自己序号小的最后一个节点注册 Watcher 监听。
4. 接收到 Watcher 通知后，重复步骤 1。

###### Barrier：分布式屏障

Barrier 原意是指障碍物、屏障，而在分布式系统中，特指系统之间的一个协调条件，规定了一个队列的元素必须都集聚后才能统一进行安排，否则一直等待。这往往出现在那些大规模分布式并行计算的应用场景上∶ 最终的合并计算需要基于很多并行计算的子结果来进行。这些队列其实是在 FIFO 队列的基础上进行了增强，大致的设计思想如下∶开始时，/queue_barrier 节点是一个已经存在的默认节点，并且将其节点的数据内容赋值为一个数字n来代表 Barrier值，例如 n=10表示只有当/queue barrier 节点下的子节点个数达到10后，才会打开 Barrier。之后，所有的客户端都会到/queue_barrier节点下创建一个临时节点，例如/queue_barrier/192.168.0.1。

创建完节点之后，根据如下 5个步骤来确定执行顺序。

1. 通过调用 getData()接口获取/queue_barrier 节点的数据内容：10。
2. 通过调用getChildren()接口获取/queue_barrier节点下的所有子节点，即获取队列中的所有元素，同时注册对子节点列表变更的 Watcher 监听。
3. 统计子节点的个数。
4. 如果子节点个数还不足 10个，那么就需要进入等待。
5. 接收到 Watcher 通知后，重复步骤 2。

### 第7章 ZooKeeper 技术内幕

#### 7.1 系统模型

##### 7.1.1 数据模型

ZooKeeper的视图结构和标准的 Unix 文件系统非常类似，但没有引入传统文件系统中目录和文件等相关概念，而是使用了其特有的"数据节点"概念，我们称之为 ZNode。ZNode是 ZooKeeper 中数据的最小单元，每个 ZNode上都可以保存数据，同时还可以挂载子节点，因此构成了一个层次化的命名空间，我们称之为树。

###### 树

在 ZooKeeper 中，每一个数据节点都被称为一个 ZNode，所有 ZNode 按层次化结构进行组织，形成一棵树。ZNode 的节点路径标识方式和 Unix 文件系统路径非常相似，都是由一系列使用斜杠（/）进行分割的路径表示，开发人员可以向这个节点中写入数据，也可以在节点下面创建子节点。

<img src="img/Zookeeper/13.PNG" style="zoom:67%;" />

###### 事务 ID

在 ZooKeeper 中，事务是指能够改变 ZooKeeper 服务器状态的操作，我们也称之为事务操作或更新操作，一般包括数据节点创建与删除、数据节点内容更新和客户端会话创建与失效等操作。对于每一个事务请求，ZooKeeper都会为其分配一个全局唯一的事务 ID，用 ZXID 来表示，通常是一个 64 位的数字。每一个 ZXID 对应一次更新操作，从这些 ZXID 中可以间接地识别出 ZooKeeper 处理这些更新操作请求的全局顺序。

##### 7.1.2 节点特性

###### 节点类型

在 ZooKeeper 中，每个数据节点都是有生命周期的，其生命周期的长短取决于数据节点的节点类型。在 ZooKeeper 中，节点类型可以分为持久节点（PERSISTENT）、临时节点（EPHEMERAL）和顺序节点（SEQUENTIAL）三大类，具体在节点创建过程中，通过组合使用，可以生成以下四种组合型节点类型∶

- 持久节点（PERSISTENT）：持久节点是 ZooKeeper 中最常见的一种节点类型。所谓持久节点，是指该数据节点被创建后，就会一直存在于ZooKeeper 服务器上，直到有删除操作来主动清除这个节点。
- 持久顺序节点（PERSISTENT_SEQUENTIAL）：持久顺序节点的基本特性和持久节点是一致的，额外的特性表现在顺序性上。在 ZooKeeper 中，每个父节点都会为它的第一级子节点维护一份顺序，用于记录下每个子节点创建的先后顺序。基于这个顺序特性，在创建子节点的时候，可以设置这个标记，那么在创建节点过程中，ZooKeeper 会自动为给定节点名加上一个数字后缀，作为一个新的、完整的节点名。另外需要注意的是，这个数字后缀的上限是整型的最大值。
- 临时节点（EPHEMERAL）：和持久节点不同的是，临时节点的生命周期和客户端的会话绑定在一起，也就是说，如果客户端会话失效，那么这个节点就会被自动清理掉。**注意，这里提到的是客户端会话失效，而非 TCP连接断开。另外，ZooKeeper 规定了不能基于临时节点来创建子节点，即临时节点只能作为叶子节点。**

- 临时顺序节点（EPHEMERAL SEQUENTIAL）：临时顺序节点的基本特性和临时节点也是一致的，同样是在临时节点的基础上，添加了顺序的特性。

###### 状态信息

Stat 类中包含了 ZooKeeper 上一个数据节点的所有状态信息，包括事务 ID、版本信息和子节点个数等。

| 状态属性       | 说明                                                         |
| -------------- | ------------------------------------------------------------ |
| czxid          | Created ZXID，表示该数据节点被创建时的事务 ID                |
| mzxid          | Modified ZXID，表示该节点最后一次被更新时的事务 ID           |
| ctime          | Created Time，表示节点被创建的时间                           |
| mtime          | Modified Time，表示该节点最后一次被更新的时间                |
| version        | 数据节点的版本号                                             |
| cversion       | 子节点的版本号                                               |
| aversion       | 节点的 ACL 版本号                                            |
| ephemeralOwner | 创建该临时节点的会话的 sessionlD。如果该节点是持久节点，那么这个属性值为0 |
| dataLength     | 数据内容的长度                                               |
| numChiLdren    | 当前节点的子节点个数                                         |
| pzxid          | 表示该节点的子节点列表最后一次被修改时的事务ID。注意，只有子节点列表变更了才会变更pzxid，子节点内客变更不会影响 |

##### 7.1.3 版本——保证分布式数据原子性操作

ZooKeeper 中为数据节点引入了版本的概念，每个数据节点都具有三种类型的版本信息，对数据节点的任何更新操作都会引起版本号的变化。

ZooKeeper 中的版本概念表示的是对数据节点的数据内容、子节点列表，或是节点ACL 信息的修改次数，我们以其中的 version这种版本类型为例来说明。在一个数据节点/zk-book 被创建完毕之后，节点的 version值是 0，表示的含义是"当前节点自从创建之后，被更新过 0次"。如果现在对该节点的数据内容进行更新操作，那么随后，version的值就会变成1。同时需要注意的是，在上文中提到的关于 version 的说明，其表示的是对数据节点数据内容的变更次数，强调的是变更次数，因此即使前后两次变更并没有使得数据内容的值发生变化，version的值依然会变更。

在 ZooKeeper 中，version 属性正是用来实现乐观锁机制中的"写入校验"的。在ZooKeeper服务器的PrepRequestProcessor处理器类中，在处理每一个数据更新（setDataRequest）请求时，会进行如清单 7-1 所示的版本检查。

```
version = setDataRequest.getVersion();
int currentVersion = nodeRecord.stat.getVersion(); 
if(version !=-1 && version != currentVersion){
	throw new KeeperException.BadVersionException(path);
}
version = currentVersion + 1;
```

##### 7.1.4 Watcher——数据变更的通知

<img src="img/Zookeeper/14.png" style="zoom:67%;" />

ZooKeeper 的 Watcher 机制主要包括客户端线程、客户端 WatchManager 和 ZooKeeper 服务器三部分。在具体工作流程上，简单地讲，客户端在向 ZooKeeper 服务器注册 Watcher 的同时，会将 Watcher 对象存储在客户端的 WatchManager 中。当ZooKeeper 服务器端触发 Watcher 事件后，会向客户端发送通知，客户端线程从 WatchManager 中取出对应的 Watcher 对象来执行回调逻辑。

###### Watcher 接口

在 ZooKeeper 中，接口类 Watcher 用于表示一个标准的事件处理器，其定义了事件通知相关的逻辑，包含KeeperState 和 EventType 两个枚举类，分别代表了通知状态和事件类型，同时定义了事件的回调方法∶ process（WatchedEvent event）。

- Watcher 事件

  <img src="img/Zookeeper/15.png" style="zoom:67%;" />

  NodeDataChanged 事件，此处说的变更包括节点的数据内容和数据的版本号 dataVersion。因此，即使使用相同的数据内容来更新，还是会触发这个事件通知，因为对于 ZooKeeper 来说，无论数据内容是否变更，一旦有客户端调用了数据更新的接口，且更新成功，就会更新 dataVersion 值。

  NodeChildrenChanged 事件会在数据节点的子节点列表发生变更的时候被触发，这里说的子节点列表变化特指子节点个数和组成情况的变更，即新增子节点或删除子节点，而子节点内容的变化是不会触发这个事件的。

- 回调方法 process()

  process 方法是 Watcher 接口中的一个回调方法，当 ZooKeeper 向客户端发送一个 Watcher 事件通知时，客户端就会对相应的 process 方法进行回调，从而实现对事件的处理。process 方法的定义如下∶

  ```
  abstract pubLic void process(WatchedEvent event);
  ```

  这个回调方法的定义非常简单，我们重点看下方法的参数定义∶ WatchedEvent。

  WatchedEvent 包含了每一个事件的三个基本属性：通知状态（keeperState）、事件类型（eventType）和节点路径（path）。ZooKeeper 使用 WatchedEvent 对象来封装服务端事件并传递给 Watcher，从而方便回调方法 process 对服务端事件进行处理。

  WatchedEvent 和 WatcherEvent 两者表示的是同一个事物，都是对一个服务端事件的封装。不同的是，WatchedEvent 是一个逻辑事件，用于服务端和客户端程序执行过程中所需的逻辑对象，而 WatcherEvent 因为实现了序列化接口，因此可以用于网络传输。

- 工作机制

  <img src="img/Zookeeper/16.png" style="zoom:67%;" />
  1. 客户端注册 Watcher

     在创建一个 ZooKeeper 客户端对象实例时，可以向构造方法中传入一个默认的 Watcher∶

     ```
     public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher);
     ```

     这个 Watcher 将作为整个 ZooKeeper 会话期间的默认 Watcher，会一直被保存在客户端 ZKWatchManager的 defaultwatcher 中。另外，ZooKeeper 客户端也可以通过getData、getChildren和exist三个接口来向 ZooKeeper 服务器注册 Watcher，无论使用哪种方式，注册 Watcher 的工作原理都是一致的，这里我们以 getData 这个接口为例来说明。getData 接口用于获取指定节点的数据内容，主要有两个方法∶

     ```
     public byte[] getData(String path, boolean watch, Stat stat) 
     public byte[] getData(finaL String path, Watcher watcher, Stat stat)
     ```

     在向 getData 接口注册 Watcher 后，客户端首先会对当前客户端请求 request 进行标记，将其设置为"使用Watcher监听"，同时会封装一个Watcher的注册信息WatchRegistration对象，用于暂时保存数据节点的路径和 Watcher 的对应关系，具体的逻辑代码如下：

     ```java
     public Stat getData(final String path,watcher watcher,Stat stat){
     	...
     	WatchRegistration wcb = null;
         if (watcher != null) {
     		wcb = new DatawatchRegistration(watcher,clientPath);
     	}
     	...
     	request.setWatch(watcher != null);
     	ReplyHeader r = cnxn.submitRequest(h,request,response,wcb);
     	...
     }
     ```

     在 ZooKeeper 中，Packet 可以被看作一个最小的通信协议单元，用于进行客户端与服务端之间的网络传输，任何需要传输的对象都需要包装成一个 Packet 对象。因此，在 CLientCnxn中watchRegistration又会被封装到 Packet中去，然后放入发送队列中等待客户端发送∶

     ```java
     Packet queuePacket(RequestHeader h,ReplyHeader r,Record request,
     		Record response,AsyncCallback cb,String clientPath,
     		String serverPath,Object ctx,watchRegistration watchRegistration){ 
     	Packet packet = null;
         ...
     	synchronized (outgoingQueue) {
     		packet = new Packet(h,r,request,response,watchRegistration);
     		...
     		outgoingQueue.add(packet);
     		...
     	}
     }
     ```

     随后，ZooKeeper 客户端就会向服务端发送这个请求，同时等待请求的返回。完成请求发送后，会由客户端 SendThread 线程的 readResponse 方法负责接收来自服务端的响应，finishPacket 方法会从Packet中取出对应的Watcher并注册到ZKWatchManager中去：

     ```java
     private void finishPacket(Packet p){
     	if (p.watchRegistration != null) {
     		p.watchRegistration.register(p.replyHeader.getErr());
     	}
     	...
     }
     ```

     从上面的内容中，我们已经了解到客户端已经将 Watcher 暂时封装在了WatchRegistration对象中，现在就需要从这个封装对象中再次提取出 Watcher 来∶

     ```java
     protected Map<String,Set<Watcher>> getwatches(int rc){ 
     	return watchManager.datawatches;
     }
     public void register(int rc) {
     	if(shouldAddWatch(rc)) {
     		Map<String,Set<latcher>> watches = getwatches(rc); 
     		synchronized(watches){
     			Set<Watcher> watchers = watches.get(clientPath);
                 if (watchers == null) {	
                 	watchers = new HashSet<Watcher>();
                 	watches.put(clientPath,watchers);
                 }
     			watchers.add(watcher);
     		}
     	}
     }
     ```

     在 register方法中，客户端会将之前暂时保存的 Watcher对象转交给 ZKWatchManager，并最终保存到 datawatches 中去。ZKwatchManager.datawatches 是一个 Map<String，Set\<Watcher>>类型的数据结构，用于将数据节点的路径和Watcher 对象进行一一映射后管理起来。

     客户端每调用一次 getData()接口，就会注册上一个 Watcher，那么这些 Watcher 实体都会随着客户端请求被发送到服务端去吗？答案是否定的。在上面的流程中，我们提到把 watchRegistration 封装到了Packet 对象中去，但事实上，在底层实际的网络传输序列化过程中，并没有将 WatchRegistration 对象完全地序列化到底层字节数组中去。在 Packet.createBB()方法中，ZooKeeper只会将 requestHeader 和 request 两个属性进行序列化。

  2. 服务端处理 Watcher

     <img src="img/Zookeeper/17.png" style="zoom: 33%;" />

     - ServerCnxn 存储：对于标记了 Watcher 注册的请求，ZooKeeper 会将其对应的 ServerCnxn存储到 WatchManager中。

       服务端收到来自客户端的请求之后，在 FinalRequest.Processor.processRequest()中会判断当前请求是否需要注册 Watcher∶

       ```
       case OpCode.getData: {
       	...
       	byte b[]= zks.getZKDatabase().getData(getDataRequest.getPath(),stat.
       		getDataRequest.getwatch()? cnxn : null);
       	rsp = new GetDataResponse(b,stat);
           break;
       }
       ```

       从getData 请求的处理逻辑中，我们可以看到，当getDataRequest.getwatch()为 true 的时候， ZooKeeper 就认为当前客户端请求需要进行 Watcher 注册，于是就会将当前的 ServerCnxn 对象和数据节点路径传入 getData 方法中去。那么为什么要传入 ServerCnxn呢? ServerCnxn是一个ZooKeeper客户端和服务器之间的连接接口，代表了一个客户端和服务器的连接。ServerCnxn 接口的默认实现是 NIOServerCnxn，同时从3.4.0版本开始，引入了基于Netty 的实现∶NettyServerCnxn。无论采用哪种实现方式，都**实现了Watcher的process接口，因此我们可以把ServerCnxn看作是一个Watcher对象**。数据节点的节点路径和 ServerCnxn 最终会被存储在 WatchManager 的 watchTable 和 watch2Paths 中。

       WatchManager是 ZooKeeper 服务端 Watcher的管理者，其内部管理的watchTabLe和 watch2Paths 两个存储结构，分别从两个维度对 Watcher 进行存储。

       - watchTable 是从数据节点路径的粒度来托管 Watcher。

       - watch2Paths 是从 Watcher 的粒度来控制事件触发需要触发的数据节点。

       同时，WatchManager 还负责 Watcher 事件的触发，并移除那些已经被触发的 Watcher。注意，WatchManager 只是一个统称，在服务端，DataTree 中会托管两个WatchManager，分别是datawatches和childWatches，分别对应数据变更 Watcher 和子节点变更 Watcher。在本例中，因为是 getData 接口，因此最终会被存储在 datawatches 中。
       
     - Watcher 触发

       NodeDataChanged 事件的触发条件是"Watcher 监听的对应数据节点的数据内容发生变更"，其具体实现如下∶

       ```java
       public Stat setData(String path,byte[] data,int version,long zxid, Long 	
       	time)throws KeeperException.NoNodeException { 
       	Stat s = new Stat();
       	DataNode n = nodes.get(path); 
       	if (n == nuLL){
       		throw new KeeperException.NoNodeException();
           }
           byte[] lastdata = null; 
           synchronized (n){
           	lastdata = n.data; 
           	n.data = data; 
           	n.stat.setNtime(time); 
           	n.stat.setMzxid(zxid); 
           	n.stat.setVersion(version); 
           	n.copySstat(s);
           }
       	datawatches.triggerwatch(path,EventType.NodeDataChanged); 
       	return s;
       }
       ```

       在对指定节点进行数据更新后，通过调用 WatchManager的triggerwWatch 方法来触发相关的事件∶

       ```java
       public Set<watcher triggerwatch(String path,EventType type){
       	return triggerwatch(path,type,null));
       }
       public Set<watcher> triggermatch(String path,EventType type,Set<atcher> 	
       	supress){
       	WatchedEvent e = new watchedEvent(type, KeeperState.SyncConnected, path);
       	HashSet<Watcher> watchers; 
       	synchronized (this) {
       		watchers = watchTable.remove(path);
       		//如果不存在Watcher，直接返回
       		for (Watcher w: watchers){
       			HashSet<String> paths = watch2Paths.get(w);
                   if (paths != null) { 
                   	paths.remove(path);
                   }
               }
           }
       	for (watcher w: watchers){
       		if(supress != null s supress.contains(w)){
       			continue;
               }
               w.process(e); 
           }
           return watchers;
       }
       ```

       无论是 datawatches 还是 chidWatches 管理器，Watcher 的触发逻辑都是一致的，基本步骤如下。

       1. 封装 WatchedEvent。首先将通知状态（KeeperState）、事件类型（EventType）以及节点路径（Path）封装成一个 WatchedEvent 对象。 

       2. 查询 Watcher。根据数据节点的节点路径从 watchTabLe 中取出对应的 Watcher。如果没有找到 Watcher，说明没有任何客户端在该数据节点上注册过 Watcher，直接退出。而如果找到了这个 Watcher，会将其提取出来，同时会直接从 watchTable和 watch2Paths 中将其删除——从这里我们也可以看出，Watcher 在服务端是一次性的，即触发一次就失效了。 

       3. 调用 process 方法来触发 Watcher。在这一步中，会逐个依次地调用从步骤 2 中找出的所有 Watcher 的 process方法。那么这里的 process 方法究竟做了些什么呢?在上文中我们已经提到，对于需要注册 Watcher 的请求，ZooKeeper 会把当前请求对应的 ServerCnxn 作为一个 Watcher 进行存储，因此，这里调用的 process 方法，事实上就是 ServerCnxn 的对应方法∶

          ```java
          public class NIOServerCnxn extends ServerCnxn {
          	synchronized public void process(watchedEvent event){
          		ReplyHeader h = new ReplyHeader(-1,-1L,0);
          		// Convert watchedEvent to a type that can be sent over the wire 
                  watcherEvent e = event.getwrapper(); 	
          		sendResponse(h,e,"notification");
          	}
          }
          ```

          从上面的代码片段中，我们可以看出在 process 方法中，主要逻辑如下。

          - 在请求头中标记"-1"，表明当前是一个通知。
          - 将WatchedEvent包装成WatcherEvent，以便进行网络传输序列化。
          - 向客户端发送该通知。

          从以上几个步骤中可以看到，**ServerCnxn的 process方法中的逻辑非常简单，本质上并不是处理客户端 Watcher 真正的业务逻辑，而是借助当前客户端连接的 ServerCnxn 对象来实现对客户端的 WatchedEvent 传递，真正的客户端 Watcher回调与业务逻辑执行都在客户端**。

  3. 客户端回调 Watcher

     - SendThread 接收事件通知

       ```java
       class SendThread extends Thread {
       	void readResponse(ByteBuffer incomingBuffer)throws IOException {
       		...
       		if(replyHdr.getXid()==--1){
       		// -1 means notification/
       			WatcherEvent event = new watcherEvent(); 
       			event.deserialize(bbia,"response");
       			// convert from a server path to a client path 
       			if (chrootPath != null) {
       				String serverPath = event.getPath(); 
       				if(serverPath.compareTo(chrootPath)=0)
       					event.setPath("/");
       				else if(serverPath.length()> chrootPath.Length())
       					event.setPath(serverPath.substring(chrootPath.Length()));
       			}
       			WatchedEvent we = new watchedEvent(event);
       			eventThread.queueEvent(we);
                   return;
                }
                ...
            }
       }
       ```

       对于一个来自服务端的响应，客户端都是由 SendThread.readResponse(ByteBuffer incomingBuffer)方法来统一进行处理的，如果响应头 repLyHdr 中标识了 XID 为-1，表明这是一个通知类型的响应，对其的处理大体上分为以下4个主要步骤。

       1. 反序列化。ZooKeeper客户端接到请求后，首先会将字节流转换成 WatcherEvent 对象。
       2. 处理 chrootPath。如果客户端设置了chrootPath 属性，那么需要对服务端传过来的完整的节点路径进行 chrootPath处理，生成客户端的一个相对节点路径。例如客户端设置了 chrootPath 为/app1，那么针对服务端传过来的响应包含的节点路径为/app1/locks，经过chrootPath处理后，就会变成一个相对路径∶/locks。 
       3. 还原 WatchedEvent。process 接口的参数定义是 WatchedEvent，因此这里需要将 WatcherEvent 对象转换成 WatchedEvent。 
       4. 回调 watcher。最后将 WatchedEvent 对象交给 EventThread 线程，在下一个轮询周期中进行Watcher 回调。

     - EventThread 处理事件通知

       SendThread 接收到服务端的通知事件后，会通过调用 EventThread.queueEvent方法将事件传给EventThread线程，其逻辑如下：

       ```java
       public void queueEvent (WatchedEvent event){
       	if (event.getType()== EventType.None && sessionState == event.getState()){ 
       		return;
       	}
       	sessionState = event.getState();
       	// materialize the watchers based on the event 
       	WatcherSetEventPair pair = new WatcherSetEventPair(
       		//这个watcher就是客户端处理逻辑的watcher
       		watcher.materialize(event.getState(), event.getType(),
       			event.getPath()), event);
       		// queue the pair(watch set § event)for Later processing 	
       		waitingEvents.add(pair);
       	}
       }
       ```

       queueEvent 方法首先会根据该通知事件，从 ZKWatchManager中取出所有相关的 Watcher∶

       ```java
       public Set<Watcher> materialize(Watcher.Event.KeeperState state,
       						Watcher.Event.EventType type,
                               String clientPath){
       	Set<Watcher> result = new HashSet<watcher>();
           switch(type) {
       		...
       		case NodeDataChanged: 
       		case NodeCreated:
       			synchronized (datawatches) {
       				addTo(datawatches.remove(clientPath),result);
       			}
       			synchronized (existwatches) {
       				addTo(existwatches.remove(clientPath),result);
       			}
       			break;
       		...
       	}
       	return result;
       }
       final private void addTo(Set<natcher> from, Set<Natcher to){
       	if(from != null) {
       		to.addAll(from);
       	}
       }
       ```

       客户端在识别出事件类型 EventType 后，会从相应的 Watcher 存储（即 dataWatches、existwatches或 childWatches 中的一个或多个，本例中就是从 datawatches 和 existwatches 两个存储中获取）中去除对应的 Watcher。注意，此处使用的是 remove 接口，因此也表明了客户端的 Watcher 机制同样也是一次性的，即一旦被触发后，该 Watcher 就失效了。

       获取到相关的所有 Watcher 之后，会将其放入 waitingEvents 这个队列中去。WaitingEvents 是一个待处理 Watcher的队列，EventThread的 run方法会不断对该队列进行处理∶

       ```java
       public void run()(
       	try {
       		isRunning = true; 
       		while(true) {
       			0bject event = waitingEvents.take();
                   if (event == eventOfDeath) {
       				wasKilled = true;
       			} else {
       				processEvent (event);
       		...
       }
       private void processEvent(0bject event){ 
       	try {
       		if(event instanceof watcherSetEventPair) {
       		// each watcher will process the event
       			WatcherSetEventPair pair = (WatcherSetEventPair) event;
                   for (Watcher watcher : pair.watchers){ 
                   	try {
       					watcher.process(pair.event);
       				} catch (Throwable t) {
       		...
       }
       ```

       EventThread 线程每次都会从 waitingEvents队列中取出一个 Watcher，并进行串行同步处理。注意，此处 processEvent 方法中的 Watcher 才是之前客户端真正注册的 Watcher，调用其 process 方法就可以实现 Watcher的回调了。 

  4. Watcher 特性总结

     - 一次性：无论是服务端还是客户端，一旦一个 Watcher 被触发， ZooKeeper 都会将其从相应的存储中移除。因此，开发人员在 Watcher 的使用上要记住的一点是需要反复注册。这样的设计有效地减轻了服务端的压力。试想，如果注册一个 Watcher 之后一直有效，那么，针对那些更新非常频繁的节点，服务端会不断地向客户端发送事件通知，这无论对于网络还是服务端性能的影响都非常大。

     - 客户端串行执行：客户端 Watcher 回调的过程是一个串行同步的过程，这为我们保证了顺序，同时，需要开发人员注意的一点是，千万不要因为一个 Watcher 的处理逻辑影响了整个客户端的 Watcher 回调。

     - 轻量：WatchedEvent 是 ZooKeeper 整个 Watcher 通知机制的最小通知单元，这个数据结构中只包含三部分内容∶通知状态、事件类型和节点路径。也就是说，Watcher通知非常简单，只会告诉客户端发生了事件，而不会说明事件的具体内容。例如针对 NodeDataChanged 事件，ZooKeeper 的 Watcher 只会通知客户端指定数据节点的数据内容发生了变更，而对于原始数据以及变更后的新数据都无法从这个事件中直接获取到，而是需要客户端主动重新去获取数据——这也是ZooKeeper的 Watcher机制的一个非常重要的特性。

       另外，客户端向服务端注册 Watcher 的时候，并不会把客户端真实的 Watcher 对象传递到服务端，仅仅只是在客户端请求中使用 booLean 类型属性进行了标记，同时服务端也仅仅只是保存了当前连接的 ServerCnxn 对象。

       如此轻量的 Watcher 机制设计，在网络开销和服务端内存开销上都是非常廉价的。

##### 7.1.5 ACL———保障数据的安全

ZooKeeper 作为一个分布式协调框架，其内部存储的都是一些关乎分布式系统运行时状态的元数据，尤其是一些涉及分布式锁、Master 选举和分布式协调等应用场景的数据，会直接影响基于 ZooKeeper 进行构建的分布式系统的运行状态。因此，如何有效地保障 ZooKeeper 中数据的安全，从而避免因误操作而带来的数据随意变更导致的分布式系统异常就显得格外重要了。所幸的是，ZooKeeper 提供了一套完善的 ACL（Access Control List）权限控制机制来保障数据的安全。

提到权限控制，我们首先来看看大家都熟悉的、在 Unix/Linux 文件系统中使用的，也是目前应用最广泛的权限控制方式——UGO（User、Group 和 Others）权限控制机制。简单地讲，UGO 就是针对一个文件或目录，对创建者（User）、创建者所在的组（Group）和其他用户（Other）分别配置不同的权限。从这里可以看出，UGO 其实是一种粗粒度的文件系统权限控制模式，利用 UGO 只能对三类用户进行权限控制，即文件的创建者、创建者所在的组以及其他所有用户，很显然，UGO 无法解决下面这个场景∶

用户U1创建了文件F1，希望U1所在的用户组 G1，拥有对F1读写和执行的权限，另一个用户组 G2，拥有读权限，而另外一个用户U3则没有任何权限。

接下去我们来看另外一种典型的权限控制方式：ACL。ACL，即访问控制列表，是一种相对来说比较新颖且更细粒度的权限管理方式，可以针对任意用户和组进行细粒度的权限控制。目前绝大部分 Unix 系统都已经支持了 ACL 方式的权限控制，Linux 也从 2.6版本的内核开始支持这个特性。

###### ACL 介绍

ZooKeeper 的 ACL 权限控制和 Unix/Linux 操作系统中的 ACL 有一些区别，读者可以从三个方面来理解 ACL 机制，分别是∶ 权限模式（Scheme）、授权对象（ID）和权限（Permission），通常使用`scheme:id:permission`来标识一个有效的 ACL 信息。

- 权限模式∶ Scheme

  权限模式用来确定权限验证过程中使用的检验策略。在 ZooKeeper 中，开发人员使用最多的就是以下四种权限模式。 

  1. IP：IP 模式通过 IP 地址粒度来进行权限控制，例如配置了"ip∶192.168.0.110"，即表示权限控制都是针对这个 IP 地址的。同时，IP 模式也支持按照网段的方式进行配置，例如"ip∶192.168.0.1/24"表示针对192.168.0.*这个IP段进行权限控制。

  2. Digest：Digest是最常用的权限控制模式，也更符合我们对于权限控制的认识，其以类似于"username∶password"形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。

     当我们通过"username∶password"形式配置了权限标识后，ZooKeeper 会对其先后进行两次编码处理，分别是 SHA-1 算法加密和 BASE64 编码，其具体实现DigestAuthenticationProvider.generateDigest(String idPassword)函数进行封装。

  3. World：World 是一种最开放的权限控制模式，从其名字中也可以看出，事实上这种权限控制方式几乎没有任何作用，数据节点的访问权限对所有用户开放，即所有用户都可以在不进行任何权限校验的情况下操作 ZooKeeper 上的数据。另外，World 模式也可以看作是一种特殊的 Digest 模式，它只有一个权限标识，即"world∶anyone"。

  4. Super：Super 模式，顾名思义就是超级用户的意思，也是一种特殊的 Digest 模式。在 Super模式下，超级用户可以对任意 ZooKeeper 上的数据节点进行任何操作。

- 授权对象∶ID

  授权对象指的是权限赋予的用户或一个指定实体，例如 IP 地址或是机器等。在不同的权限模式下，授权对象是不同的，表 7-4中列出了各个权限模式和授权对象之间的对应关系。

  | 权限模式 | 授权对象                                                     |
  | -------- | ------------------------------------------------------------ |
  | IP       | 通常是一个IP地址或是IP段，例知"192.168.0.110"或-192.168.0.1/24 |
  | Digest   | 自定义，通常是"usemame:BASE64(SHA-1(username:password))"，例如"foo∶kWN6aNSbjcKWPqjV7cgON24raU=" |
  | World    | 只有一个 ID∶"anyone"                                         |
  | Super    | 与 Digest 模式一致                                           |

- 权限∶ Permission

  权限就是指那些通过权限检查后可以被允许执行的操作。在ZooKeeper 中，所有对数据的操作权限分为以下五大类∶

  - CREATE（C）∶数据节点的创建权限，允许授权对象在该数据节点下创建子节点。
  - DELETE（D）∶子节点的删除权限，允许授权对象删除该数据节点的子节点。
  - READ（R）：数据节点的读取权限，允许授权对象访问该数据节点并读取其数据内容或子节点列表等。
  - WRITE（W）：数据节点的更新权限，允许授权对象对该数据节点进行更新操作。
  - ADMIN（A）∶ 数据节点的管理权限，允许授权对象对该数据节点进行 ACL 相关的设置操作。

###### ACL 管理

- 设置ACL

  通过 zkCli 脚本登录 ZooKeeper 服务器后，可以通过两种方式进行 ACL的设置。一种是在数据节点创建的同时进行 ACL 权限的设置，命令格式如下∶

  ```
  create[-s] [-e] path data acl
  ```

  ```
  [zk: localhost coMMECTED 2] create -e /zk-book init digest: foo: MiGs3EiylpP4rvH1Q1MbP+oUF8=: cdrwa 
  Created /zk-book
  
  [zk: localhost CONNECTED 3] getAcl /zkbook
  'digest,'foo:MiGs3EiylpP4rvH101NwbP+oUF8=:cdrwa
  ```

  另一种方式则是使用 setAcl 命令单独对已经存在的数据节点进行 ACL 设置∶

  ```
  setAcl path acl
  ```

  ```
  [zk: LocaLhost CONNECTED 0] create -e /zk-book init 
  Created /zk-book
  [zk: LocaLhost cONNECTED 1] setAcl/zk-book digest:foo:MiGs3EiylpP4ryH101MwbP+oUF8=:cdrwa
  cZxid = 0x400000042
  ctime =Sun Jul 13 22:14:13 CST 2014
  mZxid = 0x400000042
  mtime = Sun Ju 13 22:14:13 CST 2014
  pZxid = 0x400000042
  cversion = 0
  dataVersion = 0
  aclVersion = 1
  ephemeral0wner = 0x1472ff49b020003
  dataLength = 4
  numChildren = 0.
  [zk; LocaLhost CONNECTED 3] getAcl /zkbook
  'digest,'foo:MiGs3EiylpP4rvH1Q1NwbP+oUF8=:cdrwa
  ```

- Super 模式的用法

  根据 ACL 权限控制的原理，一旦对一个数据节点设置了 ACL 权限控制，那么其他没有被授权的 ZooKeeper 客户端将无法访问该数据节点，这的确很好地保证了 ZooKeeper 的数据安全。但同时，ACL 权限控制也给 ZooKeeper 的运维人员带来了一个困扰∶ 如果一个持久数据节点包含了 ACL 权限控制，而其创建者客户端已经退出或已不再使用，那么这些数据节点该如何清理呢? 这个时候，就需要在 ACL 的 Super 模式下，使用超级管理员权限来进行处理了。要使用超级管理员权限，首先需要在 ZooKeeper 服务器上开启 Super 模式，方法是在 ZooKeeper 服务器启动的时候，添加如下系统属性∶

  ```
  -Dzookeeper.DigestAuthenticationProvider.superDigest=foo:kwN6aNSbjcKMPqjiV7cgON24raU=
  ```

  其中，"foo"代表了一个超级管理员的用户名；"kWN6aNSbjcKWPqjiV7cgON24raU="是可变的，由 ZooKeeper 的系统管理员来进行自主配置，此例中使用的是"foo∶zk-book"的编码。完成对 ZooKeeper 服务器的 Super 模式的开启后，就可以在应用程序中使用了。

#### 7.2 序列化与协议

##### 7.2.2 使用 Jute进行序列化

```java
pubLic class MockReqHeader implements Record {
	private Long sessionId;
    private String type;
    public MockReqHeader() {}
	public MockReqHeader(Long sessionId,String type){
		this.sessionId = sessionId;
        this.type = type;
    }
	public long getSessionId() { return sessionId;}
	public void setSessionId(long sessionId) {
		this.sessionId = sessionId;
	}
	pubLic String getType(){ return type;}
	pubLic void setType(String m_){
		type = m_;
	}
	public void serialize(OutputArchive a,String tag)throws java.io.IOException{
		a_.startRecord(this, tag);
		a_.writeLong(sessionId,"session1d"); 
		a_.writeString(type,"type"); 
		a_.endRecord(this,tag);
	}
	public void deserialize(InputArchive a,String tag)throws java.io.IOException{
		a_.startRecord(tag);
		sessionId = a_.readLong("sessionId");
        type = a_.readString("type"); 
        a_.endRecord(tag);
    }
}
```

```java
ByteArrayOutputStream baos = new ByteArrayOutputStream();
BinaryOutputArchive boa = BinaryOutputArchive.getArchive(baos);
new MockReqHeader(0x3422leccb92a34el,"ping"),serialize(boa,"header");
//这里通常是TCP 网络传输对象
ByteBuffer bb = ByteBuffer.wrap( baos.toByteArray());
//开始反序列化
ByteBufferInputStream bbis = new ByteBufferInputStream(bb);
BinaryInputArchive bbia = BinaryInputArchive.getArchive(bbis);
MockReqHeader header2 = new MockReqHeader();
header2.deserialize(bbia,"header");
bbis.close();
baos.close();
```

上面这个代码片段演示了如何使用 Jute 来对 MockReqHeader 对象进行序列化和反序列化，总的来说，大体可以分为4步。

1. 实体类需要实现 Record 接口的 serialize 和 deserialize方法。 
2. 构建一个序列化器 BinaryOutputArchive。 
3. 序列化。调用实体类的 serialize 方法，将对象序列化到指定 tag 中去。例如在本例中就将 MockReqHeader 对象序列化到 header 中去。 
4. 反序列化。调用实体类的 deserialize，从指定的 tag 中反序列化出数据内容。

##### 7.2.3 深入 Jute

###### Record 接口

Jute 定义了自己独特的序列化格式 Record，ZooKeeper 中所有需要进行网络传输或是本地磁盘存储的类型定义，都实现了该接口，其结构简单明了，操作灵活可变，是 Jute 序列化的核心。 Record 接口定义了两个最基本的方法，分别是 serialize 和 deserialize，分别用于序列化和反序列化∶

```java
package org.apache.jute;
import java.io.IOException;
public interface Record {
	public void serialize(OutputArchive archive,String tag) throws IOException;
    public void deseriaLize(InputArchive archive,String tag) throws IOException;
}
```

所有实体类通过实现 Record 接口的这两个方法，来定义自己将如何被序列化和反序列化。其中 archive 是底层真正的序列化器和反序列化器，并且每个 archive 中可以包含对多个对象的序列化和反序列化，因此两个接口方法中都标记了参数 tag，用于向序列化器和反序列化器标识对象自己的标记。

- OutputArchive 和 InputArchive

  0utputArchive 和 InputArchive 分别是 Jute 底层的序列化器和反序列化器接口定义。在最新版本的Jute中，分别有BinaryOutputArchive/BinaryInputArchive、 CsvOutputArchive/CsvInputArchive和XmLOutputArchive/XmLInputArchive三种实现。无论哪种实现，都是基于 OutputStream和 InputStream进行操作。

  BinaryOutputArchive 对数据对象的序列化和反序列化，主要用于进行网路传输和本地磁盘的存储，是 ZooKeeper 底层最主要的序列化方式。 CsvOutputArchive 对数据的序列化，则更多的是方便数据对象的可视化展现，因此被使用在 toString 方法中。最后一种XmLOutputArchive，则是为了将数据对象以 XML 格式保存和还原，但是目前在 ZooKeeper 中基本没有被使用到。

##### 7.2.4 通信协议

<img src="img/Zookeeper/18.PNG" style="zoom:50%;" />

###### 协议解析∶请求部分

<img src="img/Zookeeper/19.PNG" style="zoom:80%;" />

- 请求头∶ RequestHeader

  请求头中包含了请求最基本的信息，包括xid和type∶

  ```
  module org.apache.zookeeper.proto {
  	class RequestHeader { 
  		int xid;
  		int type;
  	}
  }
  ```

  xid 用于记录客户端请求发起的先后序号，用来确保单个客户端请求的响应顺序。type代表请求的操作类型，常见的包括创建节点（OpCode.create∶1）、删除节点（OpCode.create∶ 2）和获取节点数据（OpCode.getData∶ 4）等，所有这些操作类型都被定义在类org.apache.zookeeper.ZooDefs.OpCode 中。根据协议规定，除非是"会话创建"请求，其他所有的客户端请求中都会带上请求头。

- 请求体∶Request

  协议的请求体部分是指请求的主体内容部分，包含了请求的所有操作内容。不同的请求类型，其请求体部分的结构是不同的。

  - ConnectRequest∶ 会话创建

    ZooKeeper 客户端和服务器在创建会话的时候，会发送 ConnectRequest 请求，该请求体中包含了协议的版本号 protocoLVersion、最近一次接收到的服务器 ZXID LastZxidSeen、会话超时时间 timeOut、会话标识 sessionId 和会话密码 passwd，其数据结构定义如下∶

    ```
    module org.apache.zookeeper.proto { 
    	class ConnectRequest { 
    		int protocolVersion; 
    		Long LastZxidSeen; 
    		int timeOut;
            Long sessionId;
            buffer passwd;
        }
    }
    ```

  - GetDataRequest∶ 获取节点数据

    ZooKeeper 客户端在向服务器发送获取节点数据请求的时候，会发送 GetDataRequest请求，该请求体中包含了数据节点的节点路径 path 和是否注册 Watcher 的标识watch，其数据结构定义如下∶

    ```
    module org.apache.zookeeper.proto {
    	class GetDataRequest { 
    		ustring path;
            boolean watch;
        }
    }
    ```

  - SetDataRequest∶ 更新节点数据

    ZooKeeper 客户端在向服务器发送更新节点数据请求的时候，会发送 SetDataRequest 请求，该请求体中包含了数据节点的节点路径 path、数据内容 data 和节点数据的期望版本号 version，其数据结构定义如下∶

    ```
    module org.apache.zookeeper.proto { 
    	class SetDataRequest { 
    		ustring path;
            buffer data;
            int version;
        }
    }
    ```

###### 协议解析∶ 响应部分

<img src="img/Zookeeper/20.PNG" style="zoom: 50%;" />

- 响应头∶ ReplyHeader

  响应头中包含了每一个响应最基本的信息，包括xid、zxid 和 err∶

  ```
  module org.apache.zookeeper.proto {
  	class ReplyHeader {
          int xid;
          long zxid;
          int err;
      }
  }
  ```

  xid 和上文中提到的请求头中的 xid是一致的，响应中只是将请求中的 xid 原值返回。zxid代表 ZooKeeper 服务器上当前最新的事务 ID。err 则是一个错误码，当请求处理过程中出现异常情况时，会在这个错误码中标识出来，常见的包括处理成功（Code.0K∶0）、节点不存在（Code.NONODE∶101）和没有权限（Code．NOAUTH∶102）等，所有这些错误码都被定义在类org.apache.zookeeper.KeeperException.Code 中。

- 响应体∶ Response

  协议的响应体部分是指响应的主体内容部分，包含了响应的所有返回数据。不同的响应类型，其响应体部分的结构是不同的。

  - ConnectResponse∶ 会话创建

    针对客户端的会话创建请求，服务端会返同客户端一个 ConnectResponse 响应，该响应体中包含了协议的版本号 protocolVersion、会话的超时时间 timeOut、会话标识 sessionId 和会话密码 passwd，其数据结构定义如下∶

    ```
    module org.apache.zookeeper.proto {
    	class ConnectResponse {
        	int protocolVersion;
            int time0ut;
            Long sessionId;
            buffer passwd;
        }
    }
    ```

  - GetDataResponse∶ 获取节点数据

    针对客户端的获取节点数据请求，服务端会返回客户端一个 GetDataResponse响应，该响应体中包含了数据节点的数据内容 data 和节点状态 stat，其数据结构定义如下∶

    ```
    module org.apache.zookeeper.proto { 
    	class GetDataResponse { 
    		buffer data;
    		org.apache.zookeeper.data.Stat stat;
    	}
    }
    ```

  - SetDataResponse∶ 更新节点数据

    针对客户端的更新节点数据请求，服务端会返回客户端一个 SetDataResponse响应，该响应体中包含了最新的节点状态 stat，其数据结构定义如下∶

    ```
    module org.apache.zookeeper.proto { 
    	class SetDataResponse {
    		org.apache.zookeeper.data.Stat stat;
    	}
    }
    ```

#### 7.3 客户端

ZooKeeper 的客户端主要由以下几个核心组件组成。

- ZooKeeper 实例∶客户端的入口。

- ClientWatchManager∶客户端 Watcher 管理器。
- HostProvider∶ 客户端地址列表管理器。

- CLientCnxn∶ 客户端核心线程，其内部又包含两个线程，即 SendThread 和 EventThread。前者是一个 I/O 线程，主要负责 ZooKeeper 客户端和服务端之间的网络 I/O 通信；后者是一个事件线程，主要负责对服务端事件进行处理。

<img src="img/Zookeeper/21.PNG" style="zoom: 50%;" />

ZooKeeper 客户端的初始化与启动环节，实际上就是 ZooKeeper 对象的实例化过程。客户端的整个初始化和启动过程大体可以分为以下 3 个步骤。

1. 设置默认 Watcher。
2. 设置 ZooKeeper 服务器地址列表。
3. 创建 CLientCnxn。

##### 7.3.1 一次会话的创建过程

<img src="img/Zookeeper/22.PNG" style="zoom:67%;" />

以白色作为底色的框图流程可以看作是第一阶段，我们称之为初始化阶段；以斜线底纹表示的流程是第二阶段，称之为会话创建阶段；以点状底纹表示的则是客户端在接收到服务端响应后的对应处理，称之为响应处理阶段。

###### 初始化阶段

1. 初始化 ZooKeeper 对象。

   通过调用ZooKeeper的构造方法来实例化一个ZooKeeper对象，在初始化过程中，会创建一个客户端的 Watcher 管理器∶ ClientwatchManager。

2. 设置会话默认 Watcher。

   如果在构造方法中传入了一个 Watcher 对象，那么客户端会将这个对象作为默认 Watcher 保存在 ClientwatchManager 中。

3. 构造 ZooKeeper 服务器地址列表管理器∶ HostProvider。

   对于构造方法中传入的服务器地址，客户端会将其存放在服务器地址列表管理器 HostProvider 中。
4. 创建并初始化客户端网络连接器∶ClientCnxn。

   ZooKeeper 客户端首先会创建一个网络连接器 ClientCnxn，用来管理客户端与服务器的网络交互。另外，客户端在创建 ClientCnxn的同时，还会初始化客户端两个核心队列 outgoingQueue 和 pendingQueue，分别作为客户端的请求发送队列和服务端响应的等待队列。

   在后面的章节中我们也会讲到，ClientCnxn 连接器的底层 I/O 处理器是 ClientCnxnSocket，因此在这一步中，客户端还会同时创建 ClientCnxnSocket处理器。
5. 初始化 SendThread 和 EventThread。

   客户端会创建两个核心网络线程 SendThread 和 EventThread，前者用于管理客户端和服务端之间的所有网络 I/O，后者则用于进行客户端的事件处理。同时，客户端还会将 ClientCnxnSocket 分配给 SendThread 作为底层网络 I/O处理器，并初始化 EventThread 的待处理事件队列waitingEvents，用于存放所有等待被客户端处理的事件。

###### 会话创建阶段

1. 启动 SendThread 和 EventThread。

   SendThread 首先会判断当前客户端的状态，进行一系列清理性工作，为客户端发送"会话创建"请求做准备。

2. 获取一个服务器地址。

   在开始创建 TCP连接之前，SendThread 首先需要获取一个 ZooKeeper 服务器的目标地址，这通常是从 HostProvider 中随机获取出一个地址，然后委托给 ClientCnxnSocket 去创建与ZooKeeper 服务器之间的 TCP连接。 

3. 创建 TCP 连接。

   获取到一个服务器地址后，ClientCnxnSocket 负责和服务器创建一个TCP长连接。

4. 构造 ConnectRequest 请求。

   在TCP连接创建完毕后，可能有的读者会认为，这样是否就说明已经和ZooKeeper服务器完成连接了呢?其实不然，步骤 8只是纯粹地从网络 TCP层面完成了客户端与服务端之间的 Socket 连接，但远未完成 ZooKeeper客户端的会话创建。SendThread 会负责根据当前客户端的实际设置，构造出一个ConnectRequest请求，该请求代表了客户端试图与服务器创建一个会话。同时，ZooKeeper 客户端还会进一步将该请求包装成网络 I/O 层的 Packet 对象，放入请求发送队列 outgoingQueue 中去。

5. 发送请求。

   当客户端请求准备完毕后，就可以开始向服务端发送请求了。ClientCnxnSocket 负责从outgoingQueue 中取出一个待发送的Packet对象，将其序列化成ByteBuffer后，向服务端进行发送。

###### 响应处理阶段

1. 接收服务端响应。

   ClientCnxnSocket 接收到服务端的响应后，会首先判断当前的客户端状态是否是"已初始化"，如果尚未完成初始化，那么就认为该响应一定是会话创建请求的响应，直接交由 readConnectResult 方法来处理该响应。 

2. 处理 Response。

   ClientCnxnSocket 会对接收到的服务端响应进行反序列化，得到 ConnectResponse 对象，并从中获取到 ZooKeeper 服务端分配的会话 sessionId。

3. 连接成功。

   连接成功后，一方面需要通知 SendThread 线程，进一步对客户端进行会话参数的设置，包括 readTimeout 和 connectTimeout 等，并更新客户端状态；另一方面，需要通知地址管理器 HostProvider 当前成功连接的服务器地址。

4. 生成事件∶SyncConnected-None。

   为了能够让上层应用感知到会话的成功创建，SendThread 会生成一个事件 SyncConnected-None，代表客户端与服务器会话创建成功，并将该事件传递给 EventThread 线程。 

5. 查询 Watcher。

   EventThread 线程收到事件后，会从 ClientWatchManager 管理器中查询出对应的 Watcher，针对 SyncConnected-None 事件，那么就直接找出存储的默认 Watcher，然后将其放到 EventThread 的 waitingEvents 队列中去。

6. 处理事件。

   EventThread 不断地从 waitingEvents 队列中取出待处理的 Watcher 对象，然后直接调用该对象的 process 接口方法，以达到触发 Watcher 的目的。

##### 7.3.2 服务器地址列表

在使用ZooKeeper 构造方法时，用户传入的ZooKeeper 服务器地址列表，即 connectString参数，通常是这样一个使用英文状态逗号分隔的多个IP 地址和端口的字符串：

192.168.0.1:2181,192.168.0.1:2181,192.168.0.1:2181

从这个地址串中我们可以看出，ZooKeeper客户端允许我们将服务器的所有地址都配置在一个字符串上，于是一个问题就来了∶ ZooKeeper 客户端在连接服务器的过程中，是如何从这个服务器列表中选择服务器机器的呢?是按序访问，还是随机访问呢? ZooKeeper 客户端内部在接收到这个服务器地址列表后，会将其首先放入一个 ConnectStringParser 对象中封装起来。ConnectStringParser 是一个服务器地址列表的解析器，该类的基本结构如下∶

```java
public final class ConnectStringParser { 
	String chrootPath;
	ArrayList<InetSocketAddress> serverAddresses= new ArrayList<InetSocketAddress>();
}
```

ConnectStringParser 解析器将会对传入的 connectString 做两个主要处理∶解析 chrootPath；保存服务器地址列表。

###### Chroot∶客户端隔离命名空间

在3.2.0及其之后版本的 ZooKeeper中，添加了"Chroot"特性"，该特性允许每个客户端为自己设置一个命名空间（Namespace）。如果一个 ZooKeeper 客户端设置了 Chroot，那么该客户端对服务器的任何操作，都将会被限制在其自己的命名空间下。

举个例子来说，如果我们希望为应用 X 分配apps/X 下的所有子节点，那么该应用可以将其所有 ZooKeeper客户端的 Chroot 设置为/apps/X。一旦设置了 Chroot 之后，那么对这个客户端来说，所有的节点路径都以/apps/X 为根节点，它和 ZooKeeper 发起的所有请求中相关的节点路径，都将是一个相对路径——相对于/apps/X 的路径。例如通过 ZooKeeper 客户端 API 创建节点/test_chroot，那么实际上在服务端被创建的节点是/apps/X/test_chroot，通过设置 Chroot，我们能够将一个客户端应用与ZooKeeper 服务端的一棵子树相对应，在那些多个应用共用一个 ZooKeeper 集群的场景下，这对于实现不同应用之间的相互隔离非常有帮助。

客户端可以通过在 connectString 中添加后缀的方式来设置 Chroot，如下所示：

192.168.0.1:2181,192.168.0.1:2181,192.168.0.1:2181/apps/X

将这样一个connectString传入客户端的 ConnectStringParser后就能够解析出 Chroot 并保存在 chrootPath 属性中。

###### HostProvider∶ 地址列表管理器

在 ConnectStringParser 解析器中会对服务器地址做一个简单的处理，并将服务器地址和相应的端口封装成一个 InetSocketAddress 对象，以 ArrayList 形式保存在 ConnectStringParser.serverAddresses 属性中。然后，经过处理的地址列表会被进一步封装到 StaticHostProvider 类中。

在讲解StaticHostProvider 之前，我们首先来看其对应的接口：HostProvider。HostProvider 类定义了一个客户端的服务器地址管理器∶

```java
public interface HostProvider { 
	pubLic int size();
    /**
    * The next host to try to connect to.
    * For a spinDelay of θ there should be no wait.
    * @param spinDelay
    * 		Milliseconds to wait if all hosts have been tried once.
	*/
    public InetSocketAddress next(Long spinDelay);

    /**
    * Notify the HostProvider of a successful connection.
	* The HostProvider may use this notification to reset it's inner state.
	*/
    pubLic void onConnected();
```

| 接口方法                               | 说 明                                                        |
| -------------------------------------- | ------------------------------------------------------------ |
| int size()                             | 该方法用于返回当前服务器地址列表的个数                       |
| InetSocketAddress next(Long spinDetay) | 该 方 法 用 于 返 回 一 个 服 务 器 地 址InetSocketAddress，以便客户端进行服务器连接 |
| void onConnected()                     | 这是一个回调方法，如果客户端与服务器成功创建连接，就通过调用这个方法来通知 HostProvider |

ZooKeeper规定，任何对于该接口的实现必须满足以下3点，这里简称为"HostProvider三要素"。

- next()方法必须要有合法的返回值

  ZooKeeper 规定，凡是对该方法的调用，必须要返回一个合法的 InetSocketAddress对象。也就是说，不能返回 null或其他不合法的Inet SocketAddress。

- next()方法必须返回已解析的 InetSocketAddress 对象。

  在上面我们已经提到，服务器的地址列表已经被保存在 ConnectStringParser.serverAddresses 中，但是需要注意的一点是，此时里面存放的都是没有被解析的 InetSocketAddress。在进一步传递到 HostProvider 后， HostProvider 需要负责来对这个 InetSocketAddress 列表进行解析，不一定是在 next()方法中来解析，但是无论如何，最终在 next()方法中返回的必须是已被解析的 InetSocketAddress 对象。

- size()方法不能返回 0。

  ZooKeeper 规定了该方法不能返回 0，也就是说，HostProvider 中必须至少有一个服务器地址。

##### 7.3.3 ClientCnxn∶ 网络 I/O

ClientCnxn是 ZooKeeper客户端的核心工作类，负责维护客户端与服务端之间的网络连接并进行一系列网络通信。

###### Packet

Packet是 ClientCnxn 内部定义的一个对协议层的封装，作为 ZooKeeper 中请求与响应的载体。

<img src="img/Zookeeper/23.PNG" style="zoom: 67%;" />

Packet 中包含了最基本的请求头（requestHeader）、响应头（replyHeader）、请求体（request）、响应体（response）、节点路径（clientPath/serverPath）和注册的 Watcher（watchReqistration）等信息。

Packet的 createBB()方法负责对 Packet 对象进行序列化，最终生成可用于底层网络传输的 ByteBuffer 对象。在这个过程中，只会将 requestHeader、request 和 readOnLy 三个属性进行序列化，其余属性都保存在客户端的上下文中，不会进行与服务端之间的网络传输。

###### outgoingQueue 和 pendingQueue

CLientCnxn中，有两个比较核心的队列 outgoingQueue 和 pendingQueue，分别代表客户端的请求发送队列和服务端响应的等待队列。0utgoing 队列是一个请求发送队列，专门用于存储那些需要发送到服务端的 Packet 集合。Pending 队列是为了存储那些已经从客户端发送到服务端的，但是需要等待服务端响应的 Packet 集合。

###### CLientCnxnSocket∶ 底层 Socket 通信层

ClientCnxnSocket 定义了底层 Socket 通信的接口。在使用 ZooKeeper客户端的时候，可以通过在 zookeeper.clientCnxnSocket 这个系统变量中配置 CLientCnxnSocket 实现类的全类名，以指定底层 Socket 通信层的自定义实现，例如，-Dzookeeper.clientCnxnSocket= org.apache.zookeeper.ClientCnxnSocketNIO。在ZooKeeper中，其默认的实现是 CLientCnxnSocketNIO。该实现类使用 Java 原生的 NIO接口，其核心是 doIO 逻辑，主要负责对请求的发送和响应接收过程。

- 请求发送：在正常情况下（即客户端与服务端之间的 TCP 连接正常且会话有效的情况下），会从 outgoingQueue 队列中提取出一个可发送的 Packet 对象，同时生成一个客户端请求序号 XID 并将其设置到 Packet 请求头中去，然后将其序列化后进行发送。这里提到了"获取一个可发送的 Packet 对象"，那么什么样的 Packet 是可发送的呢？在 outgoingQueue 队列中的 Packet整体上是按照先进先出的顺序被处理的，但是如果检测到客户端与服务端之间正在处理 SASL 权限的话，那么那些不含请求头（requestHeader）的 Packet（例如会话创建请求）是可以被发送的，其余的都无法被发送。

  请求发送完毕后，会立即将该 Packet 保存到 pendingQueue队列中，以便等待服务端响应返回后进行相应的处理。

- 响应接收：客户端获取到来自服务端的完整响应数据后，根据不同的客户端请求类型，会进行不同的处理。

  - 如果检测到当前客户端还尚未进行初始化，那么说明当前客户端与服务端之间正在进行会话创建，那么就直接将接收到的 ByteBuffer（incomingBuffer）序列化成 ConnectResponse 对象。

  - 如果当前客户端已经处于正常的会话周期，并且接收到的服务端响应是一个事件，那么 ZooKeeper 客户端会将接收到的 ByteBuffer（incomingBuffer）序列化成 WatcherEvent 对象，并将该事件放入待处理队列中。

  - 如果是一个常规的请求响应（指的是 Create、GetData 和 Exist 等操作请求），那么会从 pendingQueue 队列中取出一个 Packet来进行相应的处理。ZooKeeper客户端首先会通过检验服务端响应中包含的 XID 值来确保请求处理的顺序性，然后再将接收到的 ByteBuffer（incomingBuffer）序列化成相应的 Response对象。

  最后，会在 finishPacket 方法中处理 Watcher 注册等逻辑。

###### SendThread

SendThread 是客户端 ClientCnxn 内部一个核心的 I/O 调度线程，用于管理客户端和服务端之间的所有网络 I/O 操作。在 ZooKeeper 客户端的实际运行过程中，一方面， SendThread 维护了客户端与服务端之间的会话生命周期，其通过在一定的周期频率内向服务端发送一个 PING 包来实现心跳检测。同时，在会话周期内，如果客户端与服务端之间出现 TCP 连接断开的情况，那么就会自动且透明化地完成重连操作。另一方面，SendThread 管理了客户端所有的请求发送和响应接收操作，其将上层客户端 API操作转换成相应的请求协议并发送到服务端，并完成对同步调用的返回和异步调用的回调。同时，SendThread 还负责将来自服务端的事件传递给 EventThread 去处理。 

###### EventThread

EventThread 是客户端 ClientCnxn 内部的另一个核心线程，负责客户端的事件处理，并触发客户端注册的 Watcher 监听。EventThread 中有一个 waitingEvents 队列，用于临时存放那些需要被触发的 Object，包括那些客户端注册的 Watcher 和异步接口中注册的回调器 AsyncCallback。同时，EventThread 会不断地从 waitingEvents这个队列中取出 Object，识别出其具体类型（Watcher 或者 AsyncCallback），并分别调用 process 和 processResuLt 接口方法来实现对事件的触发和回调。

#### 7.4 会话

会话（Session）是 ZooKeeper 中最重要的概念之一，客户端与服务端之间的任何交互操作都与会话息息相关，这其中就包括临时节点的生命周期、客户端请求的顺序执行以及 Watcher 通知机制等。简单地说，ZooKeeper 的连接与会话就是客户端通过实例化 ZooKeeper 对象来实现客户端与服务器创建并保持 TCP 连接的过程。

##### 7.4.1 会话状态

在 ZooKeeper 客户端与服务端成功完成连接创建后，就建立了一个会话。ZooKeeper 会话在整个运行期间的生命周期中，会在不同的会话状态之间进行切换，这些状态一般可以分为 CONNECTING，CONNECTED，RECONNECTING，RECONNECTED 和 CLOSE等。

一旦客户端开始创建 ZooKeeper 对象，那么客户端状态就会变成 CONNECTING，同时客户端开始从上述服务器地址列表中逐个选取 IP 地址来尝试进行网络连接，直到成功连接上服务器，然后将客户端状态变更为 CONNECTED。

通常情况下，伴随着网络闪断或是其他原因，客户端与服务器之间的连接会出现断开情况。一旦碰到这种情况，ZooKeeper 客户端会自动进行重连操作，同时客户端的状态再次变为 CONNECTING，直到重新连接上 ZooKeeper 服务器后，客户端状态又会再次转变成 CONNECTED。因此，通常情况下，在 ZooKeeper 运行期间，客户端的状态总是介于 CONNECTING和 CONNECTED两者之一。

另外，如果出现诸如会话超时、权限检查失败或是客户端主动退出程序等情况，那么客户端的状态就会直接变更为 CLOSE。

##### 7.4.2 会话创建

###### Session

Session是ZooKeeper 中的会话实体，代表了一个客户端会话。其包含以下4个基本属性。

- sessionID∶会话 ID，用来唯一标识一个会话，每次客户端创建新会话的时候， ZooKeeper 都会为其分配一个全局唯一的 sessionID。
- TimeOut∶会话超时时间。客户端在构造 ZooKeeper 实例的时候，会配置一个sessionTimeout 参数用于指定会话的超时时间。ZooKeeper客户端向服务器发送这个超时时间后，服务器会根据自己的超时时间限制最终确定会话的超时时间。
- TickTime：下次会话超时时间点。为了便于ZooKeeper 对会话实行"分桶策略"管理，同时也是为了高效低耗地实现会话的超时检查与清理，ZooKeeper 会为每个会话标记一个下次会话超时时间点。TickTime 是一个13 位的 Long型数据，其值接近于当前时间加上 TimeOut，但不完全相等。
- isClosing∶ 该属性用于标记一个会话是否已经被关闭。通常当服务端检测到一个会话已经超时失效的时候，会将该会话的 isClosing 属性标记为"已关闭"，这样就能确保不再处理来自该会话的新请求了。

###### sessionID

在 SessionTracker 初始化的时候，会调用 initializeNextSession 方法来生成一个初始化的 sessionID，之后在 ZooKeeper 的正常运行过程中，会在该 sessionID 的基础上为每个会话进行分配，其初始化算法如下∶

```java
public static Long initializeNextSession(Long id) {
	Long nextSid = 0;
	nextSid =(System.currentTimeMillis()<< 24) >>> 8;
    nextSid = nextSid | (id <<56); 
    return nextSid;
}
```

- id代表机器编号，高8位确定了所在机器，低56位使用当前时间的毫秒表示进行随机。
- 使用无符号右移>>>确保即使首位为1，移位后高位也是0。

######  SessionTracker

SessionTracker 是 ZooKeeper 服务端的会话管理器，负责会话的创建、管理和清理等工作。可以说，整个会话的生命周期都离不开 SessionTracker 的管理。每一个会话在 SessionTracker 内部都保留了三份，具体如下。

- sessionsById∶这是一个 HashMap<Long，SessionImpl>类型的数据结构，用于根据 sessionID 来管理 Session 实体。

- sessionsWithTimeout∶ 这是一个 ConcurrentHashMap<Long，Integer>类型的数据结构，用于根据 sessionID 来管理会话的超时时间。该数据结构和 ZooKeeper 内存数据库相连通，会被定期持久化到快照文件中去。

- sessionSets∶ 这是一个HashMap<Long，SessionSet>类型的数据结构，用于根据下次会话超时时间点来归档会话，便于进行会话管理和超时检查。

##### 7.4.3 创建连接

服务端对于客户端的"会话创建"请求的处理，大体可以分为四大步骤，分别是处理 ConnectRequest 请求、会话创建、处理器链路处理和会话响应。在 ZooKeeper 服务端，首先将会由 NIOServerCnxn 来负责接收来自客户端的"会话创建"请求，并反序列化出 ConnectRequest 请求，然后根据 ZooKeeper 服务端的配置完成会话超时时间的协商。随后，SessionTracker 将会为该会话分配一个 sessionID，并将其注册到 sessionsById 和 sessionsWithTimeout中去，同时进行会话的激活。之后，该"会话请求"还会在 ZooKeeper 服务端的各个请求处理器之间进行顺序流转，最终完成会话的创建。

###### 分桶策略

<img src="img/Zookeeper/24.PNG" style="zoom:50%;" />

ZooKeeper 的会话管理主要是由 SessionTracker 负责的，其采用了一种特殊的会话管理方式，我们称之为"分桶策略"。所谓分桶策略，是指将类似的会话放在同一区块中进行管理，以便于 ZooKeeper 对会话进行不同区块的隔离处理以及同一区块的统一处理。分配的原则是每个会话的"下次超时时间点"（ExpirationTime）。ExpirationTime是指该会话最近一次可能超时的时间点，对于一个新创建的会话而言，其会话创建完毕后， ZooKeeper 就会为其计算 ExpirationTime，计算方式如下∶

```
ExpirationTime = CurrentTime + SessionTimeout
```

其中 CurrentTime 指当前时间，单位是毫秒；SessionTimeout 指该会话设置的超时时间，单位也是毫秒。

在 ZooKeeper 的实际实现中，还做了一个处理。ZooKeeper的 Leader 服务器在运行期间会定时地进行会话超时检查，其时间间隔是 Expirationlnterval，单位是毫秒，默认值是 tickTime 的值，即默认情况下，每隔 2000 毫秒进行一次会话超时检查。为了方便对多个会话同时进行超时检查，完整的 ExpirationTime 的计算方式如下∶

```
ExpirationTime_ = CurrentTime+ SessionTimeout
ExpirationTime =(ExpirationTime_/ExpirationIntervaL+1)×ExpirationIntervaL
```

即ExpirationTime 值总是 Expirationlnterval的整数倍数。

###### 会话激活

为了保持客户端会话的有效性，在 ZooKeeper 的运行过程中，客户端会在会话超时时间过期范围内向服务端发送 PING 请求来保持会话的有效性，我们俗称"心跳检测"。同时，服务端需要不断地接收来自客户端的这个心跳检测，并且需要重新激活对应的客户端会话，我们将这个重新激活的过程称为 TouchSession。会话激活的过程，不仅能够使服务端检测到对应客户端的存活性，同时也能让客户端自己保持连接状态。

1. 检验该会话是否已经被关闭。

   Leader 会检查该会话是否已经被关闭，如果该会话已经被关闭，那么不再继续激活该会话。

2. 计算该会话新的超时时间 ExpirationTime_New。

   如果该会话尚未关闭，那么就开始激活会话。首先需要计算出该会话下一次超时时间点，使用的就是上面提到的计算公式。

3. 定位该会话当前的区块。

   获取该会话老的超时时间 ExpirationTime Old，并根据该超时时间来定位到其所在的区块。

4. 迁移会话

   将该会话从老的区块中取出，放入 ExpirationTime_New 对应的新区块中。

   <img src="img/Zookeeper/25.PNG" style="zoom:50%;" />

通过以上 4步，就基本完成会话激活的过程。在上面的会话激活过程中，我们可以看到，只要客户端发来心跳检测，那么服务端就会进行一次会话激活。心跳检测由客户端主动发起，以 PING 请求的形式向服务端发送。但实际上，在 ZooKeeper 服务端的设计中，只要客户端有请求发送到服务端，那么就会触发一次会话激活。因此，总的来讲，大体会出现以下两种情况下的会话激活。

- 只要客户端向服务端发送请求，包括读或写请求，那么就会触发一次会话激活。
- 如果客户端发现在 sessionTimeout /3 时间内尚未和服务器进行过任何通信，即没有向服务端发送任何请求，那么就会主动发起一个 PING 请求，服务端收到该请求后，就会触发上述第一种情况下的会话激活。

###### 会话超时检查

在 ZooKeeper 中，会话超时检查同样是由 SessionTracker 负责的。SessionTracker 中有一个单独的线程专门进行会话超时检查，这里我们将其称为"超时检查线程"，其工作机制的核心思路其实非常简单∶逐个依次地对会话桶中剩下的会话进行清理。

如果一个会话被激活，那么 ZooKeeper 会将其从上一个会话桶迁移到下一个会话桶中，例如图中的 session.n 这个会话，由于触发了会话激活，因此 ZooKeeper 会将其从 expirationTime 1桶迁移到 expirationTime n 桶中去。于是，expirationTime 1中留下的所有会话都是尚未被激活的。因此，超时检查线程的任务就是定时检查出这个会话桶中所有剩下的未被迁移的会话。

在会话分桶策略中，我们将 Expirationlnterval的倍数作为时间点来分布会话，因此，超时检查线程只要在这些指定的时间点上进行检查即可，这样既提高了会话检查的效率，而且由于是批量清理，因此性能非常好——这也是为什么 ZooKeeper 要通过分桶策略来管理客户端会话的最主要的原因。因为在实际生产环境中，一个 ZooKeeper集群的客户端会话数可能会非常多，逐个依次检查会话的方式会非常耗费时间。

##### 7.4.4 会话清理

当 SessionTracker 的会话超时检查线程整理出一些已经过期的会话后，那么就要开始进行会话清理了。会话清理的步骤大致可以分为以下 7 步。

1. 标记会话状态为"已关闭"。

   由于整个会话清理过程需要一段的时间，因此为了保证在此期间不再处理来自该客户端的新请求，SessionTracker 会首先将该会话的 isClosing 属性标记为 true。这样，即使在会话清理期间接收到该客户端的新请求，也无法继续处理了。 

2. 发起"会话关闭"请求。

   为了使对该会话的关闭操作在整个服务端集群中都生效，ZooKeeper 使用了提交"会话关闭"请求的方式，并立即交付给 PrepRequestProcessor处理器进行处理。

3. 收集需要清理的临时节点

   在 ZooKeeper 中，一旦某个会话失效后，那么和该会话相关的临时（EPHEMERAL）节点都需要被一并清除掉。因此，在清理临时节点之前，首先需要将服务器上所有和该会话相关的临时节点都整理出来。

   在 ZooKeeper 的内存数据库中，为每个会话都单独保存了一份由该会话维护的所有临时节点集合，因此在会话清理阶段，只需要根据当前即将关闭的会话的 sessionID 从内存数据库中获取到这份临时节点列表即可。

   但是，在实际应用场景中，情况并没有那么简单，有如下的细节需要处理∶在 ZooKeeper 处理会话关闭请求之前，正好有以下两类请求到达了服务端并正在处理中。
   - 节点删除请求，删除的目标节点正好是上述临时节点中的一个。
   - 临时节点创建请求，创建的目标节点正好是上述临时节点中的一个。

   对于这两类请求，其共同点都是事务处理尚未完成，因此还没有应用到内存数据库中，所以上述获取到的临时节点列表在遇上这两类事务请求的时候，会存在不一致的情况。

   假定我们当前获取的临时节点列表是 ephemerals，那么针对第一类请求，我们需要将所有这些请求对应的数据节点路径从 ephemerals 中移除，以避免重复删除。针对第二类请求，我们需要将所有这些请求对应的数据节点路径添加到 ephemerals 中去，以删除这些即将会被创建但是尚未保存到内存数据库中去的临时节点。

4. 添加"节点删除"事务变更。

   完成该会话相关的临时节点收集后，ZooKeeper 会逐个将这些临时节点转换成"节点删除"请求，并放入事务变更队列 outstandingChanges 中去。

5. 删除临时节点。

   在上面的步骤中，我们已经收集了所有需要删除的临时节点，并创建了对应的"节点删除"请求，FinaLRequestProcessor 处理器会触发内存数据库，删除该会话对应的所有临时节点。

6. 移除会话。

   完成节点删除后，需要将会话从 SessionTracker 中移除。主要就是从上面提到的三个数据结构（sessionsById、sessionsWithTimeout 和 sessionSets）中将该会话移除掉。 

7. 关闭 NIOServerCnxn。

   最后，从 NIOServerCnxnFactory 找到该会话对应的 NIOServerCnxn，将其关闭。

##### 7.4.5 重连

当客户端和服务端之间的网络连接断开时，ZooKeeper客户端会自动进行反复的重连，直到最终成功连接上 ZooKeeper 集群中的一台机器。在这种情况下，再次连接上服务端的客户端有可能会处于以下两种状态之一。

- CONNECTED;如果在会话超时时间内重新连接上了 ZooKeeper 集群中任意一台机器，那么被视为重连成功。
- EXPIRED∶如果是在会话超时时间以外重新连接上，那么服务端其实已经对该会话进行了会话清理操作，因此再次连接上的会话将被视为非法会话。

当客户端与服务端之间的连接断开后，用户在客户端可能主要会看到两类异常; CONNECTION_LOSS（连接断开）和 SESSION_EXPIRED（会话过期）。

###### 连接断开∶ CONNECTION_LOSS

在这种情况下，ZooKeeper 客户端会自动从地址列表中重新逐个选取新的地址并尝试进行重新连接，直到最终成功连接上服务器。举个例子，假设某应用在使用 ZooKceceper 客户端进行 setData 操作的时候，正好出现了 CONNECTION_LOSS 现象，那么客户端会立即按收到事件 None-Disconnected 通知，同时会抛出异常∶ org.apache.zookeeper.KeeperException$ConnectionLossException。**在这种情况下，我们的应用需要做的事情就是捕获住 ConnectionLossException，然后等待 ZooKeeper 的客户端自动完成重连。一旦客户端成功连接上一台 ZooKeeper 机器后，那么客户端就会收到事件 None-SyncConnected 通知，之后就可以重试刚刚出错的 setData 操作。**

###### 会话失效∶ SESSION_EXPIRED

SESSION_EXPIRED 是指会话过期，通常发生在 CONNECTION_LOSS 期间。客户端和服务器连接断开之后，由于重连期间耗时过长，超过了会话超时时间（sessionTimeout）限制后还没有成功连接上服务器、那么服务器认为这个会话已经结束了，就会开始进行会话清理。但是另一方面，该客户端本身不知道会话已经失效，并且其客户端状态还是 DISCONNECTED。之后，如果客户端重新连接上了服务器，那么很不幸，服务器会告诉客户端该会话已经失效（SESSION_EXPIRED）。**在这种情况下，用户就需要重新实例化一个 ZooKeeper 对象，并且看应用的复杂情况，重新恢复临时数据。**

###### 会话转移∶ SESSION_MOVED

会话转移是指客户端会话从一台服务器机器转移到了另一台服务器机器上。

在 3.2.0 版本之后，ZooKeeper 明确提出了会话转移的概念，同时封装了 SessionMovedException 异常。之后，在处理客户端请求的时候，会首先检查会话的所有者（Owner）∶如果客户端请求的会话 Owner 不是当前服务器的话，那么就会直接抛出 SessionMovedException 异常。当然，由于客户端已经和这个服务器断开了连接，因此无法收到这个异常的响应。只有多个客户端使用相同的 sessionId/ sessionPasSwd 创建会话时，才会收到这样的异常。因为一旦有一个客户端会话创建成功，那么 ZooKeeper 服务器就会认为该 sessionId 对应的那个会话已经发生了转移，于是，等到第二个客户端连接上服务器后，就被认为是"会话转移"的情况了。

